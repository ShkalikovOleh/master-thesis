\chapter{Methodology}
\label{sec:methodology}
The overall idea of formalization the projection step of the XLNER pipeline
is to represent it as a matching the source entities and ranges of words
of the target sentence, called candidates. It is shown on the figure \ref{fig:cand_matching}.
The set of source entities that we need to project is given, so let's take some set of
continuous word ranges from the original sentence in the target language
and consider them as a possible projection of some entities. Then compute
quantities how likely a source entity should be projected onto every candidate.
And then we should take such a combination of source entities-target candidates
that maximize overall sum of selected tuples. But we need to take in mind
natural constraints that should be ensured. Assume two source entities
projects onto overlapping candidates, then we cannot precisely answer which source
entity overlapped words are projected to, therefore we should prohibit projection
onto overlapped candidates. Also, it makes sense to limit how many candidates every
source entity can be projected to, since usually we expect 1-to-1 correspondence
between source entities and entities of the target sentence. Based on this simple
properties let's build an integer linear optimization problem and study it.

\begin{figure*}[h]
    \centering
    \begin{tikzpicture}[node distance=-0.1,
            every node/.style={text centered,
                    text height=2ex,
                    text depth=.25ex,
                },
            loc/.style={fill=orange!30, rounded rectangle, label={[anchor=center,font=\tiny\bfseries\sffamily]above:#1-LOC}},
            per/.style={fill=green!30, rounded rectangle, label={[anchor=center,font=\tiny\bfseries\sffamily]above:#1-PER}},
            cand/.style={fill=blue!30, rounded rectangle},]

        \node[per={B}, rounded rectangle east arc=none](George_src){George};
        \node[per={I}, rounded rectangle west arc=none, right=of George_src](Washington_src){Washington};
        \node[right=of Washington_src](is_src){is};
        \node[right=of is_src](the_src){the};
        \node[right=of the_src](first_src){first};
        \node[right=of first_src](president_src){president};
        \node[right=of president_src](of_src){of};
        \node[right=of of_src](the_src){the};
        \node[loc={B}, rounded rectangle east arc=none, right=of the_src](United_src){United};
        \node[loc={I}, rounded rectangle west arc=none, right=of United_src](States_src){States};

        \node[cand, rounded rectangle east arc=none, below=of George_src, yshift=-1.5cm](George_tgt){George};
        \node[cand, rounded rectangle west arc=none, right=of George_tgt](Washington_tgt){Washington};
        \node[right=of Washington_tgt](ist_tgt){ist};
        \node[right=of ist_tgt](der_tgt){die};
        \node[right=of der_tgt](erste_tgt){erste};
        \node[right=of erste_tgt](president_tgt){PrÃ¤sident};
        \node[right=of president_tgt](der_tgt){der};
        \node[cand, rounded rectangle east arc=none, right=of der_tgt](Vereinigten_tgt){Vereinigten};
        \node[cand, rounded rectangle west arc=none, right=of Vereinigten_tgt](Staaten_tgt){Staaten};

        \node[text=gray, font=\scriptsize, above=of first_src, yshift=0.2cm, xshift=0.2cm](source){Source labeled sentence};
        \node[text=gray, font=\scriptsize, below=of source, yshift=-3cm]{Original sentence with extracted candidates};

        \draw[->] (George_src.south east) -- node[left]{\(c_{11}\)} (George_tgt.north east);
        \draw[->] (United_src.south east) -- node[right]{\(c_{22}\)} (Vereinigten_tgt.north east);
        \draw[->] (George_src.south east) -- node[above left, yshift=0.1cm, xshift=-0.2cm]{\(c_{12}\)} (Vereinigten_tgt.north east);
        \draw[->] (United_src.south east) -- node[below left]{\(c_{21}\)} (George_tgt.north east);
    \end{tikzpicture}
    \caption{Illustration of the proposed idea of matching source entities and candidates in the target sentence}
    \label{fig:cand_matching}
\end{figure*}

\section{Formulation of the ILP problem}
First of all let's start with a definition of overlapping.

\begin{definition}[Relation of overlapping] \label{def:overlapping}
    We consider that target candidates \linebreak
    \( \tgt{p_1} = (i_{\tgt{p_1}}, j_{\tgt{p_1}}), \tgt{p_2} = (i_{\tgt{p_2}}, j_{\tgt{p_2}}) \in T \) overlap if and only if
    \( i_{\tgt{p_1}} \leq j_{\tgt{2}} \land i_{\tgt{p_2}} \leq j_{\tgt{1}} \), i. e. sets of word indices are
    not disjoint. We will denote that candidates overlap using
    the following notation: \( \tgt{p_1} \cap \tgt{p_2} \neq \emptyset \).
\end{definition}

Having defined all necessary objects we can finally formulate the ILP problem for a
projection step of the XLNER pipeline that aligns with our requirements.

Let \( S \) be a set of source entities, \( T \) -- set of target candidates and \( \cap \subset T^2 \) is
a relation of overlapping, then the projection ILP problem is the following:
\begin{align}
    \label{eq:objective}
     & \max\limits_x \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}}                                             \\
     & \text{subject to} \notag                                                                                                                            \\
    \label{eq:num_proj_const}
     & \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} \lessgtr n_{proj}                                      & \forall \src{p} \in S                     \\
    \label{eq:non_overlap_const}
     & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1
     & \forall (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi}(S, T)                                                                            \\
    \label{eq:binary_prog}
     & x_{\src{p}, \tgt{p}} \in \{ 0, 1 \}                                                                     & \forall (\src{p}, \tgt{p}) \in S \times T
\end{align}
where \( \hat{\Pi}(S, T) \) is a set of source entities -- target candidates combinations that can not be
projected onto together since they are overlapping. This set is defined as follows:
\begin{equation}  \label{eq:overlapping_set}
    \begin{aligned}
        \hat{\Pi}(S, T) = \Big\{ (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \Big| \src{p_1}, \src{p_2} \in S, \tgt{p_1}, \tgt{p_2} \in T, \quad \tgt{p_1} \cap \tgt{p_2} \neq \emptyset, \\
        (\src{p_1} \neq \src{p_2}) \lor (\tgt{p_1} \neq \tgt{p_2}) \Big\}
    \end{aligned}
\end{equation}

Here every variable \( x_{\src{p}, \tgt{p}} \) represents whether the source entity \( \src{p} \in S \)
is projected to the target candidates \( \tgt{p} \in T \).
The set of constraints \eqref{eq:non_overlap_const} ensures that it is impossible to have a situation when
two or one source entity are being projected onto overlapping target candidates.
And other set of constraints \eqref{eq:num_proj_const} limits number of projections
for every source entity. Note, that the inequality sign is not fixed since any form
could make sense: less, less or equal and equal can be used to ensure that we don't
project a source entity to as much available candidates as possible as long as it
increase the objection value, and great or great or equal can be a way to enforce
that a source entiry will be projected at least specified number of times, e.g. once,
and solver will not simply ignore it. It is clear that \( n_{proj} = 0 \) is a corner
case and usually doesn't make sense except greater inequality when it simply
eliminates any limits and equivalent to not have any constraints of such type.

Also it is worth to note that definition of the ILP problem is not bounded to a specific form of sets \( S, T \)
and overlapping relation \( \cap \subset T^2 \), so given any sets and overlapping relation with the same properties
as for \ref{def:overlapping}, the ILP problem won't change since it doesn't use explicitly word indices.

However the number of constraints \eqref{eq:non_overlap_const} scales as a cube with respect to
length of the target sentence, assuming that we use all possible continuous subranges as candidates, and as
a square with respect to the number of source entities.
It can make this problem harder to solve. Therefore we need to try to lower number of constraints
of this type. For this let's take a look at the overlapping relation that a basis of it.
\begin{lemma} \label{lemma:not_transitive}
    The relation of overlapping is not transitive.
\end{lemma}
\begin{proof}
    We will proof this by showing a counterexample.
    Assume the following target candidates
    \[
        a = (2, 4) \quad b = (1, 2) \quad c = (4, 5)
    \]
    By the definition of the overlapping relation \( a \cap b \neq \emptyset \) and
    \( a \cap c \neq \emptyset \) but \( b \) and \( c \) are not overlapping.
\end{proof}

This fact leads to the impossibility to split all target candidates into groups of mutually
overlapped candidates and choice at maximum one candidate from every group to match with a source entity.
\begin{corollary}
    It is impossible to partition set of target candidates such that every partition consists of
    candidates that are pairwise overlapping with each other and there are no overlapping candidates
    that are in different partitions.
\end{corollary}
\begin{proof}
    By Lemma \ref{lemma:not_transitive} the overlapping relation is not transitive and therefore
    is not an equivalence relation that implies we can not construct a quotient set.
\end{proof}

Nevertheless it is possible to make number of constraints \eqref{eq:non_overlap_const} lower.
The idea behind the reduction is that if it is impossible to project one or any two source entities to
overlapped candidates it means that it impossible to project any number of source entities to these candidates.
Thus we can just sum up constraints over all source entities.
We can do it because all variables \( x_{\src{p}, \tgt{p}} \) are binary and therefore
can not be negative.
\begin{theorem}
    Set of constraints \eqref{eq:non_overlap_const} is satisfied if and only if
    the following sets of reduced constraints are satisfied:
    \begin{align*}
         & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) \leq 1                                & \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T) \\
         & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1
         & \forall \tgt{p} \in T \Big| \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset                                             \\
    \end{align*}
    where
    \begin{equation*}
        \Pi(T) = \left\{ (\tgt{p_1}, \tgt{p_2}) \Big| \tgt{p_1}, \tgt{p_2} \in T,
        \quad \tgt{p_1} \cap \tgt{p_2} \neq \emptyset,
        \tgt{p_1} \neq \tgt{p_2} \right\}
    \end{equation*}
\end{theorem}
\begin{proof}
    \textit{Necessity:} Assume that constraints \eqref{eq:non_overlap_const} are satisfied, but
    the proposed constraints are not. Then there are two options:
    \[
        \exists (\tgt{p_1}, \tgt{p_2}) \in \Pi(T) \Bigg| \quad
        \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) > 1
    \]
    or
    \[
        \exists \tgt{p} \in T \Big|
        \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
        \Bigg| \quad \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} > 1
    \]

    Consider the first option:
    \begin{align*}
         & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) > 1
        \stackrel{\eqref{eq:binary_prog}}{\implies}                                                                     \\
         & \exists \src{p_1}, \src{p_2} \in S \Big| x_{\src{p_1}, \tgt{p_1}} = 1, x_{\src{p_2}, \tgt{p_2}} = 1 \implies \\
         & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} = 2 > 1
    \end{align*}
    that contradicts our assumption that constraints \eqref{eq:non_overlap_const} are satisfied.

    The same result we can obtain for the second case:
    \begin{align*}
         & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} > 1
        \stackrel{\eqref{eq:binary_prog}}{\implies}                                                                                           \\
         & \exists \src{p_1}, \src{p_2} \in S, \src{p_1} \neq \src{p_2} \Big| x_{\src{p_1}, \tgt{p}} = 1, x_{\src{p_2}, \tgt{p}} = 1 \implies \\
         & x_{\src{p_1}, \tgt{p}} + x_{\src{p_2}, \tgt{p}} = 2 > 1
    \end{align*}

    Thus we proved necessity.

    \textit{Sufficiency:}
    Suppose that the following constraints are satisfied:
    \[
        \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) \leq 1
        \qquad \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T)
    \]
    Then the sum could be equal to \( 0 \) or \( 1 \). If it is zero then none entities are projected onto
    any of these overlapped candidates and therefore constraints \eqref{eq:non_overlap_const} are not violated.
    Let's look at case when the sum is equal to \( 1 \).
    \begin{equation*} \label{eq:derivation_of_const_reduction}
        \begin{aligned}
             & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) = 1
            \stackrel{\eqref{eq:binary_prog}}{\implies}                                                       \\
             & \exists! \src{p} \in S, \exists! \tgt{p} \in \{ \tgt{p_1}, \tgt{p_2}\}
            \Bigg| x_{\src{p}, \tgt{p}} = 1 \implies                                                          \\
             & \forall \src{p_1}, \src{p_2} \in S, x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1
        \end{aligned}
    \end{equation*}
    Therefore we proved that
    \[
        x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1 \qquad
        \forall (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi'}(S, T)
    \]
    where
    \begin{align*}
        \hat{\Pi'}(S, T) = \Big\{ (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \Big| \src{p_1}, \src{p_2} \in S, \tgt{p_1}, \tgt{p_2} \in T, \quad \tgt{p_1} \cap \tgt{p_2} \neq \emptyset, \\
        \tgt{p_1} \neq \tgt{p_2} \Big\}
    \end{align*}

    But it is not the same as set \eqref{eq:overlapping_set}:
    \[
        \hat{\Pi}(S, T) \setminus \hat{\Pi'}(S, T) = \Big\{ (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \Big| \src{p_1}, \src{p_2} \in S, \tgt{p_1}, \tgt{p_2} \in T, \tgt{p_1} = \tgt{p_2},
        \src{p_1} \neq \src{p_2} \Big\}
    \]
    So we need to check whether constraints \eqref{eq:non_overlap_const} are satisfied on this set difference.

    First of all, let's notice that
    \begin{multline*}
        \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T)
        \qquad
        \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}})
        =
        \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p_1}} +
        \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p_2}} \leq 1
        \implies                        \\
        \forall \tgt{p} \in T \Bigg| \exists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
        \quad
        \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1
    \end{multline*}
    Therefore there exists at most one source entity \( \src{p} \in S \) that are projected
    to the target candidate \( \tgt{p} \).
    And using exactly the same derivation as above we get:
    \begin{multline*}
        \forall \tgt{p} \in T \Bigg| \exists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
        \quad
        \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1
        \stackrel{\eqref{eq:binary_prog}}{\implies}                                                                                                                                                       \\
        \begin{aligned}
             & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} = 0
            \stackrel{\eqref{eq:binary_prog}}{\implies}
            \forall \src{p} \in S, x_{\src{p}, \tgt{p}} = 0         \\
            \text{or}                                               \\
             & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} = 1
            \stackrel{\eqref{eq:binary_prog}}{\implies}
            \exists! \src{p} \in S, x_{\src{p}, \tgt{p}} = 1        \\
        \end{aligned} \implies \\
        x_{\src{p_1}, \tgt{p}} + x_{\src{p_2}, \tgt{p}} \leq 1 \qquad
        \begin{aligned}
             & \forall \src{p_1}, \src{p_1} \in S, \src{p_1} \neq \src{p_2}                                                        \\
             & \forall \tgt{p} \in T \Bigg| \exists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
        \end{aligned}
    \end{multline*}
    So, only the case, when entity has no overlapped entity that are
    not equal to it, remains. But this case is fully covered by the second part of the proposed reduced constraints:
    \[
        \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1 \qquad
        \forall \tgt{p} \in T \Big| \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
    \]
    And by considering that this sum can be either \( 0 \) or \( 1 \) and making
    exactly the same derivation we conclude that it implies that
    constraints \eqref{eq:non_overlap_const} are satisfied.
\end{proof}

The theorem allows us to reduce number of constraints and make them scale as a constant
with respect to the number of source entities. As a result we end up with the following final formulation of the ILP problem for the projection step
of the XLNER pipeline.
\begin{equation} \label{eq:ilp}
    \begin{aligned}
         & \max\limits_x \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}}                                                                                                                       \\
         & \text{subject to}                                                                                                                                                                                                             \\
         & \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} \lessgtr n_{proj}                                      & \forall \src{p} \in S                                                                                               \\
         & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) \leq 1                    & \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T)                                                                           \\
         & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1                                                 & \forall \tgt{p} \in T \Big| \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset \\
         & x_{\src{p}, \tgt{p}} \in \{ 0, 1 \}                                                                     & \forall (\src{p}, \tgt{p}) \in S \times T
    \end{aligned}
\end{equation}

\section{Candidates extraction}
Whereas the set \( S \) of source entities are given, since all source entities have been
extracted on the previous step of the XLNER pipeline, the construction of the set \( T \)
of target candidates is an open question.

The simplest way for candidates extraction is to consider all possible n-grams
(continuous ranges of words) of the target sentence as candidates. This approach ensures
that there won't be any possibility that we don't include any actual target entity in the set
target candidates. But when it comes to the computational point of view it can be a problem since
number of all n-grams scales quadratically. And from the application point of view most of
the candidates generated in such way are not good nominee to be a real target entity.

One of the simplest idea to handle this problem is to limit the maximum possible length of the candidate.
From the point of NER it is natural to expect that real target entities usually do not exceed some
predefined number. For example it is unlikely that a person's name will contain more than 10 words or
if we have 3 source entities there is a real target entity that contains all words of the target sentence.

Thus, the algorithm for candidates extraction wilt bounded maximum length of n-grams are the following.
\begin{algorithm}
    \KwData{\( n \in \mathbb{N} \) -- number of words in the target sentence,
        \(M \leq n \in \mathbb{N} \) -- maximum length of a target candidate}
    \KwResult{\( T \) -- set of target candidates}

    \( T \gets \emptyset \) \;
    \For{\(i \gets 0 \) \KwTo \( n \)}{
        \( m \gets \min(s + M, n) \) \;
        \For{\(j \gets i \) \KwTo \( m \)}{
            \( T \gets T \cup \{ (i, j) \} \) \;
        }
    }
    \caption{Bounded length n-gram candidates extraction}
    \label{alg:ngram_extraction}
\end{algorithm}

An alternative approach for candidates extraction is to use some model to generate candidates.
TProjection uses fine-tuned T5 model with beam-search to generate candidates. In theory any LLM can be finetuned
to do it. But the main drawback of this approach is that we need to finetune these models for which we need
a labeled dataset in a target language or rely on cross-lingual model transfer.
But in this case it will be easier and more effective to train a model to label
target entities itself rather than generate candidates for the projection step of the
XLNER pipeline. In addition, autoregressive models are very resource consuming and it is
inefficient to use them only for candidates extraction.

Therefore it make sense also consider multilingual encoder-only Transformer model with a high recall,
but we still need to train these model to extract candidates. In the following chapters we will not consider all
these alternatives and leave for further works.

\section{Matching scores}
So far we have formulated and discussed the ILP problem \eqref{eq:ilp} that projects source entities onto
target candidates using a matching score \( c_{\src{p}, \tgt{p}} \). This score represents
how likely given source entity \( \src{p} \in S \) should be projected onto the target candidate \( \tgt{p} \in T \).
But the question how to compute all these scores remains unanswered. In this section we will propose
some options to evaluate them.

\subsection{Alignment-based score}
In the chapter \ref{sec:background} it has been showed that word-to-word alignments can be used
for the projection step of the XLNER pipeline. But it was done by the heuristic algorithm.
Nevertheless we can try incorporate them into the proposed ILP problem by calculating scores of matching with use of
word-to-word alignments.

The matching score that inspired from this idea is the following:
\begin{equation} \label{eq:align_cost}
    c_{\src{p}, \tgt{p}}^{align} =
    \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
    {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} - i_{\tgt{p}})}
\end{equation}

The form of the score based on the natural properties that we can expect from score that based on word-to-word alignments.
First of all, the more aligned words between source entity and target candidates the higher should be a score.
Secondly, we can not divide number of aligned word by length of only source entity then
score will be higher for just longer target candidates.
And the third property is especially useful: if we consider two candidates where one is a
substring of other, but total number of aligned words between words of
source entity and these candidates are the same, we should prefer smaller candidate.
And the score \eqref{eq:align_cost} satisfies this requirement.
\begin{lemma} \label{lemma:align_cost_decrease}
    Suppose that for some pair of source entity \( \src{p} \in S \) and target candidate \( \tgt{p} \in T \)
    the score \eqref{eq:align_cost} equal to c, then if there is an extended by 1 word to left or
    right candidate \( \tgt{\hat{p}} \in T \) such as this word is not aligned to any word of source
    entity, the score \eqref{eq:align_cost} between source entity and this target candidate
    will be lower than \( c \).
\end{lemma}
\begin{proof}
    Consider extension to the right, then \( \tgt{\hat{p}} = (i_{\tgt{p}}, j_{\tgt{p}} + 1) \). Since it is
    given that this additional word is not aligned to any word of the source entity it means that:
    \[
        \sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} a_{k,j_{\tgt{p}} + 1} = 0
    \]
    Then
    \begin{multline*}
        c_{\src{p}, \tgt{\hat{p}}}^{align} =
        \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}} + 1} a_{kl}}
        {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{\hat{p}}} - i_{\tgt{\hat{p}}})} =                                \\
        \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
        {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} + 1 - i_{\tgt{p}})} +
        \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} a_{k,j_{\tgt{p}} + 1}}
        {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} + 1 - i_{\tgt{p}})} =                                        \\
        \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
        {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} - i_{\tgt{p}}) + 1} <
        \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
        {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} - i_{\tgt{p}})} = c
    \end{multline*}
    The derivation for the extension to the left where \( \tgt{\hat{p}} = (i_{\tgt{p}} - 1, j_{\tgt{p}}) \) is structurally the same, so we
    proved the lemma.
\end{proof}

\begin{corollary} \label{col:shrink_cand}
    The cost of the extended candidate \( \tgt{p_{+n}} \) of the target candidate \( \tgt{p} \) to the left or right with
    \( n \in \mathbb{N} \) non-aligned to any word of the source entity \( \src{p} \in S \) words is lower than
    the cost of the original candidate.
\end{corollary}
\begin{proof} Let's prove it by the mathematical induction for the case of extension to the right (for the left it will the same).

    \textit{Base:} By the lemma \ref{lemma:align_cost_decrease} \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+1}}}^{align} \).

    \textit{Induction step:} Assume that for some \( k \in \mathbb{N} \) holds \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+k}}}^{align} \).
    Then by the lemma~\ref{lemma:align_cost_decrease} \( c_{\src{p}, \tgt{p_{+k}}}^{align} > c_{\src{p}, \tgt{p_{+(k+1)}}}^{align} \) and therefore
    \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+(k+1)}}}^{align} \).

    Thus, by the mathematical induction, \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+n}}}^{align}   \)

\end{proof}

The main advantages of this property is that it is possible to shrink the set of target candidates without losing
optimal solutions: we can consider as candidates only subranges that are between the leftmost and rightmost words that are aligned to any
word of any source entity.
\begin{theorem}
    Let \( m, M \in \mathbb{N} \) be indices of a leftmost and rightmost aligned word to any source entity, then there is
    an optimal solution for the problem \eqref{eq:ilp} where constraints \eqref{eq:num_proj_const} has a form
    of \( < \) or \( \leq \), such that it projects source entity to a target candidate from the set \( T \) generated by the algorithm
    \ref{alg:ngram_extraction} where any word's index is inside the range \( [m, M] \).
\end{theorem}
\begin{proof}
    Suppose that there is no such an optimal solution that satisfies this requirement. It means that there exists
    a source entity \( \src{p} \in S \) and a source candidate
    \( \tgt{\hat{p}} \in S \) such that \( (i_{\tgt{\hat{p}}} < m) \lor (M < j_{\tgt{\hat{p}} < m}) \)
    and \( x_{\src{p}, \tgt{\hat{p}}} = 1 \) for an optimal solution \( x \).

    This case can be divided into two parts: when \( \tgt{\hat{p}} \) has no words that aligned to words of source entities or
    is and vice versa.

    Consider the first option, then since \( \tgt{\hat{p}}  \) consists of words that are not aligned to any word
    of source entities the alignment-based matching cost according to the definition \eqref{eq:align_cost} equals to \( 0 \).
    Then if we take such a solution \( x^* \) that it is equal to this optimal solution \( x \) everywhere except
    for the target candidate \( \tgt{\hat{p}}  \), i. e. \( x_{\src{p}, \tgt{\hat{p}}} = 0 \). Thus the objective function
    remains the same because cost of matching is zero. And we don't violate non-overlapping constraints~\eqref{eq:non_overlap_const}:
    \begin{align*}
         & \forall \src{p_1} \in S, \tgt{p_1} \in T \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{\hat{p}}) \in \hat{\Pi}(S, T) \\
         & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p}, \tgt{\hat{p}}} =
        x_{\src{p_1}, \tgt{p_1}} + 1 \leq 1 \implies                                                                         \\
         & x^*_{\src{p_1}, \tgt{p_1}} + x^*_{\src{p}, \tgt{\hat{p}}} \leq 1 =
        x_{\src{p_1}, \tgt{p_1}} + 0 \leq 1
    \end{align*}
    as well as constraints \eqref{eq:num_proj_const}:
    \begin{multline*}
        \sum\limits_{t \in T} x_{\src{p}, t} \leq n_{proj} \implies                             \\
        \sum\limits_{t \in T} x*_{\src{p}, t} =
        \sum\limits_{t \in T \setminus \{ \tgt{p} \}} x_{\src{p}, t} + x^*_{\src{p}, \tgt{p}} =
        \sum\limits_{t \in T \setminus \{ \tgt{p} \}} x_{\src{p}, t} + 0 \leq n_{proj}
    \end{multline*}
    Therefore such a solution \( x^* \) is optimal as well as original solution \( x \), but
    target candidates have indices in the range \( [m, M] \).

    When it comes to the second option where a target candidate \( \tgt{\hat{p}} \) overlaps with some
    candidates from the shrunk set in a way that there is some aligned word between a source entity \( \src{p} \) and these target candidate, i.e.
    \( \left[ (i_{\tgt{\hat{p}}} < m) \lor (M < j_{\tgt{\hat{p}} < m}) \right] \land
    \left[ (i_{\tgt{\hat{p}}} \leq M) \lor m \leq j_{\tgt{\hat{p}}} \right] \).
    By the corollary \ref{col:shrink_cand} such a candidate \( \tgt{p} = ( \max(i_{\tgt{\hat{p}}}, m), \min(M, j_{\tgt{\hat{p}}}) ) \) will have
    higher aligned-based matching score.

    Let's consider a solution \( x^* \) that equals
    to a solution \( x \) everywhere except for target candidates \( \tgt{\hat{p}}  \) and \(  \tgt{p} \),
    i. e. \( x^*_{\src{p}, \tgt{\hat{p}}} = 0, x^*_{\src{p}, \tgt{p}} = 1 \). This solution also satisfies all constraints
    of the ILP problem. For constraints \eqref{eq:num_proj_const} the proof is the following:
    \begin{align*}
         & \sum\limits_{t \in T} x_{\src{p}, t} =
        \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + x_{\src{p}, \tgt{p}} + x_{\src{p}, \tgt{\hat{p}}}  =     \\
         & \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + 0 + 1
        \leq n_{proj} \implies                                                                                                                 \\
         & \sum\limits_{t \in T} x*_{\src{p}, t} =
        \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + x^*_{\src{p}, \tgt{p}} + x^*_{\src{p}, \tgt{\hat{p}}}  = \\
         & \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + 1 + 0 \leq n_{proj}
    \end{align*}
    And for non-overlapping constraints, since \( \tgt{p} \) is a substring of \( \tgt{\hat{p}} \) by a
    construction we have:
    \[
        \forall t \in T \quad t \cap \tgt{p} \neq \emptyset \implies t \cap \tgt{\hat{p}} \neq \emptyset
    \]
    It gets us that all constraints that should be hold for \( \tgt{p} \) should also be satisfied for \( \tgt{\hat{p}} \):
    \begin{equation} \label{eq:overlap_substring}
        \left\{ (\src{p_1}, \tgt{p_1}) \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{p}) \in \hat{\Pi}(S, T) \right\} \subset
        \left\{ (\src{p_1}, \tgt{p_1}) \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{\hat{p}}) \in \hat{\Pi}(S, T) \right\}
    \end{equation}
    Suppose that constraints \eqref{eq:non_overlap_const} are violated for the solution \( x^* \), then
    \begin{align*}
         & \exists \src{p_1} \in S, \tgt{p_1} \in T \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{p}) \in \hat{\Pi}(S, T) \\
         & x^*_{\src{p_1}, \tgt{p_1}} + x^*_{\src{p}, \tgt{p}} =
        x_{\src{p_1}, \tgt{p_1}} + 1 > 1 \implies                                                                      \\
         & x_{\src{p_1}, \tgt{p_1}} = 1
        \implies                                                                                                       \\
         & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p}, \tgt{\hat{p}}} = 1 + 1 = 2 > 1
    \end{align*}
    But because of \eqref{eq:overlap_substring}
    \( (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{\hat{p}} ) \in \hat{\Pi}(S, T) \),
    therefore it contradicts that the solution \( x \) was feasible.
    Hence constraints \eqref{eq:non_overlap_const} are satisfied by the solution \( x^* \).

    And in the same time the solution \( x^* \) has a higher objective value that implies that the original solution \( x \) was not optimal.
    This contradicts our assumption that there is no such an optimal solution that ensures requirements of the theorem.
\end{proof}
Note that the theorem holds only for constraints \eqref{eq:num_proj_const} with a form of \( < \) or
\( \leq \) just because the ILP problem can lost feasibility if we remove projections with zero scores that
exist only to ensure these constraints and has no application sense.

In comparison to the heuristic algorithm that uses word-to-word alignment the formulation of the optimization
problem with matching scores \eqref{eq:align_cost} has an important advantage: we can evaluate the confidence of
every particular projection, since we have a score of it, whereas heuristics provide us only with labelling of the target
sentence. In the same time the prediction of this formulation and heuristics are not always the same, e.g.
the heuristic algorithm merges every continuous ranges of aligned words separated but at most some fixed number of non-aligned words,
but for the ILP formulation it will be so only for some specific cases.

Also it worth to mention that for the alignment-based score there is no fixed upper bound, since
the maximum value of the denominator is a product of length of a source entity and a target candidate.
It is possible to overcome it if we exchange one of the summation operation in the denominator from the
equation \eqref{eq:align_cost} with a logical OR. Nevertheless, during experiments we will use the
original form of the alignment-based matching score.

\subsection{NER model-based score}

An other way to evaluate matching scores between source entities and target candidates is to
use a multilingual NER model. The idea is the following: a model for every word of the target candidate
predicts probability distribution over a set of classes that a word has a specific label. In order to compute
a matching score that a source entity with a class \( l \) should be projected onto a target candidate,
we compute an average probability of the class \( l \) over all words of the candidate.

Let \( L \) be a set of classes. Since a NER model predicts output in the IOB format, i.e. the first word of a predicted
entity with class \( l \in L \) has a label \textit{B-l} and other words of this entity have labels \textit{I-l},
the output of a model can be represented as a matrix \( p_{m, o} \) where \( m \in \mathbb{N} \) is an index of a word and
\( o \in \{ 1, 2|L| + 1 \} \) is an index of a label (assume that the label \( O \), i. e. a word doesn't belong to any class,
has an index \( 2|L| + 1 \)).

Then let's introduce mappings \( B[l]: L \rightarrow \{ 1, 2|L| \} \) and
\( I[l]: L \rightarrow \{ 1, 2|L| \} \) that returns index of B and I labels respectively
for a class \( l \in L \) in a probability matrix.

So the NER model-based matching score is the following:
\begin{equation} \label{eq:ner_cost}
    c_{\src{p}, \tgt{p}}^{ner} = \alpha^{(j_{\tgt{p}} - i_{\tgt{p}}) - 1}
    \frac{
        p_{i_{\tgt{p}}, B[l_{\src{p}}]} +
        \sum\limits_{k = i_{\tgt{p}} + 1}^{j_{\tgt{p}}} p_{k, I[l_{\src{p}}]}
    }
    {j_\tgt{p} - i_{\tgt{p}}}
\end{equation}
where \( l_{\src{p}} \in L \) is a class of the source entity \( \src{p} \in S \) and
\( \alpha > 0 \in \mathbb{R} \) is a length-scaling constant.

The factor \( \alpha \) is necessary to align matching scores with NER model predictions.
We naturally expect from the matching cost that if a target candidate is a substring of a predicted
by a NER model entity, then its score should be lower that a score of the target candidate that
equal to the predicted entity. For example, let the set of classes contains only class \textit{PER}: \( L = \{ PER \}\),
set of source entities consists of only one entity \( \src{p} \) with a class \textit{PER},
set of target candidates be \( T =  \{ \tgt{p_1} = (1, 1), \tgt{p_2} = (1, 2) \} \) and a probability matrix predicted by the model is:
\[
    \begin{blockarray}{cccc}
        & \text{\textit{B-PER}} & \text{\textit{I-PER}} & \text{\textit{O}} \\
        \begin{block}{c(ccc)}
            1 & 0.9 & 0.1 & 0 \\
            2 & 0.2 & 0.8 & 0 \\
            3 & 0   & 0   & 1 \\
        \end{block}
    \end{blockarray}
\]
By taking maximum over rows it turns out that the entity predicted by a model has a class \textit{PER} and
consists of words with indices 1 and 2. But the NER model-based matching score without scaling factor
\( \alpha \) gives the following results:
\[
    c_{\src{p}, \tgt{p_1}}^{ner} = 0.9 \qquad c_{\src{p}, \tgt{p_2}}^{ner} = 0.85
\]
And in this case the ILP problem will prefer projecting source entity onto a substring of the entity predicted
by the model. Therefore, length-scaling constant \( \alpha \) can not be omitted.

If \( \alpha \) is a crucial part of the score then it make sense to evaluate the range of its values.
For this let's notice the following fact.
\begin{lemma} \label{lemma:maxprob_greater_than_avg}
    Let \( p_{i}, i \in K \) be a probability distribution over a finite set \( K \) with cardinality \( k \).
    Then
    \[
        \max\limits_{i \in K} p_i \geq \frac{1}{k}
    \]
\end{lemma}
\begin{proof}
    Suppose that it is wrong, i.e.
    \[
        \max\limits_{i \in K} p_i < \frac{1}{k}
    \]
    Then let's sum up all probabilities:
    \[
        \sum\limits_{i \in K} p_i \leq \sum\limits_{i \in K} \max\limits_{j \in K} p_j =
        k \cdot \max\limits_{j \in K} p_j < 1
    \]
    But the sum of all probabilities of the distribution should be equal to \( 1 \), therefore our assumption was
    wrong that proofs the lemma.
\end{proof}
Having this useful fact let's finally make some estimation of the constant \( \alpha \).
\begin{theorem}

\end{theorem}
\begin{proof} Let's write down a desired inequality
    \begin{align*}
         & c_{\src{p}, \tgt{p_2}}^{ner} > c_{\src{p}, \tgt{p_1}}^{ner} \implies \\
         & \alpha^{n}
        \frac{
            p_{i, B[l_{\src{p}}]} +
            \sum\limits_{k = i + 1}^{j + 1} p_{k, I[l_{\src{p}}]}
        }
        {n + 1} >
        \alpha^{n - 1}
        \frac{
            p_{i, B[l_{\src{p}}]} +
            \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]}
        }
        {n}
        \implies                                                                \\
         & \alpha >
        \frac
        {(n+1) \cdot \left( p_{i, B[l_{\src{p}}]} +
            \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} \right)}
        {
            n \cdot \left( p_{i, B[l_{\src{p}}]} +
            \sum\limits_{k = i + 1}^{j + 1} p_{k, I[l_{\src{p}}]} \right)}
        =
        \frac
        {(n+1) \cdot \left( p_{i, B[l_{\src{p}}]} +
            \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} \right)}
        {
            n \cdot \left( p_{i, B[l_{\src{p}}]} +
            \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} + p_{j+1, I[l_{\src{p}}]} \right)}
    \end{align*}
    In order to simplify derivation we will introduce aliases for some quantities:
    \[
        S = p_{i, B[l_{\src{p}}]}  + \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} \qquad
        s = p_{j+1, I[l_{\src{p}}]} \qquad
        M = \frac{1}{2|L| + 1}
    \]
    Note, that by the lemma \ref{lemma:maxprob_greater_than_avg} the following
    inequalities holds:
    \begin{equation} \label{eq:ner_sum_ineq}
        M \leq s \leq 1 \qquad nM \leq S \leq n
    \end{equation}
    Then we have:
    \begin{equation*}
        \alpha > \frac{(n + 1) S}{n (S + s)} =
        \frac{nS + S + ns - ns}{nS + ns} =
        1 + \frac{S - ns}{n(S + s)}
    \end{equation*}
    Let's compute derivatives of this expression.
    \begin{equation*}
        \left( 1 + \frac{S - ns}{n(S + s)} \right)_s^{'} =
        \frac{-n^2S - nS}{n^2 (S + s)^2} < 0
        \qquad
        \left( 1 + \frac{S - ns}{n(S + s)} \right)_S^{'}
        = \frac{nS + n^2s}{n^2 (S + s)^2} > 0
    \end{equation*}
    Hence the maximum of the function is located at the point
    \( s = M \) and \( S = n \), then we can take \( \alpha \) such that
    \begin{equation*}
        \alpha > 1 + \frac{1 - M}{1 + M}
        \stackrel{n \geq 1}{\geq}
        1 + \frac{n - nM}{n(n + M)}
        \stackrel{\max}{\geq}
        1 + \frac{S - ns}{n(S + s)}
    \end{equation*}
\end{proof}
As it is shown in the proof it is just an upper estimation of such an alpha for the worst possible case,
in practice there is a sense to try even smaller \( \alpha \), but then it doesn't guarantee that the
desired property are satisfied.

The main difference between a plain model transfer and XLNER pipeline with the ILP projection problem that uses NER model-based
matching cost is that during latter we project labels from source entities. Therefore some, possibly wrong, predictions of the
NER model could be ignored since in the solution of the ILP problem there is no source entity that projects
to this target entity.

One of the problem when it comes to using some model is the fact that models tend to be overconfident in their
predictions, i.e. outputs high probabilities for every, sometimes even wrong, prediction. And since the
proposed score directly uses these probabilities it can affects it. In order to overcome this issue the
calibration of the model should be applied. The simplest approach to do it is to scale output's logits of the
model by some temperature. Nevertheless, in further chapter we will use the NER model without any calibration.

The NER model-based matching score \eqref{eq:ner_cost} also has another property that is worth to mention.
Since the computation of the score involves only the class of the source entity, scores will be equal
for all source entities that have the same class. And from the point of view of the ILP problem
\eqref{eq:ilp} it can lead to a solution where a source entity is projected onto a semantically wrong candidate, but
nevertheless a correct class will be assigned to the target candidate.

\subsection{Translation-based score}


\begin{equation} \label{eq:nmt_cost}
    c_{\src{p}, \tgt{p}}^{nmt}
\end{equation}

\subsection{Fused score}
The performance of alignment, NER and translation models that were used for computation of
scores above varies depending on the language, domain, set of classes, etc.
Every matching score has their advantages and drawbacks, but the natural way to minimize fails is
to fuse scores together. Since all scores are real numbers one of the approaches to do it is to compute a
weighted sum of scores of all types:
\begin{equation} \label{eq:fused_cost}
    c_{\src{p}, \tgt{p}}^{fused} =
    \lambda_{align} c_{\src{p}, \tgt{p}}^{align} +
    \lambda_{ner} c_{\src{p}, \tgt{p}}^{ner} +
    \lambda_{nmt} c_{\src{p}, \tgt{p}}^{nmt}
\end{equation}
where \( \lambda_{align} \geq, \lambda_{ner} \geq 0, \lambda_{nmt} \geq 0 \in R\).

Moreover, the fused score can extend possibilities to handle issues of basic scores.
A NER model that is used in the NER-based cost \eqref{eq:ner_cost} sometimes can properly
predict spans of entities in the target sentence, but confuse classes.
We assume that labels are predicted correctly during computation of the NER model-based cost because
we have no way to correct them. But when the cost consists of different basic score, we can leverage a
NER model-based score only evaluate how likely a given candidate can form a target entity of any class and
determine label by using other types of scores. The modified \eqref{eq:ner_cost} that implements it
is the following:
\begin{equation} \label{eq:ner_cost_wo_classes}
    c_{\src{p}, \tgt{p}}^{ner} = \alpha^{(j_{\tgt{p}} - i_{\tgt{p}}) - 1}
    \max\limits_{l \in L}
    \frac{
        p_{i_{\tgt{p}}, B[l]} +
        \sum\limits_{k = i_{\tgt{p}} + 1}^{j_{\tgt{p}}} p_{k, I[l]}
    }
    {j_\tgt{p} - i_{\tgt{p}}}
\end{equation}

\section{Analysis of the ILP problem}
Despite the fact that the proposed ILP problem is just a particular case
of well-studied the general binary programming problem it doesn't imply that
it inherits all properties of it. For example, maximum bipartite matching problem
can be formulated as an instance of the ILP problem, but still can be solved in a
polynomial time, whereas general case of ILP is NP-hard. That motivates a deeper analysis
of the proposed formulation.

\subsection{Complexity}
Constraints, especially \eqref{eq:non_overlap_const}, play a crucial role in a complexity
of the proposed problem. In the case if all candidates are nor overlapping and constraints
\eqref{eq:num_proj_const} has a form of equalities with \( n_{proj} = 1 \) the problem reduces to
an instance of the weighted bipartite matching problem, that is in complexity class \( P \). But let's study
more general case.

For the sake of convenience we will use the first form \eqref{eq:objective}--\eqref{eq:binary_prog}
of the ILP problem in this section, but since it equivalent to the \eqref{eq:ilp} it doesn't affect
the results of the analysis.

The usual way to prove the complexity of some problem is to reduce other problem with known complexity to
an instance of the studied one. Thus let's consider the maximum independent set problem \cite{pemmaraju2003computational}.

\begin{figure}[ht]
    \begin{subfigure}{.5\textwidth}
        \centering
        \begin{tikzpicture}[every node/.style = {draw, circle}]
            \node[fill=green!10] (1) {1};
            \node[right=of 1] (2) {2};
            \node[below right=of 2, fill=green!10] (3) {3};
            \node[below=of 1, fill=green!10] (4) {4};
            \node[left=of 4] (5) {5};
            \node[below right=of 4] (6) {6};

            \graph{
                (1) -- (2) -- (4) -- (6) -- (5) -- (4),
                (6) -- (2) -- (3)
            };
        \end{tikzpicture}
        \caption{Maximum independent set problem (MaxIS). Vertices that form an optimal solution are colored in \textbf{\textcolor{green!50}{green}}}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
        \centering
        \begin{tikzpicture}[
                node distance=0.1,
                realstate/.style = {draw, circle, font=\scriptsize},
                dummystate/.style = {draw, circle, minimum width=4, inner sep=6, fill=blue!10}
            ]
            \node[dummystate] (src_1) {};
            \node[right=of src_1, xshift=10, dummystate] (src_2) {};
            \node[right=of src_2, xshift=10, dummystate] (src_3) {};

            \node[below=of src_1, fill=green!10, xshift=-50, yshift=-1.75cm, realstate] (1) {1};
            \node[right=of 1, realstate] (2) {2};
            \node[right=of 2, fill=green!10, realstate] (3) {3};
            \node[right=of 3, fill=green!10, realstate] (4) {4};
            \node[right=of 4, realstate] (5) {5};
            \node[right=of 5, realstate] (6) {6};
            \node[right=of 6, dummystate](dummy_1) {};
            \node[right=of dummy_1] (dots) {\dots};
            \node[right=of dots, dummystate] (dummy_2) {};

            \node[above=of src_2, text=gray, font=\scriptsize] {Source nodes};
            \node[below=of 5, text=gray, font=\scriptsize] {Target nodes};

            \draw[green] (src_1) -- (1);
            \draw (src_1) -- node[above left, xshift=3, rotate=60, black, font=\footnotesize]{c=1} (2);
            \draw (src_1) -- (3);
            \draw (src_1) -- (4);
            \draw (src_1) -- (5);
            \draw (src_1) -- (6);
            \draw[gray!40] (src_1) -- (dummy_1);
            \draw[gray!40] (src_1) -- (dummy_2);
            \draw (src_2) -- (2);
            \draw[green] (src_2) -- (3);
            \draw (src_2) -- (4);
            \draw (src_2) -- (5);
            \draw (src_2) -- (6);
            \draw[gray!40] (src_2) -- (dummy_1);
            \draw[gray!40] (src_2) -- (dummy_2);
            \draw (src_3) -- (2);
            \draw (src_3) -- (3);
            \draw[green] (src_3) -- (4);
            \draw (src_3) -- (5);
            \draw (src_3) -- (6);
            \draw[gray!40] (src_3) -- (dummy_1);
            \draw[gray!40] (src_3) -- node[above right, black, font=\footnotesize]{c=0} (dummy_2);
        \end{tikzpicture}
        \caption{Reduction to the proposed ILP problem. Dummy nodes are colored in \textcolor{blue!60}{blue}}
    \end{subfigure}
    \caption{The diagram of the general idea of the reduction MaxIS problem to the proposed ILP problem}
    \label{fig:maxis_reduction}
\end{figure}

The maximum independent set problem is a problem of finding a subset of vertices of some undirected
graph with maximum cardinality, such that it doesn't contain any vertices connected by edges of the graph.
An example of the maximum independent set problem is depicted on the figure \ref{fig:maxis_reduction} (a).
The formal definition as an ILP problem is given by \ref{def:maxis}.
\begin{definition}[MaxIS] \label{def:maxis}
    Let \( G=(V, E), V \neq \emptyset \) be an undirected graph, then a maximum independent set problem for the graph \( G \) is the following:
    \begin{align*}
         & \max \sum\limits_{v \in V} x_v                               \\
         & x_u + x_v \leq 1               \qquad \forall \{u, v\} \in E \\
         & x_v \in \{0, 1\}
    \end{align*}
\end{definition}

The MaxIS problem is an NP-hard \cite{pemmaraju2003computational} problem since VertexCover can be reduced to an instance of
this problem. But what is even more interesting for us is that this problem looks very similar to the
our projection ILP problem, except the fact, that set of constraint \eqref{eq:num_proj_const} are
omitted and costs are equal to \( 1 \). Moreover, the relation whether two edges of the graph are adjacent
is not transitive as well as the relation of overlapping \ref{def:overlapping}! That's why we will try to reduce
the maximum independent set problem to an instance of the proposed ILP problem.

Idea of the reduction to the MIS (explanation) (without constraint with nproj)

We will consider that target nodes \( t_u, t_v \in T_{V} \) that correspond to graph's nodes \( u, v \in V \) overlap
if and only if there is an edge in the graph \( G \) between these nodes or \( u = v \).
\[
    t_u \cap t_v \neq \emptyset \Leftrightarrow
    \Biggl[
    \begin{array}{l}
        \{ u, v \} \in E \\
        u = v
    \end{array}
\]
Such a definition of overlapping allow us to match all properties, i.e. reflexivity, symmetricity and non transitivity,
of overlapping relation on word ranges in the initial definition \ref{def:overlapping}. The fact that we consider
overlapping nodes that correspond to the same vertex simply means that it is impossible to
add the same vertex in the independent set to time which is a property of the any set.

\begin{equation} \label{eq:reduction_without_nproj}
    \begin{aligned}
         & \max \sum\limits_{v \in V} x_v                          \\
         & x_u + x_v \leq 1               & \forall \{u, v\} \in E \\
         & x_v \in \{0, 1\}
    \end{aligned}
\end{equation}

\begin{table}[h]
    \begin{subtable}[t]{0.5\linewidth}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|c|c|}
            \hline
            \(<\)                                 & \( \leq \), \( = \)               & \(>\), \( \geq \) \\
            \hline
            \( \ceil{\frac{|V|}{n_{proj} - 1}} \) & \( \ceil{\frac{|V|}{n_{proj}}} \) & 1                 \\
            \hline
        \end{tabular}
        \caption{For source nodes}
        \label{tab:src_dummy_nodes_num}
    \end{subtable}
    \begin{subtable}[t]{0.5\linewidth}
        \centering
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{|c|c|c|c|}
            \hline
            \(<\), \(\leq\) & \( = \)                                    & \(>\)          & \( \geq \)         \\
            \hline
            \( 0 \)         & \( \ceil{\frac{|V|}{n_{proj}}} n_{proj} \) & \( n_{proj} \) & \( n_{proj} + 1 \) \\
            \hline
        \end{tabular}
        \caption{For target nodes}
        \label{tab:tgt_dummy_nodes_num}
    \end{subtable}
    \caption{Number of dummy nodes for every type of constraints \eqref{eq:num_proj_const}}
    \label{tab:dummy_nodes_num}
\end{table}

What is optimal solution and quotient set of solutions
\[
    T^*_{x} = \{ t \in T_V | \exists s \in S, x_{s, t} = 1 \}
\]

Let's \( x_1, x_2 \) be a two feasible solutions of problem \eqref{eq:reduction_without_nproj}, then we will say that
they are in relation~\( \sim \) if their corresponding solutions of the MaxIS problem are equal:
\[
    x_1 \sim x_2 \Leftrightarrow T^{*}_{x_1} = T^{*}_{x_2}
\]
Since this relation defined by an equality of sets it is reflexive, symmetric and transitive and therefore
equivalence relation.

Then quotient set \( \quot{X}{\sim} \) will consist of all equivalence classes of feasible solutions
that differ only in matching of dummy nodes. Let's notice that the objective function value is equal for all
elements of any equivalence class.
\begin{lemma}
    Cost inside equiv. class are constant
\end{lemma}
\begin{proof}
    Trust me!
\end{proof}

Thus we will call optimal an equivalence class with the highest objective value.

\begin{lemma}
    Let \( x \) be a feasible solution of the problem \eqref{eq:reduction_without_nproj}.
    Then a solution \( x^* \) such that \( t \in T \)
\end{lemma}
\begin{proof}
    Trust me!
\end{proof}

\begin{corollary}
    It is possible to bound sum over all candidates for one source entity
\end{corollary}

\begin{theorem}[]
    The proposed ILP problem \eqref{eq:objective}--\eqref{eq:binary_prog} is NP-hard
\end{theorem}
\begin{proof}
    Trust me!
\end{proof}


\subsection{Approaches to compute the solution of the problem}
As any ILP problem the problem \eqref{eq:ilp} can be solved by branch and bound, cutting planes,
branch and cut methods. Nevertheless it can take a lot of time to find and optimal solution and
it can make such formulation inefficient from the application point of view where it is required to
solve instances of this problem for thousands sentences in a limited time.

It motivates the necessity of the approximate algorithm that can compute a solution that is not always
optimal, but don't violate crucial constraints. One of the approach for such an algorithm for the problem
\eqref{eq:ilp} is to iteratively assign \( 1 \) to a variable with the highest matching score and then remove
all target candidates that overlap with the projected candidate to enforce non-overlapping constraints.
Algorithm \ref{alg:ilp_greedy} is the variant of a such greedy algorithm.

\begin{algorithm}
    \caption{Approximate greedy algorithm for the proposed ILP problem} \label{alg:ilp_greedy}
    \KwData{instance of the ILP problem \eqref{eq:ilp}}
    \KwResult{\( x \) -- "solution" of the ILP problem}

    \( x \gets 0 \) \;
    \( P \gets 0 \) \Comment*[r]{number of projections by source entity}
    \While{\( \exists \src{p} \in S, \tgt{p} \in T \Big| c_{\src{p}, \tgt{p}} > 0 \)}{
        \( s, t \gets \argmax\limits_{\src{p} \in S, \tgt{p} \in T } c_{\src{p}, \tgt{p}} \) \;
        \( x_{s,t} \gets 1 \) \;
        \( P_{s} \gets P_{s} + 1 \) \;

        \ForAll(\tcp*[f]{remove all overlapping with \( t \) candidates}){\( \hat{t} \in T \Big| \hat{t} \cap t \neq \emptyset \)}{
            \( c_{s, \hat{t}} \gets 0 \) \;
        }

        \Comment{ensure constraints \eqref{eq:num_proj_const}}
        \If{constraints \eqref{eq:num_proj_const} is a type of \( =, \leq \)}{
            \If{\( P_s = n_{proj} \)}{
                \ForAll{\( \hat{t} \in T \)}{
                    \( c_{s, \hat{t}} \gets 0 \) \;
                }
            }
        }
        \If{constraints \eqref{eq:num_proj_const} is a type of \( < \)}
        {
            \If{\( P_s = n_{proj} - 1 \)}{
                \ForAll{\( \hat{t} \in T \)}{
                    \( c_{s, \hat{t}} \gets 0 \) \;
                }
            }
        }
    }
\end{algorithm}

From the steps of the algorithm \ref{alg:ilp_greedy} it implies that after removing all
overlapping target candidates by making their costs equal to zero the algorithm ensures the
constants \eqref{eq:num_proj_const} on a number of projected from every source entity candidates.
But only for the case where constraints have a form of \( <, \leq, = \).

In the case \( >, \geq \) the algorithm will output a "solution" that violates constraint
\eqref{eq:num_proj_const} only in two cases. The first one is when there is a source entity for which
all target candidates, that are not overlapping with already projected ones, initially have zero matching cost. But then,
from the application point of view it doesn't make sense to project the source entity onto these candidates
since it is definitely not a counterpart of the source entity in the target sentence, otherwise it would not
have a matching cost equal to \( 0 \). The second option is the situation where target candidates
that could potentially be projected onto have been removed on the previous iterations of the algorithm.
And it is hard to handle this issue without backtracking and losing feasibility on
non-overlapping constraints \eqref{eq:non_overlap_const}.

Nevertheless, the algorithm \ref{alg:ilp_greedy} is linear on a number of variables and therefore can
solve the ILP problem significantly faster than the exact ILP solver in a general case.
