\chapter{Methodology}
\label{sec:methodology}
The overall idea of formalizing the projection step of the XLNER pipeline is
to formulate it as a matching process between source entities and ranges
of words in the target sentence, referred to as target candidates. This process is
illustrated in Figure \ref{fig:cand_matching}. Whereas a predetermined set of source entities
for projection is provided; a subset of continuous word ranges from the original
sentence in the target language can be selected and considered as potential
projections for source entities. The likelihood scores that each source entity
should be projected onto each candidate will be computed. The goal is to identify a
combination of source entities and target candidates that maximizes the overall
sum of the corresponding scores. However, natural constraints must also be taken into account.
For instance, when two source entities project onto overlapping candidates, it becomes
ambiguous to which source entity the overlapping words pertain. Therefore, projections
onto overlapping candidates should be prohibited. Furthermore, it is logical to
restrict the number of candidates to which each source entity can be projected,
as a one-to-one correspondence between source entities and entities in the
target sentence is typically anticipated. Based on these fundamental properties,
we can establish an integer linear optimization problem.

\begin{figure*}[ht]
  \centering
  \begin{tikzpicture}[node distance=-0.1,
      every node/.style={text centered,
        text height=2ex,
        text depth=.25ex,
      },
      loc/.style={fill=orange!30, rounded rectangle, label={[anchor=center,font=\tiny\bfseries\sffamily]above:#1-LOC}},
      per/.style={fill=green!30, rounded rectangle, label={[anchor=center,font=\tiny\bfseries\sffamily]above:#1-PER}},
    cand/.style={fill=blue!30, rounded rectangle},]

    \node[per={B}, rounded rectangle east arc=none](George_src){George};
    \node[per={I}, rounded rectangle west arc=none, right=of George_src](Washington_src){Washington};
    \node[right=of Washington_src](is_src){is};
    \node[right=of is_src](the_src){the};
    \node[right=of the_src](first_src){first};
    \node[right=of first_src](president_src){president};
    \node[right=of president_src](of_src){of};
    \node[right=of of_src](the_src){the};
    \node[loc={B}, rounded rectangle east arc=none, right=of the_src](United_src){United};
    \node[loc={I}, rounded rectangle west arc=none, right=of United_src](States_src){States};

    \node[cand, rounded rectangle east arc=none, below=of George_src, yshift=-1.5cm](George_tgt){George};
    \node[cand, rounded rectangle west arc=none, right=of George_tgt](Washington_tgt){Washington};
    \node[right=of Washington_tgt](ist_tgt){ist};
    \node[right=of ist_tgt](der_tgt){der};
    \node[right=of der_tgt](erste_tgt){erste};
    \node[right=of erste_tgt](president_tgt){Präsident};
    \node[right=of president_tgt](der_tgt){der};
    \node[cand, rounded rectangle east arc=none, right=of der_tgt](Vereinigten_tgt){Vereinigten};
    \node[cand, rounded rectangle west arc=none, right=of Vereinigten_tgt](Staaten_tgt){Staaten};

    \node[text=gray, font=\scriptsize, above=of first_src, yshift=0.2cm, xshift=0.2cm](source){Source labeled sentence};
    \node[text=gray, font=\scriptsize, below=of source, yshift=-3cm]{Original sentence with extracted candidates};

    \draw[->] (George_src.south east) -- node[left]{\(c_{11}\)} (George_tgt.north east);
    \draw[->] (United_src.south east) -- node[right]{\(c_{22}\)} (Vereinigten_tgt.north east);
    \draw[->] (George_src.south east) -- node[above left, yshift=0.1cm, xshift=-0.2cm]{\(c_{12}\)} (Vereinigten_tgt.north east);
    \draw[->] (United_src.south east) -- node[below left]{\(c_{21}\)} (George_tgt.north east);
  \end{tikzpicture}
  \caption{Illustration of the proposed idea of matching source entities and candidates in the target sentence}
  \label{fig:cand_matching}
\end{figure*}

\section{Formulation of the ILP problem}
First and foremost, a definition of overlapping candidate should be established.

\begin{definition}[Relation of overlapping] \label{def:overlapping}
  Target candidates \( \tgt{p_1} = (i_{\tgt{p_1}}, j_{\tgt{p_1}}) \) and \linebreak
  \( \tgt{p_2} = (i_{\tgt{p_2}}, j_{\tgt{p_2}}) \in T \) are considered to overlap
  if and only if \( ( i_{\tgt{p_1}} \leq j_{\tgt{p_2}} ) \land ( i_{\tgt{p_2}} \leq j_{\tgt{p_1}} ) \),
  which indicates that the sets of word indices are not disjoint. Overlapping candidates
  will be denoted using the following notation: \( \tgt{p_1} \cap \tgt{p_2} \neq \emptyset \).
\end{definition}

Having defined all necessary objects, the ILP problem for the projection step of the XLNER pipeline that
aligns with our requirements can now be formulated.
Let \( S \) be a set of source entities, \( T \) -- set of target candidates and \( \cap \subset T^2 \) is
a relation of overlapping, then the projection ILP problem is the following:
\begin{align}
  \label{eq:objective}
  & \max\limits_x \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}}                                             \\
  & \text{subject to} \notag                                                                                                                            \\
  \label{eq:num_proj_const}
  & \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} \lessgtr n_{proj}                                      & \forall \src{p} \in S                     \\
  \label{eq:non_overlap_const}
  & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1
  & \forall (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi}(S, T)                                                                            \\
  \label{eq:binary_prog}
  & x_{\src{p}, \tgt{p}} \in \{ 0, 1 \}                                                                     & \forall (\src{p}, \tgt{p}) \in S \times T
\end{align}
where \( \hat{\Pi}(S, T) \) represents a set of combinations of
source entities and target candidates that cannot be projected onto
together due to overlapping. This set is defined as follows:
\begin{equation}  \label{eq:overlapping_set}
  \begin{aligned}
    \hat{\Pi}(S, T) = \Big\{ (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \Big| \src{p_1}, \src{p_2} \in S, \tgt{p_1}, \tgt{p_2} \in T, \quad \tgt{p_1} \cap \tgt{p_2} \neq \emptyset, \\
    (\src{p_1} \neq \src{p_2}) \lor (\tgt{p_1} \neq \tgt{p_2}) \Big\}
  \end{aligned}
\end{equation}

Here, each variable \( x_{\src{p}, \tgt{p}} \) indicates whether
the source entity \( \src{p} \in S \) is projected onto the target
candidates \( \tgt{p} \in T \). The set of constraints \eqref{eq:non_overlap_const}
ensures that it is impossible for one or more source entities to be
projected onto overlapping target candidates. Additionally, another
set of constraints \eqref{eq:num_proj_const} limits the number of
projections for each source entity. It should be noted that the
inequality sign is not fixed, as various forms may be relevant: "less,"
"less than or equal," and "equal" can be used to ensure that a source entity
is not projected onto as many available candidates as possible solely to increase
the objective value. Conversely, "greater" or "greater than or equal" can be
employed to enforce that a source entity will be projected at least a specified
number of times, e.g., once, ensuring that the solver does not simply ignore it.
It is evident that \( n_{proj} = 0 \) represents a corner case that typically
lacks meaning, except in the case of a "greater" or "greater or equal" inequality, which effectively
eliminates any limits and is equivalent to having no constraints of this type.

However, the number of constraints \eqref{eq:non_overlap_const} scales as
\( \Theta(n^3 m^2) \) where \( m \) is the the number of source entities and
\( n \) is the length of the target sentence, assuming that all possible continuous subranges are utilized as candidates.
This considerable growth in constraints can complicate the problem-solving process.
Consequently, it is essential to explore methods to reduce the number of such
constraints. To begin, let us examine the overlapping relation that forms the
basis of this issue.
\begin{lemma} \label{lemma:not_transitive}
  The relation of overlapping is not transitive.
\end{lemma}
\begin{proof}
  This will be demonstrated by providing a counterexample.
  Assume the following target candidates:
  \[
    a = (2, 4) \quad b = (1, 2) \quad c = (4, 5)
  \]
  By the definition of the overlapping relation \( a \cap b \neq \emptyset \) and
  \( a \cap c \neq \emptyset \) but \( b \) and \( c \) are not overlapping.
\end{proof}

This fact leads to the conclusion that it is impossible to partition all target
candidates into groups of mutually overlapping candidates and therefore select at most
one candidate from each group to match with a source entity.
% \begin{corollary}
%   It is impossible to partition set of target candidates such that every partition consists of
%   candidates that are pairwise overlapping with each other and there are no overlapping candidates
%   that are in different partitions.
% \end{corollary}
% \begin{proof}
%   By Lemma \ref{lemma:not_transitive} the overlapping relation is not transitive and therefore
%   is not an equivalence relation that implies we can not construct a quotient set.
% \end{proof}

Nevertheless, it is possible to reduce the number of constraints
\eqref{eq:non_overlap_const}. The underlying idea for this reduction is that
if it is impossible to project one or any two source entities onto overlapping
candidates, then it is also impossible to project any number of source entities
onto these candidates. Consequently, it suffices to sum the constraints across
all source entities. This approach is feasible because all variables
\( x_{\src{p}, \tgt{p}} \) are binary, and therefore, they cannot be
negative.
\begin{theorem}
  The set of constraints \eqref{eq:non_overlap_const} is satisfied if and only if
  the following sets of reduced constraints are satisfied:
  \begin{align*}
    & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) \leq 1                                & \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T) \\
    & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1
    & \forall \tgt{p} \in T \Big| \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset                                             \\
  \end{align*}
  where
  \begin{equation*}
    \Pi(T) = \left\{ (\tgt{p_1}, \tgt{p_2}) \Big| \tgt{p_1}, \tgt{p_2} \in T,
      \quad \tgt{p_1} \cap \tgt{p_2} \neq \emptyset,
    \tgt{p_1} \neq \tgt{p_2} \right\}
  \end{equation*}
\end{theorem}
\begin{proof}
  \textit{Necessity:} Assume that constraints \eqref{eq:non_overlap_const} are satisfied, but
  the proposed constraints are not. Then there are two options:
  \[
    \exists (\tgt{p_1}, \tgt{p_2}) \in \Pi(T) \Bigg| \quad
    \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) > 1
  \]
  or
  \[
    \exists \tgt{p} \in T \Big|
    \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
    \Bigg| \quad \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} > 1
  \]

  Consider the first option, then
  \begin{align*}
    \exists \src{p_1}, \src{p_2} \in S \Big| x_{\src{p_1}, \tgt{p_1}} = 1, x_{\src{p_2}, \tgt{p_2}} = 1 \implies
    x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} = 2 > 1 \\
  \end{align*}
  Or
  \begin{align*}
    & \exists \src{p_1}, \src{p_2} \in S, \src{p_1} \neq \src{p_2}, \; \tgt{p} \in \{\tgt{p_1}, \tgt{p_2}  \} \Big| x_{\src{p_1}, \tgt{p}} = 1, x_{\src{p_2}, \tgt{p}} = 1 \implies \\
    & x_{\src{p_1}, \tgt{p}} + x_{\src{p_2}, \tgt{p}} = 2 > 1
  \end{align*}
  This situation contradicts our initial assumption that the constraints \eqref{eq:non_overlap_const} are satisfied.

  The same result we can obtain for the second case:
  \begin{align*}
    & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} > 1
    \stackrel{\eqref{eq:binary_prog}}{\implies}                                                                                           \\
    & \exists \src{p_1}, \src{p_2} \in S, \src{p_1} \neq \src{p_2} \Big| x_{\src{p_1}, \tgt{p}} = 1, x_{\src{p_2}, \tgt{p}} = 1 \implies \\
    & x_{\src{p_1}, \tgt{p}} + x_{\src{p_2}, \tgt{p}} = 2 > 1
  \end{align*}

  Thus, the necessity has been established.

  \textit{Sufficiency:}
  Suppose that the following constraints are satisfied:
  \[
    \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) \leq 1
    \qquad \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T)
  \]
  Therefore, the sum can take on values of \( 0 \) or \( 1 \).
  If the sum is zero, then no entities are projected onto any of the overlapping
  candidates, and consequently, the constraints \eqref{eq:non_overlap_const} are
  not violated. Now, let us consider the case when the sum is equal to \( 1 \).
  \begin{equation*} \label{eq:derivation_of_const_reduction}
    \begin{aligned}
      & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) = 1
      \stackrel{\eqref{eq:binary_prog}}{\implies}                                                       \\
      & \exists! \src{p} \in S, \exists! \tgt{p} \in \{ \tgt{p_1}, \tgt{p_2}\}
      \Bigg| x_{\src{p}, \tgt{p}} = 1 \implies                                                          \\
      & \forall \src{p_1}, \src{p_2} \in S, x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1
    \end{aligned}
  \end{equation*}
  Therefore we proved that
  \[
    x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1 \qquad
    \forall (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi'}(S, T)
  \]
  where
  \begin{align*}
    \hat{\Pi'}(S, T) = \Big\{ (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \Big| \src{p_1}, \src{p_2} \in S, \tgt{p_1}, \tgt{p_2} \in T, \quad \tgt{p_1} \cap \tgt{p_2} \neq \emptyset, \\
    \tgt{p_1} \neq \tgt{p_2} \Big\}
  \end{align*}

  But it is not the same as set \eqref{eq:overlapping_set}:
  \[
    \hat{\Pi}(S, T) \setminus \hat{\Pi'}(S, T) = \Big\{ (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \Big| \src{p_1}, \src{p_2} \in S, \tgt{p_1}, \tgt{p_2} \in T, \tgt{p_1} = \tgt{p_2},
    \src{p_1} \neq \src{p_2} \Big\}
  \]
  So we need to check whether constraints \eqref{eq:non_overlap_const} are satisfied on this set difference.

  First of all, let's notice that
  \begin{multline*}
    \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T)
    \qquad
    \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}})
    =
    \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p_1}} +
    \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p_2}} \leq 1
    \implies                        \\
    \forall \tgt{p} \in T \Bigg| \exists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
    \quad
    \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1
  \end{multline*}
  Therefore there exists at most one source entity \( \src{p} \in S \) that are projected
  to the target candidate \( \tgt{p} \).
  And using exactly the same derivation as above we get:
  \begin{multline*}
    \forall \tgt{p} \in T \Bigg| \exists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
    \quad
    \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1
    \stackrel{\eqref{eq:binary_prog}}{\implies}                                                                                                                                                       \\
    \begin{aligned}
      & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} = 0
      \stackrel{\eqref{eq:binary_prog}}{\implies}
      \forall \src{p} \in S, x_{\src{p}, \tgt{p}} = 0         \\
      \text{or}                                               \\
      & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} = 1
      \stackrel{\eqref{eq:binary_prog}}{\implies}
      \exists! \src{p} \in S, x_{\src{p}, \tgt{p}} = 1        \\
    \end{aligned} \implies \\
    x_{\src{p_1}, \tgt{p}} + x_{\src{p_2}, \tgt{p}} \leq 1 \qquad
    \begin{aligned}
      & \forall \src{p_1}, \src{p_1} \in S, \src{p_1} \neq \src{p_2}                                                        \\
      & \forall \tgt{p} \in T \Bigg| \exists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
    \end{aligned}
  \end{multline*}
  So, only the case, when entity has no overlapped entity that are
  not equal to it, remains. But this case is fully covered by the second part of the proposed reduced constraints:
  \[
    \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1 \qquad
    \forall \tgt{p} \in T \Big| \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset
  \]
  And by considering that this sum can be either \( 0 \) or \( 1 \) and making
  exactly the same derivation we conclude that it implies that
  constraints \eqref{eq:non_overlap_const} are satisfied.
\end{proof}

The theorem allows to reduce number of constraints, ensuring that they scale scale as a constant
with respect to the number of source entities. Consequently, we arrive at the following final
formulation of the ILP problem for the projection step of the XLNER pipeline.
\begin{equation} \label{eq:ilp}
  \begin{aligned}
    & \max\limits_x \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}}                                                                                                                       \\
    & \text{subject to}                                                                                                                                                                                                             \\
    & \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} \lessgtr n_{proj}                                      & \forall \src{p} \in S                                                                                               \\
    & \sum\limits_{\src{p} \in S} (x_{\src{p}, \tgt{p_1}} + x_{\src{p}, \tgt{p_2}}) \leq 1                    & \forall (\tgt{p_1}, \tgt{p_2}) \in \Pi(T)                                                                           \\
    & \sum\limits_{\src{p} \in S} x_{\src{p}, \tgt{p}} \leq 1                                                 & \forall \tgt{p} \in T \Big| \nexists \tgt{p_2} \in T: \tgt{p} \neq \tgt{p_2}, \tgt{p} \cap \tgt{p_2} \neq \emptyset \\
    & x_{\src{p}, \tgt{p}} \in \{ 0, 1 \}                                                                     & \forall (\src{p}, \tgt{p}) \in S \times T
  \end{aligned}
\end{equation}

\section{Candidates extraction}
Whereas the set \( S \) of source entities is given, as all source entities have been
extracted in the previous step of the XLNER pipeline, the construction of the set
\( T \) of target candidates remains an open question.

The simplest method for candidate extraction is to consider all possible n-grams
(continuous ranges of words) from the target sentence as candidates. This approach
guarantees that no actual target entity will be excluded from the set of target
candidates. However, from a computational perspective, this can pose a challenge,
as the number of all n-grams scales quadratically. Furthermore, from an application
standpoint, the majority of candidates generated in this manner are unlikely to
represent valid target entities.

One of the simplest strategies to address this problem is to limit the maximum
possible length of the candidates. From the perspective of Named Entity Recognition,
it is reasonable to expect that actual target entities typically do not exceed a
certain predefined length. For instance, it is unlikely for a person's name to consist
of more than 10 words, and if there are three source entities, it is unlikely
that there exists a single target entity that consists of all words of the target
sentence.

Thus, the algorithm for candidates extraction wilt a bounded maximum length of n-grams
is the following.
\begin{algorithm}
  \SetAlgorithmName{Algorithm}{}{}
  \KwData{\( n \in \mathbb{N} \) -- number of words in the target sentence,
  \(M \leq n \in \mathbb{N} \) -- maximum length of a target candidate}
  \KwResult{\( T \) -- set of target candidates}

  \( T \gets \emptyset \) \;
  \For{\(i \gets 0 \) \KwTo \( n \)}{
    \( m \gets \min(s + M, n) \) \;
    \For{\(j \gets i \) \KwTo \( m \)}{
      \( T \gets T \cup \{ (i, j) \} \) \;
    }
  }
  \caption{Bounded length n-gram candidates extraction}
  \label{alg:ngram_extraction}
\end{algorithm}

An alternative approach for candidate extraction is to employ a model to generate
candidates. In this regard, TProjection \cite{garcia-ferrero-etal-2023-projection} utilizes a fine-tuned T5 model with beam
search for candidate generation. In theory, any large language model can be
fine-tuned for this purpose. However, the primary drawback of this approach is
the necessity to fine-tune these models, which requires a labeled dataset in
the target language or relies on cross-lingual model transfer. In this context,
it may be more feasible and effective to train a model specifically to label target
entities rather than to generate candidates for the projection step of the
XLNER pipeline. Moreover, autoregressive models tend to be resource-intensive,
making it inefficient to utilize them solely for candidate extraction.

It also makes sense to consider a multilingual encoder-only Transformer \cite{vaswani2017attention}
model with high recall. However, these models still require training to extract candidates
effectively. In the following chapters, these alternatives will not be discussed
and will remain as topics for future work.

\section{Matching scores}
Thus far, the ILP problem \eqref{eq:ilp} has been formulated and discussed,
which involves projecting source entities onto target candidates using a matching
score \( c_{\src{p}, \tgt{p}} \). This score represents the likelihood that a given
source entity \( \src{p} \in S \) should be projected onto the corresponding target
candidate \( \tgt{p} \in T \). However, the question of how to compute all these
scores remains unresolved. In this section, various options for evaluating these
scores will be proposed.

\subsection{Alignment-based score}
In Chapter \ref{sec:background}, it was demonstrated that word-to-word alignments
can be utilized for the projection step of the XLNER pipeline. However this was
achieved through a heuristic algorithm. Nonetheless, an attempt can be made to
incorporate these alignments into the proposed ILP problem by calculating matching
scores using word-to-word alignments.

The matching score that inspired from this idea is the following:
\begin{equation} \label{eq:align_cost}
  c_{\src{p}, \tgt{p}}^{align} =
  \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
  {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} - i_{\tgt{p}})}
\end{equation}

The form of the score is grounded in the natural properties expected from a score
based on word-to-word alignments. First, the greater the number of aligned words
between the source entity and the target candidates, the higher the score should be.
Second, it is insufficient to simply divide the number of aligned words by the length
of the source entity, as this would lead to a higher score for longer target candidates.
Lastly, the third property is particularly beneficial: when considering two candidates,
where one is a substring of the other, if the total number of aligned words between the
source entity and these candidates is the same, preference should be given to the
smaller candidate. The score defined in equation \eqref{eq:align_cost} fulfills this
requirement.
\begin{lemma} \label{lemma:align_cost_decrease}
  Suppose that for a specific pair of source entity \( \src{p} \in S \) and
  target candidate \( \tgt{p} \in T \), the score defined in equation
  \eqref{eq:align_cost} is equal to \( c \). If there exists an extended
  candidate \( \tgt{\hat{p}} \in T \) that is increased by one word to the
  left or right, such that this additional word is not aligned with any word
  of the source entity, then the score \eqref{eq:align_cost} between the source
  entity and this target candidate will be lower than \( c \).
\end{lemma}
\begin{proof}
  Considering the extension to the right, let \( \tgt{\hat{p}} = (i_{\tgt{p}}, j_{\tgt{p}} + 1) \).
  Given that this additional word is not aligned with any word of the source entity,
  it implies that:
  \[
    \sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} a_{k,j_{\tgt{p}} + 1} = 0
  \]
  Then:
  \begin{multline*}
    c_{\src{p}, \tgt{\hat{p}}}^{align} =
    \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}} + 1} a_{kl}}
    {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{\hat{p}}} - i_{\tgt{\hat{p}}})} =                                \\
    \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
    {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} + 1 - i_{\tgt{p}})} +
    \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} a_{k,j_{\tgt{p}} + 1}}
    {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} + 1 - i_{\tgt{p}})} =                                        \\
    \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
    {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} - i_{\tgt{p}}) + 1} <
    \frac{\sum\limits_{k=i_{\src{p}}}^{j_{\src{p}}} \sum\limits_{l=i_{\tgt{p}}}^{j_{\tgt{p}}} a_{kl}}
    {(j_{\src{p}} - i_{\src{p}}) + (j_{\tgt{p}} - i_{\tgt{p}})} = c
  \end{multline*}
  The derivation for the extension to the left, where
  \( \tgt{\hat{p}} = (i_{\tgt{p}} - 1, j_{\tgt{p}}) \), is structurally analogous.
  Thus, the lemma has been proven.
\end{proof}

\begin{corollary} \label{col:shrink_cand}
  The cost of the extended candidate \( \tgt{p_{+n}} \) derived from the target
  candidate \( \tgt{p} \), when extended to the left or right by
  \( n \in \mathbb{N} \) non-aligned words that do not correspond to any word
  of the source entity \( \src{p} \in S \), is lower than the cost of the original
  candidate.
\end{corollary}
\begin{proof}
  Let us prove this statement using mathematical induction for the case of extension to the right (the proof for the left extension will follow a similar structure).

  \textit{Base:} By the lemma \ref{lemma:align_cost_decrease} \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+1}}}^{align} \).

  \textit{Induction step:} Assume that for some \( k \in \mathbb{N} \) holds \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+k}}}^{align} \).
  Then by the lemma~\ref{lemma:align_cost_decrease} \( c_{\src{p}, \tgt{p_{+k}}}^{align} > c_{\src{p}, \tgt{p_{+(k+1)}}}^{align} \) and therefore
  \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+(k+1)}}}^{align} \).

  Thus, by the mathematical induction, \( c_{\src{p}, \tgt{p}}^{align} > c_{\src{p}, \tgt{p_{+n}}}^{align}   \)
\end{proof}

The main advantage of this property is that it allows for the reduction of the set of
target candidates without compromising optimal solutions. Specifically, it permits
the consideration of candidates only from the subrange between the leftmost and
rightmost words aligned with any word of any source entity.
\begin{theorem} \label{thm:reduce_T_for_align}
  Let \( m, M \in \mathbb{N} \) denote the indices of the leftmost and rightmost
  aligned words corresponding to any source entity. Then there exists
  an optimal solution for the problem \eqref{eq:ilp} where the constraints
  \eqref{eq:num_proj_const} take the form of \( < \) or \( \leq \), such that
  all source entity are projected onto a target candidates from the set \( T \)
  generated by the algorithm \ref{alg:ngram_extraction}, where all word indices
  are contained within the range \( [m, M] \).
\end{theorem}
\begin{proof}
  Suppose that there is no optimal solution that satisfies this requirement.
  This implies the existence of a source entity \( \src{p} \in S \) and a target
  candidate \( \tgt{\hat{p}} \in T \) such that \( (i_{\tgt{\hat{p}}} < m) \lor (M < j_{\tgt{\hat{p}}}) \)
  and \( x_{\src{p}, \tgt{\hat{p}}} = 1 \) for an optimal solution \( x \).

  This case can be divided into two parts: one where \( \tgt{\hat{p}} \) contains
  no words aligned with any words of the source entities, and the other where it
  does have such alignments.

  Considering the first scenario, if \( \tgt{\hat{p}} \) consists entirely of
  words that are not aligned with any word from the source entities, then the
  alignment-based matching cost, according to the definition in \eqref{eq:align_cost},
  equals \( 0 \).

  In this situation, we can take a solution \( x^* \) that is identical to the
  optimal solution \( x \), except for the target candidate \( \tgt{\hat{p}} \);
  that is, we set \( x_{\src{p}, \tgt{\hat{p}}} = 0 \). Consequently, the objective
  function remains unchanged because the cost of matching is zero. Additionally,
  we do not violate the non-overlapping constraints \eqref{eq:non_overlap_const}:
  \begin{align*}
    & \forall \src{p_1} \in S, \tgt{p_1} \in T \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{\hat{p}}) \in \hat{\Pi}(S, T) \\
    & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p}, \tgt{\hat{p}}} =
    x_{\src{p_1}, \tgt{p_1}} + 1 \leq 1 \implies                                                                         \\
    & x^*_{\src{p_1}, \tgt{p_1}} + x^*_{\src{p}, \tgt{\hat{p}}} \leq 1 =
    x_{\src{p_1}, \tgt{p_1}} + 0 \leq 1
  \end{align*}
  As well as constraints \eqref{eq:num_proj_const}:
  \begin{multline*}
    \sum\limits_{t \in T} x_{\src{p}, t} \leq n_{proj} \implies                             \\
    \sum\limits_{t \in T} x^*_{\src{p}, t} =
    \sum\limits_{t \in T \setminus \{ \tgt{p} \}} x_{\src{p}, t} + x^*_{\src{p}, \tgt{p}} =
    \sum\limits_{t \in T \setminus \{ \tgt{p} \}} x_{\src{p}, t} + 0 \leq n_{proj}
  \end{multline*}
  Therefore, such a solution \( x^* \) is also optimal, just like the original solution \( x \),
  but with target candidates constrained to have indices within the range \( [m, M] \).

  Considering the second option, where a target candidate \( \tgt{\hat{p}} \) contains words
  that are aligned with words from a source entity \( \src{p} \), i.e.,
  \( \left[ (i_{\tgt{\hat{p}}} < m) \lor (M < j_{\tgt{\hat{p}} < m}) \right] \land
  \left[ (i_{\tgt{\hat{p}}} \leq M) \lor m \leq j_{\tgt{\hat{p}}} \right] \).
  By corollary \ref{col:shrink_cand}, the candidate \( \tgt{p} = ( \max(i_{\tgt{\hat{p}}}, m), \min(M, j_{\tgt{\hat{p}}}) ) \),
  which is a substring of \( \tgt{\hat{p}} \), will have a higher alignment-based
  matching score.

  Let us consider a solution \( x^* \) that is identical to the solution \( x \)
  everywhere except for the target candidates \( \tgt{\hat{p}} \) and \( \tgt{p} \);
  specifically, \( x^*_{\src{p}, \tgt{\hat{p}}} = 0 \) and \( x^*_{\src{p}, \tgt{p}} = 1 \).
  This solution also satisfies all constraints of the ILP problem.

  For the constraints \eqref{eq:num_proj_const}, the proof is as follows:
  \begin{align*}
    & \sum\limits_{t \in T} x_{\src{p}, t} =
    \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + x_{\src{p}, \tgt{p}} + x_{\src{p}, \tgt{\hat{p}}}  =     \\
    & \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + 0 + 1
    \leq n_{proj} \implies                                                                                                                 \\
    & \sum\limits_{t \in T} x^*_{\src{p}, t} =
    \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + x^*_{\src{p}, \tgt{p}} + x^*_{\src{p}, \tgt{\hat{p}}}  = \\
    & \sum\limits_{t \in T \setminus \{ \tgt{p}, \tgt{\hat{p}} \}} x_{\src{p}, t} + 1 + 0 \leq n_{proj}
  \end{align*}
  And for non-overlapping constraints, since \( \tgt{p} \) is a substring of \( \tgt{\hat{p}} \) by a
  construction, we have:
  \[
    \forall t \in T \quad t \cap \tgt{p} \neq \emptyset \implies t \cap \tgt{\hat{p}} \neq \emptyset
  \]
  It gets us that all constraints that should be hold for \( \tgt{p} \) should also be satisfied for \( \tgt{\hat{p}} \):
  \begin{equation} \label{eq:overlap_substring}
    \left\{ (\src{p_1}, \tgt{p_1}) \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{p}) \in \hat{\Pi}(S, T) \right\} \subset
    \left\{ (\src{p_1}, \tgt{p_1}) \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{\hat{p}}) \in \hat{\Pi}(S, T) \right\}
  \end{equation}
  This implies that all constraints that must hold for \( \tgt{p} \) should also be satisfied for \( \tgt{\hat{p}} \):
  \begin{align*}
    & \exists \src{p_1} \in S, \tgt{p_1} \in T \Big| (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{p}) \in \hat{\Pi}(S, T) \\
    & x^*_{\src{p_1}, \tgt{p_1}} + x^*_{\src{p}, \tgt{p}} =
    x_{\src{p_1}, \tgt{p_1}} + 1 > 1 \implies                                                                      \\
    & x_{\src{p_1}, \tgt{p_1}} = 1
    \implies                                                                                                       \\
    & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p}, \tgt{\hat{p}}} = 1 + 1 = 2 > 1
  \end{align*}
  However, due to \eqref{eq:overlap_substring}, it follows that
  \( (\src{p_1}, \src{p}, \tgt{p_1}, \tgt{\hat{p}}) \in \hat{\Pi}(S, T) \).
  This contradiction indicates that the solution \( x \) was feasible.
  Consequently, the constraints \eqref{eq:non_overlap_const} are satisfied by the
  solution \( x^* \).

  At the same time, the solution \( x^* \) has a higher objective value, which
  implies that the original solution \( x \) was not optimal. This leads to a
  contradiction with our assumption that there is no optimal solution that satisfies
  the requirements of the theorem.
\end{proof}
It is important to note that the theorem applies only to the constraints
\eqref{eq:num_proj_const} in the forms of \( < \) or \( \leq \). This is due to
the fact that the ILP problem may lose feasibility if projections with zero
scores—those that exist solely to satisfy these constraints and have no practical
application—are removed.

In comparison to the heuristic algorithm that employs word-to-word alignment,
the formulation of the optimization problem with matching scores \eqref{eq:align_cost}
presents a significant advantage: it allows for the evaluation of the confidence
associated with each specific projection, as a score is available for each one.
In contrast, heuristics only provide a labeling of the target sentence. Additionally,
the predictions made by this formulation and the heuristics are not always the same.
For example, the heuristic algorithm merges all continuous ranges of aligned words
that are separated by at most a fixed number of non-aligned words, whereas the
ILP formulation will only do so under certain specific conditions.

It is also worth mentioning that, for the alignment-based score, there is no fixed
upper bound, as the maximum value of the denominator is the product of the lengths of
the source entity and the target candidate. This limitation may be addressed by
replacing one of the summation operations in the denominator of equation
\eqref{eq:align_cost} with a logical OR. Nevertheless, in our experiments, we will
utilize the original form of the alignment-based matching score.

\subsection{NER model-based score}

Another approach to evaluate matching scores between source entities
and target candidates is to utilize a multilingual NER model.
The underlying concept is as follows: for each word in the target
candidate, the model predicts a probability distribution over a
set of classes to determine the likelihood that a word has a
specific label. To compute the matching score indicating how likely a source
entity classified as \( l \) should be projected onto a target candidate,
we calculate the average probability of class \( l \) across all words in
the candidate.

Let \( L \) denote a set of classes. Since a NER model outputs results in the IOB format,
where the first word of a predicted entity with class \( l \in L \) is labeled as
\textit{B-l}, and subsequent words in the entity are labeled as \textit{I-l}, the
model's output can be represented as a matrix \( p_{m, o} \). In this representation,
\( m \in \mathbb{N} \) is the index of a word, and \( o \in \{ 1, 2|L| + 1 \} \) is the
index of a label. We will assume that the label \( O \), which indicates that a word does not
belong to any class, has an index of \( 2|L| + 1 \).

Next, we introduce mappings \( B[l]: L \rightarrow \{ 1, \dots, 2|L| \} \) and
\( I[l]: L \rightarrow \{ 1, \dots, 2|L| \} \), which return the indices of the
B and I labels, respectively, for a class \( l \in L \) in the probability matrix.

Therefore, the NER model-based matching score can be expressed as follows:
\begin{equation} \label{eq:ner_cost}
  c_{\src{p}, \tgt{p}}^{ner} = \alpha^{(j_{\tgt{p}} - i_{\tgt{p}}) - 1}
  \frac{
    p_{i_{\tgt{p}}, B[l_{\src{p}}]} +
    \sum\limits_{k = i_{\tgt{p}} + 1}^{j_{\tgt{p}}} p_{k, I[l_{\src{p}}]}
  }
  {j_\tgt{p} - i_{\tgt{p}}}
\end{equation}
where \( l_{\src{p}} \in L \) denotes the class of the source entity
\( \src{p} \in S \) and \( \alpha > 0 \in \mathbb{R} \) is a length-scaling constant.

The factor \( \alpha \) is necessary to align the matching scores with the NER model
predictions, i.e it is naturally expected that the matching cost should be lower if a
target candidate is a substring of a target candidate that is identical to the predicted
by the NER model entity.

For example, let the set of classes contain only the class \textit{PER},
so \( L = \{ \text{PER} \} \). Assume the set of source entities consists of
only one entity \( \src{p} \) with the class \textit{PER}. Let the set of target
candidates be \( T = \{ \tgt{p_1} = (1, 1), \tgt{p_2} = (1, 2) \} \), and the
probability matrix predicted by the model is as follows:
\[
  \begin{blockarray}{cccc}
    & \text{\textit{B-PER}} & \text{\textit{I-PER}} & \text{\textit{O}} \\
    \begin{block}{c(ccc)}
      1 & 0.9 & 0.1 & 0 \\
      2 & 0.2 & 0.8 & 0 \\
      3 & 0   & 0   & 1 \\
    \end{block}
  \end{blockarray}
\]
By taking the maximum over the rows, it is revealed that the entity predicted by
the model has the class \textit{PER} and consists of words with indices 1 and 2.
However, the NER model-based matching score, computed without the scaling factor
\( \alpha \), yields the following results:
\[
  c_{\src{p}, \tgt{p_1}}^{ner} = 0.9 \qquad c_{\src{p}, \tgt{p_2}}^{ner} = 0.85
\]
In this case, the ILP problem will preferentially project the source entity onto a
substring of the entity predicted by the model. Therefore, the length-scaling
constant \( \alpha \) cannot be omitted.

If \( \alpha \) is a crucial component of the score, then it is worthwhile to
evaluate the range of its possible values. To this end, let us notice the
following observation.
\begin{lemma} \label{lemma:maxprob_greater_than_avg}
  Let \( p_{i}, i \in K \) be a probability distribution over a finite set \( K \) with cardinality \( k \).
  Then
  \[
    \max\limits_{i \in K} p_i \geq \frac{1}{k}
  \]
\end{lemma}
\begin{proof}
  Suppose that it is wrong, i.e.
  \[
    \max\limits_{i \in K} p_i < \frac{1}{k}
  \]
  Then the sum of all probabilities:
  \[
    \sum\limits_{i \in K} p_i \leq \sum\limits_{i \in K} \max\limits_{j \in K} p_j =
    k \cdot \max\limits_{j \in K} p_j < 1
  \]
  However, since the sum of all probabilities in the distribution must equal \( 1 \),
  our assumption was incorrect, which proves the lemma.
\end{proof}
Having this useful fact it is possible to provide some estimates for the constant \( \alpha \).
\begin{theorem}
  Consider a source entity \( \src{p} \in S \) and two target candidates \( \tgt{p_1} = (i, j+1) \)
  and \( \tgt{p_2} = (i, j) \). Suppose the predictions \( p \) from a NER model satisfy
  the following constraints:
  \[
    B[l_{\src{p}}] = \argmax\limits_{l \in \{ 1, \dots, 2|L| + 1 \}} p_{i, l} \qquad
    I[l_{\src{p}}] = \argmax\limits_{l \in \{ 1, \dots, 2|L| + 1 \}} p_{k, l} \; \forall k \in \{ i + 1, \dots ,j +1 \}
  \]
  i.e all these candidates are substrings of the entity predicted by the model.
  Let \( M = \frac{1}{2|L| + 1} \) and \( n = j - i \). Then:
  \[
    \alpha > 1 + \frac{1 - M}{1 + M} \implies c_{\src{p}, \tgt{p_2}}^{ner} > c_{\src{p}, \tgt{p_1}}^{ner}
  \]
\end{theorem}
\begin{proof}
  Starting with the desired inequality we have:
  \begin{align*}
    & c_{\src{p}, \tgt{p_2}}^{ner} > c_{\src{p}, \tgt{p_1}}^{ner} \Leftrightarrow \\
    & \alpha^{n}
    \frac{
      p_{i, B[l_{\src{p}}]} +
      \sum\limits_{k = i + 1}^{j + 1} p_{k, I[l_{\src{p}}]}
    }
    {n + 1} >
    \alpha^{n - 1}
    \frac{
      p_{i, B[l_{\src{p}}]} +
      \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]}
    }
    {n}
    \Leftrightarrow                                                                \\
    & \alpha >
    \frac
    {(n+1) \cdot \left( p_{i, B[l_{\src{p}}]} +
    \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} \right)}
    {
      n \cdot \left( p_{i, B[l_{\src{p}}]} +
    \sum\limits_{k = i + 1}^{j + 1} p_{k, I[l_{\src{p}}]} \right)}
    =
    \frac
    {(n+1) \cdot \left( p_{i, B[l_{\src{p}}]} +
    \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} \right)}
    {
      n \cdot \left( p_{i, B[l_{\src{p}}]} +
    \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} + p_{j+1, I[l_{\src{p}}]} \right)}
  \end{align*}
  To simplify the derivation, we will introduce aliases for certain quantities:
  \[
    S = p_{i, B[l_{\src{p}}]}  + \sum\limits_{k = i + 1}^{j} p_{k, I[l_{\src{p}}]} \qquad
    s = p_{j+1, I[l_{\src{p}}]}
  \]
  Note that, according to lemma \ref{lemma:maxprob_greater_than_avg},
  the following inequalities hold:
  \begin{equation} \label{eq:ner_sum_ineq}
    M \leq s \leq 1 \qquad nM \leq S \leq n
  \end{equation}
  Then we have:
  \begin{equation*}
    \alpha > \frac{(n + 1) S}{n (S + s)} =
    \frac{nS + S + ns - ns}{nS + ns} =
    1 + \frac{S - ns}{n(S + s)}
  \end{equation*}
  The derivatives of this expression are the following:
  \begin{equation*}
    \left( 1 + \frac{S - ns}{n(S + s)} \right)_s^{'} =
    \frac{-n^2S - nS}{n^2 (S + s)^2} < 0
    \qquad
    \left( 1 + \frac{S - ns}{n(S + s)} \right)_S^{'}
    = \frac{nS + n^2s}{n^2 (S + s)^2} > 0
  \end{equation*}
  Hence, the maximum of the function is located at the point \( s = M \) and
  \( S = n \). Thus, we can select \( \alpha \) such that:
  \begin{equation*}
    \alpha > 1 + \frac{1 - M}{1 + M}
    \stackrel{n \geq 1}{\geq}
    1 + \frac{n - nM}{n(n + M)}
    \stackrel{\max}{\geq}
    1 + \frac{S - ns}{n(S + s)}
  \end{equation*}
\end{proof}
As demonstrated in the proof, this represents just an upper estimate of such an
\( \alpha \) for the worst-case scenario. In practice, it may be reasonable to attempt
even smaller values of \( \alpha \). However, doing so does not guarantee that the
desired properties will be satisfied.

The primary distinction between a standard model transfer and the XLNER pipeline,
which employs the ILP projection problem utilizing NER model-based matching costs,
lies in the fact that in the latter, we project labels from source entities.
Consequently, certain predictions made by the NER model, which may be incorrect,
can be disregarded since the solution to the ILP problem does not include a
source entity projecting onto that specific target entity.

One of the challenges associated with using models is that they often exhibit
overconfidence in their predictions, producing high probabilities for each output,
including those that may be incorrect. Since the proposed score directly relies on
these probabilities, it can be adversely affected. To mitigate this issue, calibration
of the model can be implemented. The simplest method to achieve this is to scale
the model's output logits by a temperature factor. Nevertheless, in the subsequent chapters,
we will utilize the NER model without any calibration.

The NER model-based matching score \eqref{eq:ner_cost} possesses another noteworthy
property. Since the computation of the score relies solely on the class of the source
entity, the scores will be identical for all source entities sharing the same class.
From the perspective of the ILP problem \eqref{eq:ilp}, this can result in a solution
where a source entity is projected onto a semantically incorrect candidate;
nonetheless, a correct class will still be assigned to the target candidate.

\subsection{Translation-based score}

Since every projected target entity should be an exact translation of its corresponding
source entity, a matching score can be formulated based on the probability that a
source entity translates into a target candidate.

The NMTScore \cite{vamvas_sennrich_2022_nmtscore} is a method for computing such a
translation score given a neural machine translation model capable of translating
between the desired languages.

The NMTScore proposes three translation-based measures: direct, pivot, and cross-likelihood.
However, since the computation of the latter two measures requires significantly more
computational resources than the first, the direct measure is chosen for our matching
score, considering time constraints. The direct NMTScore operates with the direct
translation probability, which is computed as follows: let \( A \) and \( B \) be
phrases in two different languages \( a \) and \( b \), and let \( p_{\theta} \)
represent the probabilities generated by a translation model. The direct translation
probability is given by:
\[
  P_{direct}(A, B) = \left[ \prod\limits_{t=0}^{|A|} p_{\theta}(A^i|B, A^{<i}) \right]^{\frac{1}{|A|}}
\]

Essentially, this is a probability, induced by an autoregressive translation model,
normalized by the sequence length. The directed NMTScore similarity is a normalized
version that constrains it to be less than or equal to 1:
\[
  \hat{sim}(A, B) = \frac{P_{direct}(A, B)}{P_{direct}(A, A)}
\]

However, because the translation probability \( p_{\theta} \) is a directed measure,
it will differ if the arguments are swapped. To make the similarity score symmetric
with respect to the arguments, the authors compute an average of both directions:
\[
  sim(A, B) = \frac{\hat{sim}(A, B) + \hat{sim}(B, A)}{2}
\]

Thus, the translation-based matching score can be computed as the direct NMTScore of
the given source entity and target candidate:
\begin{equation} \label{eq:nmt_cost}
  c_{\src{p}, \tgt{p}}^{nmt} =
  sim \left(\src{w}[i_{\src{p}} : j_{\src{p}}],
  \tgt{w}[i_{\tgt{p}} : j_{\tgt{p}}] \right)
\end{equation}
where \( \src{w}, \tgt{w} \) is an array of words of the source and target
candidates respectively.

The quality of this type of matching score is heavily dependent on the performance
of the translation model and its ability to accurately translate not just entire
sentences, but also arbitrary subphrases.

\subsection{Fused score}
The performance of the alignment, NER, and translation models utilized for the
computation of the scores discussed above varies based on factors such as language,
domain, and the set of classes. Each matching score has its own advantages and
drawbacks; however, a natural way to minimize failures is to combine the scores.
Since all scores are real numbers, one approach to achieve this is to compute a
weighted sum of scores across all types:
\begin{equation} \label{eq:fused_cost}
  c_{\src{p}, \tgt{p}}^{fused} =
  \lambda_{align} c_{\src{p}, \tgt{p}}^{align} +
  \lambda_{ner} c_{\src{p}, \tgt{p}}^{ner} +
  \lambda_{nmt} c_{\src{p}, \tgt{p}}^{nmt}
\end{equation}
where \( \lambda_{align} \geq, \lambda_{ner} \geq 0, \lambda_{nmt} \geq 0 \in R\).

Furthermore, the fused score enhances the ability to address issues associated with
the individual scores. For instance, the NER model used in the NER-based cost
\eqref{eq:ner_cost} may occasionally correctly predict the spans of entities in the
target sentence but misclassify their labels. During the computation of the NER
model-based cost, we assume that the labels are predicted accurately, as there is
no method to amend them. However, when the cost is comprised of various basic scores,
we can utilize the NER model-based score solely to evaluate the likelihood that a
given candidate can represent a target entity of any class, while determining the
label through other types of scores. The modified version of \eqref{eq:ner_cost} that
implements this concept is as follows:
\begin{equation} \label{eq:ner_cost_wo_classes}
  c_{\src{p}, \tgt{p}}^{ner} = \alpha^{(j_{\tgt{p}} - i_{\tgt{p}}) - 1}
  \max\limits_{l \in L}
  \frac{
    p_{i_{\tgt{p}}, B[l]} +
    \sum\limits_{k = i_{\tgt{p}} + 1}^{j_{\tgt{p}}} p_{k, I[l]}
  }
  {j_\tgt{p} - i_{\tgt{p}}}
\end{equation}

\section{Analysis of the ILP problem}
Although the proposed ILP problem is a specific instance of the
well-studied general binary programming problem, it does not
necessarily inherit all properties associated with it. For instance,
while the maximum bipartite matching problem \cite{pemmaraju2003computational} can be formulated as an
instance of the ILP problem, it can still be solved in polynomial
time, whereas the general case of ILP is NP-hard. This distinction
necessitates a more in-depth analysis of the proposed formulation.

\subsection{Complexity}
Constraints play a critical role in determining the complexity of the proposed problem.
In the scenario where all candidates are non-overlapping and the constraints
\eqref{eq:num_proj_const} take the form of equalities with \( n_{proj} = 1 \),
the problem simplifies to an instance of the weighted bipartite matching problem
\cite{pemmaraju2003computational}, which belongs to complexity class \( P \).
However, let us explore the more general case.

For the sake of convenience, we will utilize the first formulation of the ILP
problem \eqref{eq:objective}--\eqref{eq:binary_prog} in this section. However,
since it is equivalent to the formulation in \eqref{eq:ilp}, this choice does not
impact the results of the analysis.

A common approach to establishing the complexity of a problem is to reduce another
problem with a known complexity to an instance of the problem under study.
Therefore, let us consider the maximum weight independent set problem
\cite{pemmaraju2003computational}.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[every node/.style = {draw, circle}]
    \node[fill=green!10] (1) {1};
    \node[right=of 1] (2) {1};
    \node[below right=of 2, fill=green!10] (3) {1};
    \node[below=of 1, fill=green!10] (4) {3};
    \node[left=of 4] (5) {2};
    \node[below right=of 4] (6) {2};

    \graph{
      (1) -- (2) -- (4) -- (6) -- (5) -- (4),
      (6) -- (2) -- (3)
    };
  \end{tikzpicture}
  \caption{Maximum weigh independent set problem (MaxWIS). Vertices that form an optimal solution are colored in \textbf{\textcolor{green!50}{green}}}
  \label{fig:maxwis}
\end{figure}

The maximum weight independent set problem (MaxWIS) involves finding a subset of
vertices in an undirected graph with maximum total weight, such that no two vertices in
the subset are connected by edges in the graph. An example of the maximum weight
independent set problem is illustrated in Figure \ref{fig:maxwis}.
The formal definition of MaxWIS as an ILP problem is provided in Definition \ref{def:maxwis}.
\begin{definition}[MaxWIS] \label{def:maxwis}
  Let \( G=(V, E, w), V \neq \emptyset, \; w: V \rightarrow \mathbb{R} \) be an undirected weighted graph, then a maximum weigh
  independent set problem for the graph \( G \) is the following:
  \begin{align*}
    & \max \sum\limits_{v \in V} w_v x_v                               \\
    & x_u + x_v \leq 1               \qquad \forall \{u, v\} \in E \\
    & x_v \in \{0, 1\}
  \end{align*}
\end{definition}

In the scenario where all weights are equal to \( 1 \), the problem is referred to as
the maximum independent set (MaxIS) problem. In this case, the objective is to maximize
the cardinality of the independent set.

It can be demonstrated that the generalized form of the projection ILP problem
\eqref{eq:objective}--\eqref{eq:binary_prog}, where the relation \( \cap \) is an
arbitrary reflexive, symmetric, and non-transitive relation, is NP-hard. The
reduction of the maximum independent set problem to the instance of the generalized
ILP problem that proves this is presented in Appendix~\ref{sec:gen_ilp_is_np_hard}.
However, the overlapping relation defined in \ref{def:overlapping}, which is
employed in the projection ILP problem, represents a specific case in which
the induced graph of the MaxIS problem is an interval graph. Consequently, the
reduction from the proof is no longer valid. Moreover, it is known that for interval graphs,
the maximum independent set problem can be solved in polynomial time \cite{bhattacharya2014maximum}.

Nevertheless, these results motivate further analysis. It turns out that the maximum
weight independent set problem on interval graphs can also be solved in polynomial
time \cite{PalB96}. So, we can prove that the non-overlapping constraints themselves
do not make the entire projection ILP problem \eqref{eq:objective}--\eqref{eq:binary_prog}
computationally hard.

Consider the projection ILP problem without the constraints \eqref{eq:num_proj_const}:
\begin{equation} \label{eq:ilp_without_nproj}
  \begin{aligned}
    & \max\limits_x \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}}                                             \\
    & \text{subject to}                                                                                                                                   \\
    & x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1
    & \forall (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi}(S, T)                                                                            \\
    & x_{\src{p}, \tgt{p}} \in \{ 0, 1 \}                                                                     & \forall (\src{p}, \tgt{p}) \in S \times T
  \end{aligned}
\end{equation}
It is possible to reduce this problem to an instance of the maximum weight
independent set problem on an interval graph.

The idea of the reduction is straightforward. For every target candidate
\( t_v \in T \), we will create a distinct vertex \( v \in V \) that corresponds
to this candidate. The weight of each vertex \( v \) is computed based on the matching
scores between all source entities and the target candidate that corresponds to this
vertex:
\[
  w_v = \max\limits_{s \in S} c_{s, t_v}
\]
The conversion between the solutions of the ILP problem \eqref{eq:ilp_without_nproj} and
the solutions of the MaxIS problem can be established as follows:
\begin{equation} \label{eq:link_variables}
  \begin{aligned}
    &s^*_v = \argmax\limits_{s \in S} x_{s, t_v} \\
    &
    x_v = 1 \implies
    \begin{cases}
      x_{s^*, t_v} = 1 \\
      x_{s, t_v} = 0 \quad \forall s \in S \setminus \{ s^* \} \\
    \end{cases}
  \end{aligned}
  \qquad
  \exists s \in S \Big| x_{s, t_v} = 1 \implies
  x_v = 1
\end{equation}
That is, the vertex \( v \in V \) belongs to the maximum independent set if and only
if the source entity with the highest matching score is projected onto the target
candidate \( \tgt{p_v} \) corresponding to this vertex. In the event that several source
entities have equal maximum scores, we select only one entity.

In the instance of the MaxWIS problem, nodes \( v \) and \( u \in V \), where \( u \neq v \),
are connected if and only if their corresponding target candidates overlap:
\begin{equation} \label{eq:edge_reduction}
  \{ v, u \} \in E \Leftrightarrow t_u \cap t_v \neq \emptyset
\end{equation}
Furthermore, since the set of target candidates is, by definition, a set of intervals,
the corresponding graph will indeed be an interval graph.
This completes the reduction procedure. It is important to note that the reduction
is executed in polynomial time.

Given the one-to-one correspondence between the vertices in the MaxWIS problem and
the target candidates from the set \( T \), we can consider the following set that
is completely determined by and determines the solution of the MaxWIS problem:
\[
  T^*_{x} = \{ t \in T | \exists s \in S, x_{s, t} = 1 \}
\]

It is important to note that all feasible solutions of the MaxWIS will
correspond to feasible solutions of the ILP problem \eqref{eq:ilp_without_nproj}
and vice versa.

\begin{lemma} \label{lemma:maxis_f_implies_ilp}
  Suppose \( x_v \) is a feasible solution of the maximum weight independent set
  problem; then, there exists a corresponding feasible solution for the problem
  \eqref{eq:ilp_without_nproj}.
\end{lemma}
\begin{proof}
  Suppose that a solution \( x \) that corresponds to a feasible
  solution \( x_v \) of the MaxWIS problem is non-feasible. Then, we have:
  \[
    \exists (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi}(S, T) \Big|
    x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} > 1
  \]
  However, since the solution \( x_v \) is feasible, the constraints
  involving \( \tgt{p_1} \neq \tgt{p_2} \) cannot be violated:
  \begin{align*}
    & \forall \{ u, w \} \in E \quad x_u + x_w \leq 1 \stackrel{\eqref{eq:edge_reduction}}{\implies}                  \\
    & \forall \tgt{p_1} \in T^*_x \; \nexists \tgt{p_2} \in T^*_x \Big| \tgt{p_1} \cap \tgt{p_2} \neq \emptyset \implies \\
    & \forall \src{p_1}, \src{p_2} \in S \quad x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1
  \end{align*}
  Therefore, \( \tgt{p_1} = \tgt{p_2} \), and the constraints are violated because
  there exist at least two source entities \( \src{p_1}, \src{p_2} \in S \) that are
  projected onto the same target candidate \( \tgt{p} \in T \):
  \[
    x_{\src{p_1}, \tgt{p}} = 1 \qquad x_{\src{p_2}, \tgt{p}} = 1
  \]
  However, this contradicts the fact that, by the construction of the
  corresponding solution to the ILP problem \eqref{eq:link_variables}, there is
  only one source entity projected onto each target candidate.
\end{proof}

\begin{lemma} \label{lemma:ilp_f_implies_maxis}
  Suppose \( x \) is a feasible solution of the problem \eqref{eq:ilp_without_nproj};
  then, the corresponding solution to the maximum weight independent set problem is also feasible.
\end{lemma}
\begin{proof}
  By construction, the non-overlapping constraints of the problem
  \eqref{eq:ilp_without_nproj} imply the constraints of the MaxWIS problem:
  \begin{align*}
    & \forall (\src{p_1}, \src{p_2}, \tgt{p_1}, \tgt{p_2}) \in \hat{\Pi}(S, T) \quad
    x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} \leq 1 \implies                                                     \\
    & \forall \tgt{p_1}, \tgt{p_2} \in T, \tgt{p_1} \neq \tgt{p_2}, \tgt{p_1} \cap \tgt{p_2} \neq \emptyset
    \quad \nexists \src{p_1}, \src{p_2} \in S \Big|
    x_{\src{p_1}, \tgt{p_1}} + x_{\src{p_2}, \tgt{p_2}} > 1 \implies                                                     \\
    & \nexists \tgt{p_1}, \tgt{p_2} \in T, \tgt{p_1} \neq \tgt{p_2}, \tgt{p_1} \cap \tgt{p_2} \neq \emptyset \Big|
    \tgt{p_1} \in T^*_x, \tgt{p_2} \in T^*_x \implies                                                                    \\
    & \forall \{ v_{\tgt{p_1}}, v_{\tgt{p_2}} \} \in E \quad x_{v_{\tgt{p_1}}} + x_{v_{\tgt{p_2}}}\leq 1
  \end{align*}
\end{proof}

Finally, we can demonstrate that it is possible to reduce the projection ILP problem, without the
constraints \eqref{eq:num_proj_const}, to an instance of the maximum weight
independent set problem on an interval graph. Consequently, it can be solved using a polynomial-time
algorithm.
\begin{theorem}
  The formulation \eqref{eq:ilp_without_nproj} of the projection ILP problem,
  which does not include constraints on the number of target candidates projected
  from each source entity, can be solved in polynomial time.
\end{theorem}
\begin{proof}
  Let \( x \) denote the corresponding solution to the optimal solution \( x_v \) of
  the MaxWIS problem, which has been constructed through the reduction from the ILP
  problem. If \( x \) is always an optimal solution for the problem
  \eqref{eq:ilp_without_nproj} then this would establish that we can reduce this
  problem to an instance of the maximum weighted independent set problem, which has a
  polynomial time algorithm.

  By Lemma \ref{lemma:maxis_f_implies_ilp}, the feasibility of the solution for the
  MaxWIS problem ensures the feasibility of the corresponding solution for the ILP
  problem. The objective function for the solution \( x \) is, by construction as
  indicated in \eqref{eq:link_variables}, equal to the objective function of the
  MaxWIS problem:
  \[
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}} =
    \sum\limits_{v \in V} w_v x_v
  \]

  Assume that there exists an optimal solution \( x^* \) of the problem \eqref{eq:ilp_without_nproj}
  such that it is better than \( x \):
  \[
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}} <
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x^*_{\src{p}, \tgt{p}}
  \]
  Consequently, for the corresponding solution \( x^*_v \) of the MaxWIS problem, we obtain:
  \[
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x^*_{\src{p}, \tgt{p}} \leq
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} \max\limits_{s \in S} c_{s, \tgt{p}} x^*_{\src{p}, \tgt{p}} =
    \sum\limits_{v \in V} w_v x^*_v
  \]
  By Lemma \ref{lemma:ilp_f_implies_maxis}, it follows that \( x^*_v \) will be a
  feasible solution. However, this contradicts the fact that \( x_v \) was an optimal
  solution:
  \[
    \sum\limits_{v \in V} w_v x_v =
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x_{\src{p}, \tgt{p}} <
    \sum\limits_{(\src{p}, \tgt{p}) \in S \times T} c_{\src{p}, \tgt{p}} x^*_{\src{p}, \tgt{p}} \leq
    \sum\limits_{v \in V} w_v x^*_v
  \]
\end{proof}

Thus, the question of whether the problem \eqref{eq:ilp} is NP-hard remains open.
However, if it is NP-hard, the constraints \eqref{eq:num_proj_const} are
principal factor to that complexity.

\subsection{Approaches to compute the solution of the problem}
Like any ILP problem, the problem \eqref{eq:ilp} can be solved using methods such as
branch-and-bound \cite{land2010automatic}, cutting planes \cite{gilmore1963linear, dyckhoff1981new},
branch-and-cut \cite{branchAndCut}, and other exact algorithms.
However, these approaches may require significant time to find an optimal solution,
which can render such formulations inefficient from an application standpoint,
particularly when it is necessary to solve instances of this problem for thousands
of sentences within a limited timeframe.

This situation highlights the need for an approximate algorithm capable of computing
a solution that may not always be optimal but adheres to crucial constraints. One
approach for developing such an algorithm for the problem \eqref{eq:ilp} involves
iteratively assigning \( 1 \) to the variable with the highest matching score and
then removing all target candidates that overlap with the selected candidate to
enforce non-overlapping constraints. Algorithm \ref{alg:ilp_greedy} represents a
variant of this greedy algorithm.

\begin{algorithm}
  \SetAlgorithmName{Algorithm}{}{}
  \caption{Approximate greedy algorithm for the proposed ILP problem} \label{alg:ilp_greedy}
  \KwData{instance of the ILP problem \eqref{eq:ilp}}
  \KwResult{\( x \) -- "solution" of the ILP problem}

  \( x \gets 0 \) \;
  \( P \gets 0 \) \Comment*[r]{number of projections by source entity}
  \While{\( \exists \src{p} \in S, \tgt{p} \in T \Big| c_{\src{p}, \tgt{p}} > 0 \)}{
    \( s, t \gets \argmax\limits_{\src{p} \in S, \tgt{p} \in T } c_{\src{p}, \tgt{p}} \) \;
    \( x_{s,t} \gets 1 \) \;
    \( P_{s} \gets P_{s} + 1 \) \;

    \ForAll(\tcp*[f]{remove all overlapping with \( t \) candidates}){\( \hat{t} \in T \Big| \hat{t} \cap t \neq \emptyset \)}{
      \( c_{s, \hat{t}} \gets 0 \) \;
    }

    \Comment{try to ensure constraints \eqref{eq:num_proj_const}}
    \If{constraints \eqref{eq:num_proj_const} is a type of \( =, \leq \)}{
      \If{\( P_s = n_{proj} \)}{
        \ForAll{\( \hat{t} \in T \)}{
          \( c_{s, \hat{t}} \gets 0 \) \;
        }
      }
    }
    \If{constraints \eqref{eq:num_proj_const} is a type of \( < \)}
    {
      \If{\( P_s = n_{proj} - 1 \)}{
        \ForAll{\( \hat{t} \in T \)}{
          \( c_{s, \hat{t}} \gets 0 \) \;
        }
      }
    }
  }
\end{algorithm}

The steps of Algorithm \ref{alg:ilp_greedy} imply that by removing all overlapping
target candidates—thereby setting their costs to zero—the algorithm maintains the non-overlapping constraints.c
Constraints \eqref{eq:num_proj_const} regarding the number of candidates projected from
each source entity are ensured by removing all source entities that reach maximum allowed number
of projections.  However, this holds true only in cases where the constraints are of
the form \( < \) or \( \leq \).

In the cases of \( =, >, \) or \( \geq \), the algorithm may produce a "solution" that
violates the constraint \eqref{eq:num_proj_const} under two specific circumstances.
The first occurs when there is a source entity for which all target candidates,
which do not overlap with the already projected ones, initially have a matching
cost of zero. However, in such cases, it does not make sense from an application standpoint
to project the source entity onto these candidates, as they are definitively not
counterparts of the source entity in the target sentence; otherwise, their
matching cost would not be zero.

The second situation arises when target candidates that could potentially be
projected onto have been eliminated in previous iterations of the algorithm.
Addressing this issue is challenging without backtracking or violating the
feasibility of the non-overlapping constraints \eqref{eq:non_overlap_const}.

Nevertheless, Algorithm \ref{alg:ilp_greedy} operates in linear time with respect to
the number of variables and, thus, can solve the ILP problem significantly faster than
an exact ILP solver in general cases.

Even in cases where the output of the greedy algorithm is a feasible solution, it does not necessarily imply that the solution is optimal; there may exist another feasible solution with a higher objective value. This can be easily illustrated with an example.

Consider the ILP problem \eqref{eq:ilp} with \( n_{proj} = 2 \) and the \( \leq \)
type of constraints \eqref{eq:num_proj_const}. Let \( T = \{ \tgt{p_1} = (1, 2), \tgt{p_2} = (2, 3), \tgt{p_3} = (3, 5) \} \)
and \( S = \{ \src{p} \} \), and let the matching scores be as follows:
\[
  c_{\src{p}, \tgt{p_1}} = 0.2 \qquad
  c_{\src{p}, \tgt{p_2}} = 0.3 \qquad
  c_{\src{p}, \tgt{p_3}} = 0.2
\]
The output of Algorithm \ref{alg:ilp_greedy} is the solution \( x \) with only one
non-zero variable, \( x_{\src{p}, \tgt{p_2}} = 1 \). This solution has an objective
value of \( 0.3 \). In contrast, the optimal solution involves projecting the source
entity onto both \( \tgt{p_1} \) and \( \tgt{p_2} \), yielding an objective value
of \( 0.4 \).
