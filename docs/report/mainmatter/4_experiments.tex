\chapter{Evaluation}
\label{sec:experiments}

The evaluation of the XLNER pipelines can be classified into two groups: extrinsic and
intrinsic. Extrinsic evaluation involves utilizing the XLNER pipeline to produce a
labeled dataset in the target language, followed by training a NER model on this dataset.
In this context, the performance of the trained NER model serves as a measure of quality.
A primary concern with this approach is its dependency on the training procedure; that is,
results may vary due to different random sampling of batches, alterations in batch size,
mixing of generated data with manually labeled data, other factors. Furthermore,
this process demands substantial computational resources to train a model, which becomes
problematic when conducting hundreds of experiments.

Consequently, the results of intrinsic evaluation will be presented in this chapter.
This entails taking a manually labeled dataset in the target language, feeding it to
the pipeline to generate labels, and comparing the predictions with the ground truth
labels. It is reasonable to anticipate that superior intrinsic performance will
correlate with improved extrinsic performance, assuming a consistent training setup.

To assess the proposed formulation of the projection step of the XLNER pipeline as an
ILP problem, as delineated in \eqref{eq:ilp}, and to analyze various forms of it, two
sets of experiments have been conducted. The first set evaluates the performance of
the projection step in isolation. The second set of experiments confirms the quality
of projections within the complete XLNER pipeline on the MasakhaNER2 dataset
\cite{adelani-etal-2022-masakhaner}. The source code utilized for conducting all
experiments is made publicly available on GitHub\footnote{\url{https://github.com/ShkalikovOleh/master-thesis}}.
The GUROBI optimizer \cite{gurobi}, employed under an academic license, served as the
ILP solver for all experiments.

Word-to-word alignments for all experiments were computed using a non-fine-tuned
AWESOME aligner with the following default hyperparameters: extraction method set
to softmax, softmax\_threshold of \( 0.001 \), and align\_layer is \( 8 \).

For source labelling, candidate evaluation, and model transfer, the MDeBERTa-v3-base
model, fine-tuned on the English split of the CONLL-2003 dataset \cite{tjong-kim-sang-de-meulder-2003-introduction-conll},
was utilized. This selection is based by the findings of the original study in which
MasakhaNER2 was introduced, demonstrating that MDeBERTa-v3 achieves superior performance
compared to other multilingual non-African-centric models, despite being smaller than
XLM-RoBERTA-Large \cite{conneau-etal-2020-unsupervised-xlmr}. The model was trained
for 5 epochs, with a total batch size of \( 32 \) (consisting of \( 16 \) with gradient
accumulation every 2 steps), utilizing the Adam optimizer \cite{Kingma2014AdamAM} with
betas set to \( (0.9,0.999) \) and a learning rate of \( 2 \cdot 10^{-5} \). The model
is publicly accessible on the HuggingFace Hub\footnote{\url{https://huggingface.co/ShkalikovOleh/mdeberta-v3-base-conll2003-en}}.

In all experiments involving NER model-based and translation-based matching scores,
in order to simplify the problem (by making scores matrix sparse) and based on the interpretation of the scores
(where small values indicate a poor projection), all \eqref{eq:ner_cost} scores were
thresholded at \( 0.05 \) and all \eqref{eq:nmt_cost} scores were thresholded
at \( 0.1 \). The \( \alpha \) parameter fo the NER model-based score was set to \( 1.15 \).

\section{Isolated evaluation of the projection step}
The XLNER pipeline comprises three steps: forward translation, source NER labelling,
and projection. Each of these steps can introduce errors. The new ILP-based approach for the
projection step has been proposed, and it is essential to evaluate the performance
of this step independently from errors associated with the preceding steps.

This necessitates the availability of a labelled dataset for parallel text,
which would enable the exclusion of the translation and source labelling phases.
The Europarl-NER dataset \cite{agerri-etal-2018-building} serves as such a dataset.
This dataset consists of 799 parallel sentences derived from the Europarl corpus \cite{koehn2005europarl}, manually annotated according to four entity types, adhering to the CoNLL 2002 and 2003 guidelines for four languages: English, German, Italian, and Spanish.

\begin{table}[ht]
  \centering
  \input{mainmatter/tables/europarl_heuristic}
  \caption{Overall F1 scores for word-to-word alignments-based heuristic
  algorithm with different hyperparameter  on the Europarl NER dataset}
  \label{tab:europarl_heur_f1}
\end{table}

Consequently, it becomes feasible to evaluate the performance of the proposed
projection step as the ILP problem in isolation. Prior to this, however, it is
important to investigate the performance of the heuristic word-to-word alignment-based
algorithm \ref{alg:heuristics} to determine the optimal combination of hyperparameters
and the resulting performance metrics for comparison with the proposed projection step.
The F1 scores for the heuristic algorithm, assessed with varying hyperparameters, are
presented in Table \ref{tab:europarl_heur_f1}.

The results indicate that the optimal combination of hyperparameters for the majority of
languages is as follows: a merging distance \( d = 1 \), an unrestricted maximum number
of aligned subranges sorted by length that can be projected (i.e., \( k \) should not be
limited), merging of aligned ranges should disregard whether the first word of the
right-aligned range is aligned to the first word of a source entity, and a word length
ratio threshold of \( 0.8 \)  between the source entity and the target range of words.
Hence, these hyperparameters will be employed for subsequent experiments.

The overall results are as follows. The most significant improvement in performance
occurs when the algorithm is permitted to merge aligned ranges together, provided
that only one non-aligned word exists between them. This enhancement is attributed to
the algorithm's ability to fill gaps in imperfect or missed alignments. Concurrently,
imposing a limit \( k \) on the number of projected target ranges for any source entity
results in a slight decrease in performance. Additionally, due to errors in alignments,
the merging of aligned subranges, which only occurs when the right range begins with a
word aligned to the first word of a source entity, leads to outcomes that are less
favorable in comparison to scenarios without such a limit. Finally, applying a threshold
based on the ratio of length between the source entity and the ranges of aligned target
words enhances performance. This improvement is attributed to the filtering out of
incorrect alignments involving single words, which would otherwise be incorrectly part of projections.

The subsequent step involves evaluating the performance of the proposed ILP
formulation \eqref{eq:ilp} for the projection step and identifying which type
of constraints \eqref{eq:num_proj_const} yield superior results. Additionally, a
comparison was made among all proposed matching scores, excluding fused scores,
as well as between greedy and exact solvers. The results including the model transfer pipeline
are presented in Table \ref{tab:europarl_ilp_f1}. It is important to note that, given the
the variables in the ILP problem are binary, certain constraint types can be
omitted from testing, for example,
\( \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} < n_{proj} \Leftrightarrow \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} \leq n_{proj} - 1 \).

\begin{table}[t]
  \centering
  \input{mainmatter/tables/europarl_ilp}
  \caption{Overall F1 scores for the model transfer and ILP based projection pipelines
    on the Europarl NER dataset. Here \textit{align} refers to the alignment-based score,
    \textit{ner} denotes the NER model-based score, and \textit{nmt} corresponds to
  the translation-based score.}
  \label{tab:europarl_ilp_f1}
\end{table}

First and foremost, the best results obtained for all languages utilizing the
proposed ILP formulation surpass the highest results achieved with the heuristic
word-to-word alignment-based algorithm. In the case of the Italian and Spanish languages,
this improved performance is attributed to the superior effectiveness of the
translation-based cost, whereas alignment-based matching scores exhibit inferior results.
This discrepancy can be explained by the heuristic algorithm's capability to merge aligned
ranges if only one misaligned word exists between them, while the ILP formulation with
alignment-based scoring can achieve similar merging only under specific circumstances.
Nevertheless, for the German language, the proposed alignment-based matching score
within the ILP-based projection step pipeline demonstrated superior results across
all experimental evaluations.

In the comparison between the greedy algorithm \ref{alg:ilp_greedy} and the exact solver
GUROBI, it is observed that, in most cases, the greedy algorithm performs better.
There are two primary reasons for this outcome. The first reason is that the exact
ILP solver may enforce or misuse constraints, leading to suboptimal solutions from an
application perspective. When constraints are of the type \( = \), the solver may
include incorrect projections in an effort to satisfy the constraints. When constraints
are \( \leq 2 \), it tends to favor smaller, non-overlapping candidates with a higher
overall cost rather than opting for a longer candidate with high cost. In cases where
constraints are \( \geq \), the solver attempts to project the source entity to as many
target candidates as possible, given that all scores are non-negative.
Conversely, the greedy algorithm consistently selects the candidate with the maximum score
for projection and does not generally adhere to the constraints \eqref{eq:num_proj_const}
for the cases of \( =, \geq, > \).

However, this does not account for why, in certain experiments involving the Italian and
Spanish languages with constraints of type \( \leq 1 \), the greedy algorithm still
slightly outperforms the exact solver. This discrepancy can be attributed to the fact that all
matching score calculations involve models that may themselves introduce errors,
resulting in scores that are not always aligned with the application problem of projection.
Nonetheless, it is noteworthy that for the best solutions, the GUROBI optimizer
outperformed the greedy algorithm in all cases.

Furthermore, it is important to note the runtimes of the various pipelines.
The pipeline employing the greedy algorithm is up to two times faster than that
utilizing the exact solver and is comparable to the runtime of the heuristic-based
algorithm. Conversely, the runtime for the pipelines that incorporate the
translation-based score, despite yielding the best metrics, is significantly longer
than that of all other pipelines, as it necessitates the execution of a translation
model to compute the scores. The runtimes are detailed in the Appendix \ref{sec:appendix}, specifically
in Tables \ref{tab:europarl_heur_runtime} and \ref{tab:europarl_ilp_runtime}.

Another important observation is that the ILP-based projection pipeline utilizing NER
model-based matching costs significantly outperforms the model transfer approach,
despite the fact that the model that has been used and, consequently, the outputs of the model
were identical. The improvement in F1 score can be attributed to the fact that projection
pipelines project source entities, thereby filtering out any predictions from the model
that correspond to classes not present among the source entities. And in addition, the
application of constraints \eqref{eq:num_proj_const} imposes limitations on the number
of entities of each class, which explains the circumstances under which the metric
for the \( \geq 2 \) type of constraints appears lower to that of model transfer.

Thus, for the subsequent experiments, the constraints \eqref{eq:num_proj_const} will
be fixed in the form of \( \leq 1 \). Additionally, it is noted that the translation-based
matching scores yield the best results and should consequently dominate in all fused scores.

\section{Intrinsic evaluation within a full pipeline}
It is crucial to evaluate the proposed ILP-based projection step not only in isolated
settings but also within the complete pipeline, as errors introduced by the translation
and source NER labeling may accumulate and negate the advantages of the proposed
formulation. Therefore, the projection step of the XLNER pipeline
in the form of the ILP problem \eqref{eq:ilp} has been evaluated  using the MasakhaNER2
dataset. This dataset comprises labeled sentences in 20 African low-resource
languages and classifies all entities into four categories: person (\textit{PER}),
organization (\textit{ORG}), location (\textit{LOC}), and date (\textit{DATE}).
However, since the source NER model employed does not support the DATE class, this
particular class was disregarded during the metric computation.

English has been selected as the source language. The forward translation model used
for all experiments is NLLB-200-3.3B \cite{nllbteam2022languageleftbehindscaling}.
It should be noted that this model supports only 18 of the 20 languages included
in the MasakhaNER2 dataset; therefore, only these languages will be considered in
our evaluation.

In comparison to the isolated evaluation of the projection step presented in the
previous section, this evaluation also includes fused scores \eqref{eq:fused_cost},
which represent a weighted sum of the basic matching scores. All ILP-based pipelines
that have been evaluated are characterized in terms of fused scores, with the
corresponding weights provided in Table \ref{tab:ilp_pipeline_explation}.

\begin{table}[ht]
  \centering
  \begin{tabular}{lrrrr}
    \toprule
    \( \lambda \) & align \eqref{eq:align_cost} & ner \eqref{eq:ner_cost} & ner \eqref{eq:ner_cost_wo_classes} & nmt \eqref{eq:nmt_cost} \\
    Pipeline & & & & \\
    \midrule
    align & 1 & 0 & 0 & 0 \\
    ner & 0 & 1 & 0 & 0 \\
    nmt & 0 & 0 & 0 & 1 \\
    align\_ner & 0.5 & 0.5 & 0 & 0 \\
    align\_ner\_spans & 0.5 & 0 & 0.5 & 0 \\
    align\_nmt & 0.5 & 0 & 0 & 1 \\
    ner\_spans\_nmt & 0 & 0 & 0.5 & 1 \\
    all\_fusion & 0.5 & 0 & 0.5 & 1 \\
    \bottomrule
  \end{tabular}
  \caption{A description of each tested ILP-based pipeline in a form of weights
  of the general fused score}
  \label{tab:ilp_pipeline_explation}
\end{table}

The results of the evaluation of all these pipelines, along with the heuristic
word-to-word alignment-based algorithm and the model transfer approach, are presented
in Table \ref{tab:masakhaner2_f1}. \textbf{Bold} text indicates the best overall result for a
language, while \underline{underlined} text signifies the best result among the pipelines featuring
the ILP-based projection step. The hyperparameters for the heuristic and the type of
constraints \eqref{eq:num_proj_const} have been selected based on the findings from the
previous section, specifically corresponding to the best results from the experiments
denoted in Tables \ref{tab:europarl_heur_f1} and \ref{tab:europarl_ilp_f1}.

\begin{table}[ht]
  \input{mainmatter/tables/mashkaner2}
  \caption{Overall F1 scores for XLNER pipelines with different projection steps on the
  MasakhaNER2 dataset}
  \label{tab:masakhaner2_f1}
\end{table}

The results indicate that the proposed ILP-based projection step within the full
XLNER pipeline performs overall better than both the best heuristic word-to-word
alignment-based and model transfer approaches. This advantage is evident in both
the number of languages where the proposed pipeline outperforms the others and in
the average metrics across all tested languages. Specifically, for only 5 of the 18
languages, the proposed projection formulation yields worse results than the previous
methods, although the gap in metrics remains small. In instances where the proposed
method performs better, the differences in metrics can be significant, as illustrated
by the case of isiXhosa (xho), Yorùbá (yor) and isiZulu (zul) languages.

Among the various matching scores, the \textit{all\_fusion} score, which employs a
combination of all proposed matching costs, demonstrates the best results. This can
be attributed to the fact that the \textit{all\_fusion} matching score integrates the advantages
of each individual score, thereby providing a more accurate representation of whether
a source entity should be projected onto a target candidate. In situations where the
\textit{all\_fusion} score performs worse than any other ILP-based pipeline, it
suggests that one of the individual compound scores make from the overall
fused matching score worse due to its inherent drawbacks.

Similar to the isolated evaluation, the results for the heuristic word-to-word
alignment-based algorithm are slightly superior to those for the ILP-based projection
utilizing alignment-based matching scores, attributable to the same rationale: the
heuristic algorithm can consistently merge two aligned word ranges that are separated
by only one non-aligned word to any source entity.

At the same time, the pipeline \textit{ner}, which utilizes the NER model-based matching
score, consistently outperforms (sometimes in 1.5 times) the \textit{Model transfer} approach when provided with the same
NER model outputs, for the same reasons described in the previous section. However,
being projection-based, it may be susceptible to errors from the preceding steps, namely
translation and source NER model labeling errors, which can adversely impact performance,
as observed in the cases of the Chichewa (nya) and Kiswahili (swa) languages.

Additionally, the score \eqref{eq:ner_cost_wo_classes}, referred to as \textit{ner\_spans},
involves utilizing the predictions of the NER model solely to assess the likelihood
that a given candidate may represent a valid target entity of any class. This approach
leverages other scores to assign the appropriate label to this span.
It demonstrates results that are comparable to those of other pipelines.
Notably, for the Bambara (bam) and Mossi (mos) languages, it performs the best, as
it is capable of disregarding incorrectly predicted NER model labels for spans that
correspond to actual target entities.
