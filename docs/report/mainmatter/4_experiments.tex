\chapter{Evaluation}
\label{sec:experiments}

The evaluation of the XLNER pipelines can be classified into two groups: extrinsic and
intrinsic. Extrinsic evaluation involves utilizing the XLNER pipeline to produce a
labeled dataset in the target language, followed by training a NER model on this dataset.
In this context, the performance of the trained NER model serves as a measure of quality.
A primary concern with this approach is its dependency on the training procedure; that is,
results may vary due to different random sampling of batches, alterations in batch size,
mixing of generated data with manually labeled data, other factors. Furthermore,
this process demands substantial computational resources to train a model, which becomes
problematic when conducting hundreds of experiments.

Consequently, the results of intrinsic evaluation will be presented in this chapter.
This entails taking a manually labeled dataset in the target language, feeding it to
the pipeline to generate labels, and comparing the predictions with the ground truth
labels. It is reasonable to anticipate that superior intrinsic performance will
correlate with improved extrinsic performance, assuming a consistent training setup.

To assess the proposed formulation of the projection step of the XLNER pipeline as an
ILP problem, as delineated in \eqref{eq:ilp}, and to analyze various forms of it, two
sets of experiments have been conducted. The first set evaluates the performance of
the projection step in isolation. The second set of experiments confirms the quality
of projections within the complete XLNER pipeline on the MasakhaNER2 dataset
\cite{adelani-etal-2022-masakhaner}. The source code utilized for conducting all
experiments is made publicly available on GitHub\footnote{\url{https://github.com/ShkalikovOleh/master-thesis}}.
The GUROBI optimizer \cite{gurobi}, employed under an academic license, served as the
ILP solver for all experiments.

Word-to-word alignments for all experiments were computed using a non-fine-tuned
AWESOME aligner with the following default hyperparameters: extraction method set
to softmax, softmax\_threshold of \( 0.001 \), and align\_layer is \( 8 \).

For source labeling, candidate evaluation, and model transfer, the MDeBERTa-v3-base
model, fine-tuned on the English split of the CONLL-2003 dataset \cite{tjong-kim-sang-de-meulder-2003-introduction-conll},
was utilized. This selection is based by the findings of the original study in which
MasakhaNER2 was introduced, demonstrating that MDeBERTa-v3 achieves superior performance
compared to other multilingual non-African-centric models, despite being smaller than
XLM-RoBERTA-Large \cite{conneau-etal-2020-unsupervised-xlmr}. The model was trained
for 5 epochs, with a total batch size of \( 32 \) (consisting of \( 16 \) with gradient
accumulation every 2 steps), utilizing the Adam optimizer \cite{Kingma2014AdamAM} with
betas set to \( (0.9,0.999) \) and a learning rate of \( 2 \cdot 10^{-5} \). The model
is publicly accessible on the HuggingFace Hub\footnote{\url{https://huggingface.co/ShkalikovOleh/mdeberta-v3-base-conll2003-en}}.

\section{Isolated evaluation of the projection step}
The XLNER pipeline comprises three steps: forward translation, source NER labeling,
and projection. Each of these steps can introduce errors. The new ILP-based approach for the
projection step has been proposed, and it is essential to evaluate the performance
of this step independently from errors associated with the preceding steps.

This necessitates the availability of a labelled dataset for parallel text,
which would enable the exclusion of the translation and source labeling phases.
The Europarl-NER dataset \cite{agerri-etal-2018-building} serves as such a dataset.
This dataset consists of 799 parallel sentences derived from the Europarl corpus \cite{koehn2005europarl}, manually annotated according to four entity types, adhering to the CoNLL 2002 and 2003 guidelines for four languages: English, German, Italian, and Spanish.

\begin{table}[ht]
  \centering
  \input{mainmatter/tables/europarl_heuristic}
  \caption{Overall F1 scores for word-to-word alignments-based heuristic
  algorithm with different hyperparameter  on the Europarl NER dataset}
  \label{tab:europarl_heur_f1}
\end{table}

Consequently, it becomes feasible to evaluate the performance of the proposed
projection step as the ILP problem in isolation. Prior to this, however, it is
important to investigate the performance of the heuristic word-to-word alignment-based
algorithm \ref{alg:heuristics} to determine the optimal combination of hyperparameters
and the resulting performance metrics for comparison with the proposed projection step.
The F1 scores for the heuristic algorithm, assessed with varying hyperparameters, are
presented in Table \ref{tab:europarl_heur_f1}.

The results indicate that the optimal combination of hyperparameters for the majority of
languages is as follows: a merging distance \( d = 1 \), an unrestricted maximum number
of aligned subranges sorted by length that can be projected (i.e., \( k \) should not be
limited), merging of aligned ranges should disregard whether the first word of the
right-aligned range is aligned to the first word of a source entity, and a word length
ratio threshold of \( 0.8 \)  between the source entity and the target range of words.
Hence, these hyperparameters will be employed for subsequent experiments.

The overall results are as follows. The most significant improvement in performance
occurs when the algorithm is permitted to merge aligned ranges together, provided
that only one non-aligned word exists between them. This enhancement is attributed to
the algorithm's ability to fill gaps in imperfect or missed alignments. Concurrently,
imposing a limit \( k \) on the number of projected target ranges for any source entity
results in a slight decrease in performance. Additionally, due to errors in alignments,
the merging of aligned subranges, which only occurs when the right range begins with a
word aligned to the first word of a source entity, leads to outcomes that are less
favorable in comparison to scenarios without such a limit. Finally, applying a threshold
based on the ratio of length between the source entity and the ranges of aligned target
words enhances performance. This improvement is attributed to the filtering out of
incorrect alignments involving single words, which would otherwise be incorrectly part of projections.

\begin{table}[t]
  \centering
  \input{mainmatter/tables/europarl_ilp}
  \caption{Overall F1 scores for the model transfer and ILP based projection pipelines
    on the Europarl NER dataset. Here \textit{align} refers to the alignment-based score,
    \textit{ner} denotes the NER model-based score, and \textit{nmt} corresponds to
  the translation-based score.}
  \label{tab:europarl_ilp_f1}
\end{table}

The subsequent step involves evaluating the performance of the proposed ILP
formulation \eqref{eq:ilp} for the projection step and identifying which type
of constraints \eqref{eq:num_proj_const} yield superior results. Additionally, a
comparison was made among all proposed matching scores, excluding fused scores,
as well as between greedy and exact solvers. The results including the model transfer pipeline
are presented in Table \ref{tab:europarl_ilp_f1}. It is important to note that, given the
the variables in the ILP problem are binary, certain constraint types can be
omitted from testing, for example,
\( \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} < n_{proj} \Leftrightarrow \sum\limits_{\tgt{p} \in T} x_{\src{p}, \tgt{p}} \leq n_{proj} - 1 \).

First and foremost, the best results obtained for all languages utilizing the
proposed ILP formulation surpass the highest results achieved with the heuristic
word-to-word alignment-based algorithm. In the case of the Italian and Spanish languages,
this improved performance is attributed to the superior effectiveness of the
translation-based cost, whereas alignment-based matching scores exhibit inferior results.
This discrepancy can be explained by the heuristic algorithm's capability to merge aligned
ranges if only one misaligned word exists between them, while the ILP formulation with
alignment-based scoring can achieve similar merging only under specific circumstances.
Nevertheless, for the German language, the proposed alignment-based matching score
within the ILP-based projection step pipeline demonstrated superior results across
all experimental evaluations.

In the comparison between the greedy algorithm \ref{alg:ilp_greedy} and the exact solver
GUROBI, it is observed that, in most cases, the greedy algorithm performs better.
There are two primary reasons for this outcome. The first reason is that the exact
ILP solver may enforce or misuse constraints, leading to suboptimal solutions from an
application perspective. When constraints are of the type \( = \), the solver may
include incorrect projections in an effort to satisfy the constraints. When constraints
are \( \leq 2 \), it tends to favor smaller, non-overlapping candidates with a higher
overall cost rather than opting for a longer candidate with high cost. In cases where
constraints are \( \geq \), the solver attempts to project the source entity to as many
target candidates as possible, given that all scores are non-negative.
Conversely, the greedy algorithm consistently selects the candidate with the maximum score
for projection and does not generally adhere to the constraints \eqref{eq:num_proj_const}
for the cases of \( =, \geq, > \).

However, this does not account for why, in certain experiments involving the Italian and
Spanish languages with constraints of type \( \leq 1 \), the greedy algorithm still
outperforms the exact solver. This discrepancy can be attributed to the fact that all
matching score calculations involve models that may themselves introduce errors,
resulting in scores that are not always aligned with the application problem of projection.
Nonetheless, it is noteworthy that for the best solutions, the GUROBI optimizer
outperformed the greedy algorithm in all cases.

Furthermore, it is important to note the runtimes of the various pipelines.
The pipeline employing the greedy algorithm is up to two times faster than that
utilizing the exact solver and is comparable to the runtime of the heuristic-based
algorithm. Conversely, the runtime for the pipelines that incorporate the
translation-based score, despite yielding the best metrics, is significantly longer
than that of all other pipelines, as it necessitates the execution of a translation
model to compute the scores. The runtimes are detailed in the Appendix \ref{sec:appendix}, specifically
in Tables \ref{tab:europarl_heur_runtime} and \ref{tab:europarl_ilp_runtime}.



\section{Intrinsic evaluation within a full pipeline}
masakhaner2 results

\begin{table}[ht]
  \input{mainmatter/tables/mashkaner2}
  \caption{Overall F1 scores for different projection steps on the
  MasakhaNER2 dataset}
  \label{tab:masakhaner2_f1}
\end{table}
