@book{pemmaraju2003computational,
  title     = {Computational discrete mathematics: Combinatorics and graph theory with mathematica{\textregistered}},
  author    = {Pemmaraju, Sriram and Skiena, Steven},
  year      = {2003},
  publisher = {Cambridge university press}
}
@article{PalB96,
  author    = {Madhumangal Pal and
               G. P. Bhattacharjee},
  title     = {A sequential algorithm for finding a maximum weight \emph{K}-independent
               set on interval graphs},
  journal   = {Int. J. Comput. Math.},
  volume    = {60},
  number    = {3-4},
  pages     = {205--214},
  year      = {1996},
  url       = {https://doi.org/10.1080/00207169608804486},
  doi       = {10.1080/00207169608804486},
  timestamp = {Thu, 16 May 2019 19:54:57 +0200},
  biburl    = {https://dblp.org/rec/journals/ijcm/PalB96.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bhattacharya2014maximum,
  title     = {Maximum Independent Set for Interval Graphs and Trees in Space Efficient Models.},
  author    = {Bhattacharya, Binay K and De, Minati and Nandy, Subhas C and Roy, Sasanka},
  booktitle = {CCCG},
  year      = {2014}
}
@inproceedings{garcia-ferrero-etal-2023-projection,
  title     = {{T}-Projection: High Quality Annotation Projection for Sequence Labeling Tasks},
  author    = {Garc{\'\i}a-Ferrero, Iker  and
               Agerri, Rodrigo  and
               Rigau, German},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.1015},
  doi       = {10.18653/v1/2023.findings-emnlp.1015},
  pages     = {15203--15217},
  abstract  = {In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text2text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-European and 8 low-resource African languages. We demostrate that T-projection outperforms previous annotation projection methods by a wide margin. We believe that T-Projection can help to automatically alleviate the lack of high-quality training data for sequence labeling tasks. Code and data are publicly available.}
}
@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, A},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017}
}
@misc{gurobi,
  author = {{Gurobi Optimization, LLC}},
  title  = {{Gurobi Optimizer Reference Manual}},
  year   = 2024,
  url    = {https://www.gurobi.com}
}
@inproceedings{adelani-etal-2022-masakhaner,
  title     = {{M}asakha{NER} 2.0: {A}frica-centric Transfer Learning for Named Entity Recognition},
  author    = {Adelani, David  and
               Neubig, Graham  and
               Ruder, Sebastian  and
               Rijhwani, Shruti  and
               Beukman, Michael  and
               Palen-Michel, Chester  and
               Lignos, Constantine  and
               Alabi, Jesujoba  and
               Muhammad, Shamsuddeen  and
               Nabende, Peter  and
               Dione, Cheikh M. Bamba  and
               Bukula, Andiswa  and
               Mabuya, Rooweither  and
               Dossou, Bonaventure F. P.  and
               Sibanda, Blessing  and
               Buzaaba, Happy  and
               Mukiibi, Jonathan  and
               Kalipe, Godson  and
               Mbaye, Derguene  and
               Taylor, Amelia  and
               Kabore, Fatoumata  and
               Emezue, Chris Chinenye  and
               Aremu, Anuoluwapo  and
               Ogayo, Perez  and
               Gitau, Catherine  and
               Munkoh-Buabeng, Edwin  and
               Memdjokam Koagne, Victoire  and
               Tapo, Allahsera Auguste  and
               Macucwa, Tebogo  and
               Marivate, Vukosi  and
               Elvis, Mboning Tchiaze  and
               Gwadabe, Tajuddeen  and
               Adewumi, Tosin  and
               Ahia, Orevaoghene  and
               Nakatumba-Nabende, Joyce},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.298},
  doi       = {10.18653/v1/2022.emnlp-main.298},
  pages     = {4488--4508},
  abstract  = {African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14{\%} over 20 languages as compared to using English.}
}
@inproceedings{conneau-etal-2020-unsupervised-xlmr,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}
@article{Kingma2014AdamAM,
  title   = {Adam: A Method for Stochastic Optimization},
  author  = {Diederik P. Kingma and Jimmy Ba},
  journal = {CoRR},
  year    = {2014},
  volume  = {abs/1412.6980},
  url     = {https://api.semanticscholar.org/CorpusID:6628106}
}
@inproceedings{tjong-kim-sang-de-meulder-2003-introduction-conll,
  title     = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.  and
               De Meulder, Fien},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419},
  pages     = {142--147}
}
@inproceedings{agerri-etal-2018-building,
  title     = {Building Named Entity Recognition Taggers via Parallel Corpora},
  author    = {Agerri, Rodrigo  and
               Chung, Yiling  and
               Aldabe, Itziar  and
               Aranberri, Nora  and
               Labaka, Gorka  and
               Rigau, German},
  editor    = {Calzolari, Nicoletta  and
               Choukri, Khalid  and
               Cieri, Christopher  and
               Declerck, Thierry  and
               Goggi, Sara  and
               Hasida, Koiti  and
               Isahara, Hitoshi  and
               Maegaard, Bente  and
               Mariani, Joseph  and
               Mazo, H{\'e}l{\`e}ne  and
               Moreno, Asuncion  and
               Odijk, Jan  and
               Piperidis, Stelios  and
               Tokunaga, Takenobu},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  month     = may,
  year      = {2018},
  address   = {Miyazaki, Japan},
  publisher = {European Language Resources Association (ELRA)},
  url       = {https://aclanthology.org/L18-1557}
}
@inproceedings{koehn2005europarl,
  title     = {Europarl: A parallel corpus for statistical machine translation},
  author    = {Koehn, Philipp},
  booktitle = {Proceedings of machine translation summit x: papers},
  pages     = {79--86},
  year      = {2005}
}
@misc{nllbteam2022languageleftbehindscaling,
  title         = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  author        = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  year          = {2022},
  eprint        = {2207.04672},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2207.04672}
}
@inproceedings{vamvas_sennrich_2022_nmtscore,
  title     = {{NMTS}core: A Multilingual Analysis of Translation-based Text Similarity Measures},
  author    = {Vamvas, Jannis  and
               Sennrich, Rico},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.15},
  doi       = {10.18653/v1/2022.findings-emnlp.15},
  pages     = {198--213},
  abstract  = {Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.}
}
@article{dyckhoff1981new,
  title     = {A new linear programming approach to the cutting stock problem},
  author    = {Dyckhoff, Harald},
  journal   = {Operations Research},
  volume    = {29},
  number    = {6},
  pages     = {1092--1104},
  year      = {1981},
  publisher = {INFORMS}
}
@article{gilmore1963linear,
  title     = {A linear programming approach to the cutting stock problem—Part II},
  author    = {Gilmore, Paul C and Gomory, Ralph E},
  journal   = {Operations research},
  volume    = {11},
  number    = {6},
  pages     = {863--888},
  year      = {1963},
  publisher = {INFORMS}
}
@book{land2010automatic,
  title     = {An automatic method for solving discrete programming problems},
  author    = {Land, Ailsa H and Doig, Alison G},
  year      = {2010},
  publisher = {Springer}
}
@article{branchAndCut,
  author   = {Padberg, Manfred and Rinaldi, Giovanni},
  title    = {A Branch-and-Cut Algorithm for the Resolution of Large-Scale Symmetric Traveling Salesman Problems},
  journal  = {SIAM Review},
  volume   = {33},
  number   = {1},
  pages    = {60-100},
  year     = {1991},
  doi      = {10.1137/1033004},
  url      = {https://doi.org/10.1137/1033004},
  eprint   = {https://doi.org/10.1137/1033004},
  abstract = { An algorithm is described for solving large-scale instances of the Symmetric Traveling Salesman Problem (STSP) to optimality. The core of the algorithm is a “polyhedral” cutting-plane procedure that exploits a subset of the system of linear inequalities defining the convex hull of the incidence vectors of the hamiltonian cycles of a complete graph. The cuts are generated by several identification procedures that have been described in a companion paper. Whenever the cutting-plane procedure does not terminate with an optimal solution the algorithm uses a tree-search strategy that, as opposed to branch-and-bound, keeps on producing cuts after branching. The algorithm has been implemented in FORTRAN. Two different linear programming (LP) packages have been used as the LP solver. The implementation of the algorithm and the interface with one of the LP solvers is described in sufficient detail to permit the replication of our experiments. Computational results are reported with up to 42 STSPs with sizes ranging from 48 to 2,392 nodes. Most of the medium-sized test problems are taken from the literature; all others are large-scale real-world problems. All of the instances considered in this study were solved to optimality by the algorithm in “reasonable” computation times. }
}
@article{weikum2021machine,
  title     = {Machine knowledge: Creation and curation of comprehensive knowledge bases},
  author    = {Weikum, Gerhard and Dong, Xin Luna and Razniewski, Simon and Suchanek, Fabian and others},
  journal   = {Foundations and Trends{\textregistered} in Databases},
  volume    = {10},
  number    = {2-4},
  pages     = {108--490},
  year      = {2021},
  publisher = {Now Publishers, Inc.}
}
@inproceedings{zhang-etal-2022-efficient-robust,
  title     = {Efficient and Robust Knowledge Graph Construction},
  author    = {Zhang, Ningyu  and
               Gui, Tao  and
               Nan, Guoshun},
  editor    = {Alonso, Miguel A.  and
               Wei, Zhongyu},
  booktitle = {Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Tutorial Abstracts},
  month     = nov,
  year      = {2022},
  address   = {Taipei},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.aacl-tutorials.1},
  doi       = {10.18653/v1/2022.aacl-tutorials.1},
  pages     = {1--7},
  abstract  = {Knowledge graph construction which aims to extract knowledge from the text corpus, has appealed to the NLP community researchers. Previous decades have witnessed the remarkable progress of knowledge graph construction on the basis of neural models; however, those models often cost massive computation or labeled data resources and suffer from unstable inference accounting for biased or adversarial samples. Recently, numerous approaches have been explored to mitigate the efficiency and robustness issues for knowledge graph construction, such as prompt learning and adversarial training. In this tutorial, we aim to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction. Additionally, our goal is to provide a systematic and up-to-date overview of these methods and reveal new research opportunities to the audience.}
}
@inproceedings{lai-etal-2024-llms,
  title     = {{LLM}s Beyond {E}nglish: Scaling the Multilingual Capability of {LLM}s with Cross-Lingual Feedback},
  author    = {Lai, Wen  and
               Mesgar, Mohsen  and
               Fraser, Alexander},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-acl.488},
  doi       = {10.18653/v1/2024.findings-acl.488},
  pages     = {8186--8213},
  abstract  = {To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.}
}
@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@inproceedings{gpt3,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}
@misc{sainz2024gollieannotationguidelinesimprove,
  title         = {GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction},
  author        = {Oscar Sainz and Iker García-Ferrero and Rodrigo Agerri and Oier Lopez de Lacalle and German Rigau and Eneko Agirre},
  year          = {2024},
  eprint        = {2310.03668},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.03668}
}
@article{zhou2023universalner,
  title         = {UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition},
  author        = {Wenxuan Zhou and Sheng Zhang and Yu Gu and Muhao Chen and Hoifung Poon},
  year          = {2023},
  eprint        = {2308.03279},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{parekh-etal-2024-contextual,
  title     = {Contextual Label Projection for Cross-Lingual Structured Prediction},
  author    = {Parekh, Tanmay  and
               Hsu, I-Hung  and
               Huang, Kuan-Hao  and
               Chang, Kai-Wei  and
               Peng, Nanyun},
  editor    = {Duh, Kevin  and
               Gomez, Helena  and
               Bethard, Steven},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  month     = jun,
  year      = {2024},
  address   = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.naacl-long.321},
  doi       = {10.18653/v1/2024.naacl-long.321},
  pages     = {5738--5757},
  abstract  = {Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs *contextual translation* on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on two representative structured prediction tasks - event argument extraction (EAE) and named entity recognition (NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER. We further explore the applicability of CLaP on ten extremely low-resource languages to showcase its potential for cross-lingual structured prediction.}
}
@article{Le2024ConstrainedDF,
  title   = {Constrained Decoding for Cross-lingual Label Projection},
  author  = {Duong Minh Le and Yang Chen and Alan Ritter and Wei Xu},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2402.03131},
  url     = {https://api.semanticscholar.org/CorpusID:267412651}
}
@inproceedings{liu-etal-2021-mulda,
  title     = {{M}ul{DA}: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual {NER}},
  author    = {Liu, Linlin  and
               Ding, Bosheng  and
               Bing, Lidong  and
               Joty, Shafiq  and
               Si, Luo  and
               Miao, Chunyan},
  editor    = {Zong, Chengqing  and
               Xia, Fei  and
               Li, Wenjie  and
               Navigli, Roberto},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.453},
  doi       = {10.18653/v1/2021.acl-long.453},
  pages     = {5834--5846},
  abstract  = {Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.}
}
@misc{transfusion,
  author = {Chen, Yang and Shah, Vedaant and Ritter, Alan},
  year   = {2023},
  month  = {05},
  pages  = {},
  title  = {Better Low-Resource Entity Recognition Through Translation and Annotation Fusion},
  doi    = {10.48550/arXiv.2305.13582}
}
@inproceedings{yang-etal-2022-crop,
  title     = {{CROP}: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation},
  author    = {Yang, Jian  and
               Huang, Shaohan  and
               Ma, Shuming  and
               Yin, Yuwei  and
               Dong, Li  and
               Zhang, Dongdong  and
               Guo, Hongcheng  and
               Li, Zhoujun  and
               Wei, Furu},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.34},
  doi       = {10.18653/v1/2022.findings-emnlp.34},
  pages     = {486--496},
  abstract  = {Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned cross-lingual representations or machine translation results. However, the performance of cross-lingual NER methods is severely affected by the unsatisfactory quality of translation or label projection. To address these problems, we propose a Cross-lingual Entity Projection framework (CROP) to enable zero-shot cross-lingual NER with the help of a multilingual labeled sequence translation model. Specifically, the target sequence is first translated into the source language and then tagged by a source NER model. We further adopt a labeled sequence translation model to project the tagged sequence back to the target language and label the target raw sentence. Ultimately, the whole pipeline is integrated into an end-to-end model by the way of self-training. Experimental results on two benchmarks demonstrate that our method substantially outperforms the previous strong baseline by a large margin of +3 7 F1 scores and achieves state-of-the-art performance.}
}
@inproceedings{unitrans,
  author    = {Wu, Qianhui and Lin, Zijia and Karlsson, B\"{o}rje F. and Huang, Biqing and Lou, Jian-Guang},
  title     = {UniTrans: unifying model transfer and data transfer for cross-lingual named entity recognition with unlabeled data},
  year      = {2021},
  isbn      = {9780999241165},
  abstract  = {Prior works in cross-lingual named entity recognition (NER) with no/little labeled data fall into two primary categories: model transfer based and data transfer based methods. In this paper we find that both method types can complement each other, in the sense that, the former can exploit context information via language-independent features but sees no task-specific information in the target language; while the latter generally generates pseudo target-language training data via translation but its exploitation of context information is weakened by inaccurate translations. Moreover, prior works rarely leverage unlabeled data in the target language, which can be effortlessly collected and potentially contains valuable information for improved results. To handle both problems, we propose a novel approach termed UniTrans to Unify both model and data Transfer for cross-lingual NER, and furthermore, to leverage the available information from unlabeled target-language data via enhanced knowledge distillation. We evaluate our proposed UniTrans over 4 target languages on benchmark datasets. Our experimental results show that it substantially outperforms the existing state-of-the-art methods.},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
  articleno = {543},
  numpages  = {7},
  location  = {Yokohama, Yokohama, Japan},
  series    = {IJCAI'20}
}
@inproceedings{chen-etal-2023-frustratingly,
  title     = {Frustratingly Easy Label Projection for Cross-lingual Transfer},
  author    = {Chen, Yang  and
               Jiang, Chao  and
               Ritter, Alan  and
               Xu, Wei},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.357},
  doi       = {10.18653/v1/2023.findings-acl.357},
  pages     = {5775--5796},
  abstract  = {Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we call EasyProject, is easily applied to many languages and works surprisingly well, outperforming the more complex word alignment-based methods. We analyze several key factors that affect the end-task performance, and show EasyProject works well because it can accurately preserve label span boundaries after translation. We will publicly release all our code and data.}
}
@inproceedings{torge-etal-2023-named,
  title     = {Named Entity Recognition for Low-Resource Languages - Profiting from Language Families},
  author    = {Torge, Sunna  and
               Politov, Andrei  and
               Lehmann, Christoph  and
               Saffar, Bochra  and
               Tao, Ziyan},
  editor    = {Piskorski, Jakub  and
               Marci{\'n}czuk, Micha{\l}  and
               Nakov, Preslav  and
               Ogrodniczuk, Maciej  and
               Pollak, Senja  and
               P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
               Rybak, Piotr  and
               Steinberger, Josef  and
               Yangarber, Roman},
  booktitle = {Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023)},
  month     = may,
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.bsnlp-1.1},
  doi       = {10.18653/v1/2023.bsnlp-1.1},
  pages     = {1--10},
  abstract  = {Machine learning drives forward the development in many areas of Natural Language Processing (NLP). Until now, many NLP systems and research are focusing on high-resource languages, i.e. languages for which many data resources exist. Recently, so-called low-resource languages increasingly come into focus. In this context, multi-lingual language models, which are trained on related languages to a target low-resource language, may enable NLP tasks on this low-resource language. In this work, we investigate the use of multi-lingual models for Named Entity Recognition (NER) for low-resource languages. We consider the West Slavic language family and the low-resource languages Upper Sorbian and Kashubian. Three RoBERTa models were trained from scratch, two mono-lingual models for Czech and Polish, and one bi-lingual model for Czech and Polish. These models were evaluated on the NER downstream task for Czech, Polish, Upper Sorbian, and Kashubian, and compared to existing state-of-the-art models such as RobeCzech, HerBERT, and XLM-R. The results indicate that the mono-lingual models perform better on the language they were trained on, and both the mono-lingual and language family models outperform the large multi-lingual model in downstream tasks. Overall, the study shows that low-resource West Slavic languages can benefit from closely related languages and their models.}
}
@inproceedings{garcia-ferrero-etal-2022-model,
  title     = {Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings},
  author    = {Garc{\'\i}a-Ferrero, Iker  and
               Agerri, Rodrigo  and
               Rigau, German},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.478},
  doi       = {10.18653/v1/2022.findings-emnlp.478},
  pages     = {6403--6416},
  abstract  = {Zero-resource cross-lingual transfer approaches aim to apply supervised modelsfrom a source language to unlabelled target languages. In this paper we performan in-depth study of the two main techniques employed so far for cross-lingualzero-resource sequence labelling, based either on data or model transfer.Although previous research has proposed translation and annotation projection(data-based cross-lingual transfer) as an effective technique for cross-lingualsequence labelling, in this paper we experimentally demonstrate that highcapacity multilingual language models applied in a zero-shot (model-basedcross-lingual transfer) setting consistently outperform data-basedcross-lingual transfer approaches. A detailed analysis of our results suggeststhat this might be due to important differences in language use. Morespecifically, machine translation often generates a textual signal which isdifferent to what the models are exposed to when using gold standard data,which affects both the fine-tuning and evaluation processes. Our results alsoindicate that data-based cross-lingual transfer approaches remain a competitiveoption when high-capacity multilingual language models are not available.}
}
@article{He2021DeBERTaV3ID,
  title   = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  author  = {Pengcheng He and Jianfeng Gao and Weizhu Chen},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2111.09543},
  url     = {https://api.semanticscholar.org/CorpusID:244346093}
}



