@book{pemmaraju2003computational,
  title     = {Computational discrete mathematics: Combinatorics and graph theory with mathematica{\textregistered}},
  author    = {Pemmaraju, Sriram and Skiena, Steven},
  year      = {2003},
  publisher = {Cambridge university press}
}
@article{PalB96,
  author    = {Madhumangal Pal and
               G. P. Bhattacharjee},
  title     = {A sequential algorithm for finding a maximum weight \emph{K}-independent
               set on interval graphs},
  journal   = {Int. J. Comput. Math.},
  volume    = {60},
  number    = {3-4},
  pages     = {205--214},
  year      = {1996},
  url       = {https://doi.org/10.1080/00207169608804486},
  doi       = {10.1080/00207169608804486},
  timestamp = {Thu, 16 May 2019 19:54:57 +0200},
  biburl    = {https://dblp.org/rec/journals/ijcm/PalB96.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bhattacharya2014maximum,
  title     = {Maximum Independent Set for Interval Graphs and Trees in Space Efficient Models.},
  author    = {Bhattacharya, Binay K and De, Minati and Nandy, Subhas C and Roy, Sasanka},
  booktitle = {CCCG},
  year      = {2014}
}
@inproceedings{garcia-ferrero-etal-2023-projection,
  title     = {{T}-Projection: High Quality Annotation Projection for Sequence Labeling Tasks},
  author    = {Garc{\'\i}a-Ferrero, Iker  and
               Agerri, Rodrigo  and
               Rigau, German},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.1015},
  doi       = {10.18653/v1/2023.findings-emnlp.1015},
  pages     = {15203--15217},
  abstract  = {In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text2text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-European and 8 low-resource African languages. We demostrate that T-projection outperforms previous annotation projection methods by a wide margin. We believe that T-Projection can help to automatically alleviate the lack of high-quality training data for sequence labeling tasks. Code and data are publicly available.}
}
@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, A},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017}
}
@misc{gurobi,
  author = {{Gurobi Optimization, LLC}},
  title  = {{Gurobi Optimizer Reference Manual}},
  year   = 2024,
  url    = {https://www.gurobi.com}
}
@inproceedings{adelani-etal-2022-masakhaner,
  title     = {{M}asakha{NER} 2.0: {A}frica-centric Transfer Learning for Named Entity Recognition},
  author    = {Adelani, David  and
               Neubig, Graham  and
               Ruder, Sebastian  and
               Rijhwani, Shruti  and
               Beukman, Michael  and
               Palen-Michel, Chester  and
               Lignos, Constantine  and
               Alabi, Jesujoba  and
               Muhammad, Shamsuddeen  and
               Nabende, Peter  and
               Dione, Cheikh M. Bamba  and
               Bukula, Andiswa  and
               Mabuya, Rooweither  and
               Dossou, Bonaventure F. P.  and
               Sibanda, Blessing  and
               Buzaaba, Happy  and
               Mukiibi, Jonathan  and
               Kalipe, Godson  and
               Mbaye, Derguene  and
               Taylor, Amelia  and
               Kabore, Fatoumata  and
               Emezue, Chris Chinenye  and
               Aremu, Anuoluwapo  and
               Ogayo, Perez  and
               Gitau, Catherine  and
               Munkoh-Buabeng, Edwin  and
               Memdjokam Koagne, Victoire  and
               Tapo, Allahsera Auguste  and
               Macucwa, Tebogo  and
               Marivate, Vukosi  and
               Elvis, Mboning Tchiaze  and
               Gwadabe, Tajuddeen  and
               Adewumi, Tosin  and
               Ahia, Orevaoghene  and
               Nakatumba-Nabende, Joyce},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.298},
  doi       = {10.18653/v1/2022.emnlp-main.298},
  pages     = {4488--4508},
  abstract  = {African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14{\%} over 20 languages as compared to using English.}
}
@inproceedings{conneau-etal-2020-unsupervised-xlmr,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}
@article{Kingma2014AdamAM,
  title   = {Adam: A Method for Stochastic Optimization},
  author  = {Diederik P. Kingma and Jimmy Ba},
  journal = {CoRR},
  year    = {2014},
  volume  = {abs/1412.6980},
  url     = {https://api.semanticscholar.org/CorpusID:6628106}
}
@inproceedings{tjong-kim-sang-de-meulder-2003-introduction-conll,
  title     = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.  and
               De Meulder, Fien},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419},
  pages     = {142--147}
}
@inproceedings{agerri-etal-2018-building,
  title     = {Building Named Entity Recognition Taggers via Parallel Corpora},
  author    = {Agerri, Rodrigo  and
               Chung, Yiling  and
               Aldabe, Itziar  and
               Aranberri, Nora  and
               Labaka, Gorka  and
               Rigau, German},
  editor    = {Calzolari, Nicoletta  and
               Choukri, Khalid  and
               Cieri, Christopher  and
               Declerck, Thierry  and
               Goggi, Sara  and
               Hasida, Koiti  and
               Isahara, Hitoshi  and
               Maegaard, Bente  and
               Mariani, Joseph  and
               Mazo, H{\'e}l{\`e}ne  and
               Moreno, Asuncion  and
               Odijk, Jan  and
               Piperidis, Stelios  and
               Tokunaga, Takenobu},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  month     = may,
  year      = {2018},
  address   = {Miyazaki, Japan},
  publisher = {European Language Resources Association (ELRA)},
  url       = {https://aclanthology.org/L18-1557}
}
@inproceedings{koehn2005europarl,
  title     = {Europarl: A parallel corpus for statistical machine translation},
  author    = {Koehn, Philipp},
  booktitle = {Proceedings of machine translation summit x: papers},
  pages     = {79--86},
  year      = {2005}
}
@misc{nllbteam2022languageleftbehindscaling,
  title         = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  author        = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  year          = {2022},
  eprint        = {2207.04672},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2207.04672}
}
@inproceedings{vamvas_sennrich_2022_nmtscore,
  title     = {{NMTS}core: A Multilingual Analysis of Translation-based Text Similarity Measures},
  author    = {Vamvas, Jannis  and
               Sennrich, Rico},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.15},
  doi       = {10.18653/v1/2022.findings-emnlp.15},
  pages     = {198--213},
  abstract  = {Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.}
}
@article{dyckhoff1981new,
  title     = {A new linear programming approach to the cutting stock problem},
  author    = {Dyckhoff, Harald},
  journal   = {Operations Research},
  volume    = {29},
  number    = {6},
  pages     = {1092--1104},
  year      = {1981},
  publisher = {INFORMS}
}
@article{gilmore1963linear,
  title     = {A linear programming approach to the cutting stock problem—Part II},
  author    = {Gilmore, Paul C and Gomory, Ralph E},
  journal   = {Operations research},
  volume    = {11},
  number    = {6},
  pages     = {863--888},
  year      = {1963},
  publisher = {INFORMS}
}
@book{land2010automatic,
  title     = {An automatic method for solving discrete programming problems},
  author    = {Land, Ailsa H and Doig, Alison G},
  year      = {2010},
  publisher = {Springer}
}
@article{branchAndCut,
  author   = {Padberg, Manfred and Rinaldi, Giovanni},
  title    = {A Branch-and-Cut Algorithm for the Resolution of Large-Scale Symmetric Traveling Salesman Problems},
  journal  = {SIAM Review},
  volume   = {33},
  number   = {1},
  pages    = {60-100},
  year     = {1991},
  doi      = {10.1137/1033004},
  url      = {https://doi.org/10.1137/1033004},
  eprint   = {https://doi.org/10.1137/1033004},
  abstract = { An algorithm is described for solving large-scale instances of the Symmetric Traveling Salesman Problem (STSP) to optimality. The core of the algorithm is a “polyhedral” cutting-plane procedure that exploits a subset of the system of linear inequalities defining the convex hull of the incidence vectors of the hamiltonian cycles of a complete graph. The cuts are generated by several identification procedures that have been described in a companion paper. Whenever the cutting-plane procedure does not terminate with an optimal solution the algorithm uses a tree-search strategy that, as opposed to branch-and-bound, keeps on producing cuts after branching. The algorithm has been implemented in FORTRAN. Two different linear programming (LP) packages have been used as the LP solver. The implementation of the algorithm and the interface with one of the LP solvers is described in sufficient detail to permit the replication of our experiments. Computational results are reported with up to 42 STSPs with sizes ranging from 48 to 2,392 nodes. Most of the medium-sized test problems are taken from the literature; all others are large-scale real-world problems. All of the instances considered in this study were solved to optimality by the algorithm in “reasonable” computation times. }
}
@article{weikum2021machine,
  title     = {Machine knowledge: Creation and curation of comprehensive knowledge bases},
  author    = {Weikum, Gerhard and Dong, Xin Luna and Razniewski, Simon and Suchanek, Fabian and others},
  journal   = {Foundations and Trends{\textregistered} in Databases},
  volume    = {10},
  number    = {2-4},
  pages     = {108--490},
  year      = {2021},
  publisher = {Now Publishers, Inc.}
}
@inproceedings{zhang-etal-2022-efficient-robust,
  title     = {Efficient and Robust Knowledge Graph Construction},
  author    = {Zhang, Ningyu  and
               Gui, Tao  and
               Nan, Guoshun},
  editor    = {Alonso, Miguel A.  and
               Wei, Zhongyu},
  booktitle = {Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Tutorial Abstracts},
  month     = nov,
  year      = {2022},
  address   = {Taipei},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.aacl-tutorials.1},
  doi       = {10.18653/v1/2022.aacl-tutorials.1},
  pages     = {1--7},
  abstract  = {Knowledge graph construction which aims to extract knowledge from the text corpus, has appealed to the NLP community researchers. Previous decades have witnessed the remarkable progress of knowledge graph construction on the basis of neural models; however, those models often cost massive computation or labeled data resources and suffer from unstable inference accounting for biased or adversarial samples. Recently, numerous approaches have been explored to mitigate the efficiency and robustness issues for knowledge graph construction, such as prompt learning and adversarial training. In this tutorial, we aim to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction. Additionally, our goal is to provide a systematic and up-to-date overview of these methods and reveal new research opportunities to the audience.}
}
@inproceedings{lai-etal-2024-llms,
  title     = {{LLM}s Beyond {E}nglish: Scaling the Multilingual Capability of {LLM}s with Cross-Lingual Feedback},
  author    = {Lai, Wen  and
               Mesgar, Mohsen  and
               Fraser, Alexander},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-acl.488},
  doi       = {10.18653/v1/2024.findings-acl.488},
  pages     = {8186--8213},
  abstract  = {To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.}
}
@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@inproceedings{gpt3,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}
@article{zhou2023universalner,
  title         = {UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition},
  author        = {Wenxuan Zhou and Sheng Zhang and Yu Gu and Muhao Chen and Hoifung Poon},
  year          = {2023},
  eprint        = {2308.03279},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}




