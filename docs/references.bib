@book{pemmaraju2003computational,
  title     = {Computational discrete mathematics: Combinatorics and graph theory with mathematica{\textregistered}},
  author    = {Pemmaraju, Sriram and Skiena, Steven},
  year      = {2003},
  publisher = {Cambridge university press}
}
@article{PalB96,
  author    = {Madhumangal Pal and
               G. P. Bhattacharjee},
  title     = {A sequential algorithm for finding a maximum weight \emph{K}-independent
               set on interval graphs},
  journal   = {Int. J. Comput. Math.},
  volume    = {60},
  number    = {3-4},
  pages     = {205--214},
  year      = {1996},
  url       = {https://doi.org/10.1080/00207169608804486},
  doi       = {10.1080/00207169608804486},
  timestamp = {Thu, 16 May 2019 19:54:57 +0200},
  biburl    = {https://dblp.org/rec/journals/ijcm/PalB96.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bhattacharya2014maximum,
  title     = {Maximum Independent Set for Interval Graphs and Trees in Space Efficient Models.},
  author    = {Bhattacharya, Binay K and De, Minati and Nandy, Subhas C and Roy, Sasanka},
  booktitle = {CCCG},
  year      = {2014}
}
@inproceedings{garcia-ferrero-etal-2023-projection,
  title     = {{T}-Projection: High Quality Annotation Projection for Sequence Labeling Tasks},
  author    = {Garc{\'\i}a-Ferrero, Iker  and
               Agerri, Rodrigo  and
               Rigau, German},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.1015},
  doi       = {10.18653/v1/2023.findings-emnlp.1015},
  pages     = {15203--15217},
  abstract  = {In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text2text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-European and 8 low-resource African languages. We demostrate that T-projection outperforms previous annotation projection methods by a wide margin. We believe that T-Projection can help to automatically alleviate the lack of high-quality training data for sequence labeling tasks. Code and data are publicly available.}
}
@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, A},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017}
}
@misc{gurobi,
  author = {{Gurobi Optimization, LLC}},
  title  = {{Gurobi Optimizer Reference Manual}},
  year   = 2024,
  url    = {https://www.gurobi.com}
}
@inproceedings{adelani-etal-2022-masakhaner,
  title     = {{M}asakha{NER} 2.0: {A}frica-centric Transfer Learning for Named Entity Recognition},
  author    = {Adelani, David  and
               Neubig, Graham  and
               Ruder, Sebastian  and
               Rijhwani, Shruti  and
               Beukman, Michael  and
               Palen-Michel, Chester  and
               Lignos, Constantine  and
               Alabi, Jesujoba  and
               Muhammad, Shamsuddeen  and
               Nabende, Peter  and
               Dione, Cheikh M. Bamba  and
               Bukula, Andiswa  and
               Mabuya, Rooweither  and
               Dossou, Bonaventure F. P.  and
               Sibanda, Blessing  and
               Buzaaba, Happy  and
               Mukiibi, Jonathan  and
               Kalipe, Godson  and
               Mbaye, Derguene  and
               Taylor, Amelia  and
               Kabore, Fatoumata  and
               Emezue, Chris Chinenye  and
               Aremu, Anuoluwapo  and
               Ogayo, Perez  and
               Gitau, Catherine  and
               Munkoh-Buabeng, Edwin  and
               Memdjokam Koagne, Victoire  and
               Tapo, Allahsera Auguste  and
               Macucwa, Tebogo  and
               Marivate, Vukosi  and
               Elvis, Mboning Tchiaze  and
               Gwadabe, Tajuddeen  and
               Adewumi, Tosin  and
               Ahia, Orevaoghene  and
               Nakatumba-Nabende, Joyce},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.298},
  doi       = {10.18653/v1/2022.emnlp-main.298},
  pages     = {4488--4508},
  abstract  = {African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14{\%} over 20 languages as compared to using English.}
}
@inproceedings{conneau-etal-2020-unsupervised-xlmr,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}
@article{Kingma2014AdamAM,
  title   = {Adam: A Method for Stochastic Optimization},
  author  = {Diederik P. Kingma and Jimmy Ba},
  journal = {CoRR},
  year    = {2014},
  volume  = {abs/1412.6980},
  url     = {https://api.semanticscholar.org/CorpusID:6628106}
}
@inproceedings{tjong-kim-sang-de-meulder-2003-introduction-conll,
  title     = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.  and
               De Meulder, Fien},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419},
  pages     = {142--147}
}
