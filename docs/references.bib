@book{pemmaraju2003computational,
  title     = {Computational discrete mathematics: Combinatorics and graph theory with mathematica{\textregistered}},
  author    = {Pemmaraju, Sriram and Skiena, Steven},
  year      = {2003},
  publisher = {Cambridge university press}
}
@article{PalB96,
  author    = {Madhumangal Pal and
               G. P. Bhattacharjee},
  title     = {A sequential algorithm for finding a maximum weight \emph{K}-independent
               set on interval graphs},
  journal   = {Int. J. Comput. Math.},
  volume    = {60},
  number    = {3-4},
  pages     = {205--214},
  year      = {1996},
  url       = {https://doi.org/10.1080/00207169608804486},
  doi       = {10.1080/00207169608804486},
  timestamp = {Thu, 16 May 2019 19:54:57 +0200},
  biburl    = {https://dblp.org/rec/journals/ijcm/PalB96.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{bhattacharya2014maximum,
  title     = {Maximum Independent Set for Interval Graphs and Trees in Space Efficient Models.},
  author    = {Bhattacharya, Binay K and De, Minati and Nandy, Subhas C and Roy, Sasanka},
  booktitle = {CCCG},
  year      = {2014}
}
@inproceedings{garcia-ferrero-etal-2023-projection,
  title     = {{T}-Projection: High Quality Annotation Projection for Sequence Labeling Tasks},
  author    = {Garc{\'\i}a-Ferrero, Iker  and
               Agerri, Rodrigo  and
               Rigau, German},
  editor    = {Bouamor, Houda  and
               Pino, Juan  and
               Bali, Kalika},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  month     = dec,
  year      = {2023},
  address   = {Singapore},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-emnlp.1015},
  doi       = {10.18653/v1/2023.findings-emnlp.1015},
  pages     = {15203--15217},
  abstract  = {In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text2text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-European and 8 low-resource African languages. We demostrate that T-projection outperforms previous annotation projection methods by a wide margin. We believe that T-Projection can help to automatically alleviate the lack of high-quality training data for sequence labeling tasks. Code and data are publicly available.}
}
@article{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, A},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017}
}
@misc{gurobi,
  author = {{Gurobi Optimization, LLC}},
  title  = {{Gurobi Optimizer Reference Manual}},
  year   = 2024,
  url    = {https://www.gurobi.com}
}
@inproceedings{adelani-etal-2022-masakhaner,
  title     = {{M}asakha{NER} 2.0: {A}frica-centric Transfer Learning for Named Entity Recognition},
  author    = {Adelani, David  and
               Neubig, Graham  and
               Ruder, Sebastian  and
               Rijhwani, Shruti  and
               Beukman, Michael  and
               Palen-Michel, Chester  and
               Lignos, Constantine  and
               Alabi, Jesujoba  and
               Muhammad, Shamsuddeen  and
               Nabende, Peter  and
               Dione, Cheikh M. Bamba  and
               Bukula, Andiswa  and
               Mabuya, Rooweither  and
               Dossou, Bonaventure F. P.  and
               Sibanda, Blessing  and
               Buzaaba, Happy  and
               Mukiibi, Jonathan  and
               Kalipe, Godson  and
               Mbaye, Derguene  and
               Taylor, Amelia  and
               Kabore, Fatoumata  and
               Emezue, Chris Chinenye  and
               Aremu, Anuoluwapo  and
               Ogayo, Perez  and
               Gitau, Catherine  and
               Munkoh-Buabeng, Edwin  and
               Memdjokam Koagne, Victoire  and
               Tapo, Allahsera Auguste  and
               Macucwa, Tebogo  and
               Marivate, Vukosi  and
               Elvis, Mboning Tchiaze  and
               Gwadabe, Tajuddeen  and
               Adewumi, Tosin  and
               Ahia, Orevaoghene  and
               Nakatumba-Nabende, Joyce},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.emnlp-main.298},
  doi       = {10.18653/v1/2022.emnlp-main.298},
  pages     = {4488--4508},
  abstract  = {African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14{\%} over 20 languages as compared to using English.}
}
@inproceedings{conneau-etal-2020-unsupervised-xlmr,
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  author    = {Conneau, Alexis  and
               Khandelwal, Kartikay  and
               Goyal, Naman  and
               Chaudhary, Vishrav  and
               Wenzek, Guillaume  and
               Guzm{\'a}n, Francisco  and
               Grave, Edouard  and
               Ott, Myle  and
               Zettlemoyer, Luke  and
               Stoyanov, Veselin},
  editor    = {Jurafsky, Dan  and
               Chai, Joyce  and
               Schluter, Natalie  and
               Tetreault, Joel},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.acl-main.747},
  doi       = {10.18653/v1/2020.acl-main.747},
  pages     = {8440--8451},
  abstract  = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.}
}
@article{Kingma2014AdamAM,
  title   = {Adam: A Method for Stochastic Optimization},
  author  = {Diederik P. Kingma and Jimmy Ba},
  journal = {CoRR},
  year    = {2014},
  volume  = {abs/1412.6980},
  url     = {https://api.semanticscholar.org/CorpusID:6628106}
}
@inproceedings{tjong-kim-sang-de-meulder-2003-introduction-conll,
  title     = {Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Tjong Kim Sang, Erik F.  and
               De Meulder, Fien},
  booktitle = {Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003},
  year      = {2003},
  url       = {https://aclanthology.org/W03-0419},
  pages     = {142--147}
}
@inproceedings{agerri-etal-2018-building,
  title     = {Building Named Entity Recognition Taggers via Parallel Corpora},
  author    = {Agerri, Rodrigo  and
               Chung, Yiling  and
               Aldabe, Itziar  and
               Aranberri, Nora  and
               Labaka, Gorka  and
               Rigau, German},
  editor    = {Calzolari, Nicoletta  and
               Choukri, Khalid  and
               Cieri, Christopher  and
               Declerck, Thierry  and
               Goggi, Sara  and
               Hasida, Koiti  and
               Isahara, Hitoshi  and
               Maegaard, Bente  and
               Mariani, Joseph  and
               Mazo, H{\'e}l{\`e}ne  and
               Moreno, Asuncion  and
               Odijk, Jan  and
               Piperidis, Stelios  and
               Tokunaga, Takenobu},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
  month     = may,
  year      = {2018},
  address   = {Miyazaki, Japan},
  publisher = {European Language Resources Association (ELRA)},
  url       = {https://aclanthology.org/L18-1557}
}
@inproceedings{koehn2005europarl,
  title     = {Europarl: A parallel corpus for statistical machine translation},
  author    = {Koehn, Philipp},
  booktitle = {Proceedings of machine translation summit x: papers},
  pages     = {79--86},
  year      = {2005}
}
@misc{nllbteam2022languageleftbehindscaling,
  title         = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  author        = {NLLB Team and Marta R. Costa-jussà and James Cross and Onur Çelebi and Maha Elbayad and Kenneth Heafield and Kevin Heffernan and Elahe Kalbassi and Janice Lam and Daniel Licht and Jean Maillard and Anna Sun and Skyler Wang and Guillaume Wenzek and Al Youngblood and Bapi Akula and Loic Barrault and Gabriel Mejia Gonzalez and Prangthip Hansanti and John Hoffman and Semarley Jarrett and Kaushik Ram Sadagopan and Dirk Rowe and Shannon Spruit and Chau Tran and Pierre Andrews and Necip Fazil Ayan and Shruti Bhosale and Sergey Edunov and Angela Fan and Cynthia Gao and Vedanuj Goswami and Francisco Guzmán and Philipp Koehn and Alexandre Mourachko and Christophe Ropers and Safiyyah Saleem and Holger Schwenk and Jeff Wang},
  year          = {2022},
  eprint        = {2207.04672},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2207.04672}
}
@inproceedings{vamvas_sennrich_2022_nmtscore,
  title     = {{NMTS}core: A Multilingual Analysis of Translation-based Text Similarity Measures},
  author    = {Vamvas, Jannis  and
               Sennrich, Rico},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.15},
  doi       = {10.18653/v1/2022.findings-emnlp.15},
  pages     = {198--213},
  abstract  = {Being able to rank the similarity of short text segments is an interesting bonus feature of neural machine translation. Translation-based similarity measures include direct and pivot translation probability, as well as translation cross-likelihood, which has not been studied so far. We analyze these measures in the common framework of multilingual NMT, releasing the NMTScore library. Compared to baselines such as sentence embeddings, translation-based measures prove competitive in paraphrase identification and are more robust against adversarial or multilingual input, especially if proper normalization is applied. When used for reference-based evaluation of data-to-text generation in 2 tasks and 17 languages, translation-based measures show a relatively high correlation to human judgments.}
}
@article{dyckhoff1981new,
  title     = {A new linear programming approach to the cutting stock problem},
  author    = {Dyckhoff, Harald},
  journal   = {Operations Research},
  volume    = {29},
  number    = {6},
  pages     = {1092--1104},
  year      = {1981},
  publisher = {INFORMS}
}
@article{gilmore1963linear,
  title     = {A linear programming approach to the cutting stock problem—Part II},
  author    = {Gilmore, Paul C and Gomory, Ralph E},
  journal   = {Operations research},
  volume    = {11},
  number    = {6},
  pages     = {863--888},
  year      = {1963},
  publisher = {INFORMS}
}
@book{land2010automatic,
  title     = {An automatic method for solving discrete programming problems},
  author    = {Land, Ailsa H and Doig, Alison G},
  year      = {2010},
  publisher = {Springer}
}
@article{branchAndCut,
  author   = {Padberg, Manfred and Rinaldi, Giovanni},
  title    = {A Branch-and-Cut Algorithm for the Resolution of Large-Scale Symmetric Traveling Salesman Problems},
  journal  = {SIAM Review},
  volume   = {33},
  number   = {1},
  pages    = {60-100},
  year     = {1991},
  doi      = {10.1137/1033004},
  url      = {https://doi.org/10.1137/1033004},
  eprint   = {https://doi.org/10.1137/1033004},
  abstract = { An algorithm is described for solving large-scale instances of the Symmetric Traveling Salesman Problem (STSP) to optimality. The core of the algorithm is a “polyhedral” cutting-plane procedure that exploits a subset of the system of linear inequalities defining the convex hull of the incidence vectors of the hamiltonian cycles of a complete graph. The cuts are generated by several identification procedures that have been described in a companion paper. Whenever the cutting-plane procedure does not terminate with an optimal solution the algorithm uses a tree-search strategy that, as opposed to branch-and-bound, keeps on producing cuts after branching. The algorithm has been implemented in FORTRAN. Two different linear programming (LP) packages have been used as the LP solver. The implementation of the algorithm and the interface with one of the LP solvers is described in sufficient detail to permit the replication of our experiments. Computational results are reported with up to 42 STSPs with sizes ranging from 48 to 2,392 nodes. Most of the medium-sized test problems are taken from the literature; all others are large-scale real-world problems. All of the instances considered in this study were solved to optimality by the algorithm in “reasonable” computation times. }
}
@article{weikum2021machine,
  title     = {Machine knowledge: Creation and curation of comprehensive knowledge bases},
  author    = {Weikum, Gerhard and Dong, Xin Luna and Razniewski, Simon and Suchanek, Fabian and others},
  journal   = {Foundations and Trends{\textregistered} in Databases},
  volume    = {10},
  number    = {2-4},
  pages     = {108--490},
  year      = {2021},
  publisher = {Now Publishers, Inc.}
}
@inproceedings{zhang-etal-2022-efficient-robust,
  title     = {Efficient and Robust Knowledge Graph Construction},
  author    = {Zhang, Ningyu  and
               Gui, Tao  and
               Nan, Guoshun},
  editor    = {Alonso, Miguel A.  and
               Wei, Zhongyu},
  booktitle = {Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing: Tutorial Abstracts},
  month     = nov,
  year      = {2022},
  address   = {Taipei},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.aacl-tutorials.1},
  doi       = {10.18653/v1/2022.aacl-tutorials.1},
  pages     = {1--7},
  abstract  = {Knowledge graph construction which aims to extract knowledge from the text corpus, has appealed to the NLP community researchers. Previous decades have witnessed the remarkable progress of knowledge graph construction on the basis of neural models; however, those models often cost massive computation or labeled data resources and suffer from unstable inference accounting for biased or adversarial samples. Recently, numerous approaches have been explored to mitigate the efficiency and robustness issues for knowledge graph construction, such as prompt learning and adversarial training. In this tutorial, we aim to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction. Additionally, our goal is to provide a systematic and up-to-date overview of these methods and reveal new research opportunities to the audience.}
}
@inproceedings{lai-etal-2024-llms,
  title     = {{LLM}s Beyond {E}nglish: Scaling the Multilingual Capability of {LLM}s with Cross-Lingual Feedback},
  author    = {Lai, Wen  and
               Mesgar, Mohsen  and
               Fraser, Alexander},
  editor    = {Ku, Lun-Wei  and
               Martins, Andre  and
               Srikumar, Vivek},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
  month     = aug,
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.findings-acl.488},
  doi       = {10.18653/v1/2024.findings-acl.488},
  pages     = {8186--8213},
  abstract  = {To democratize large language models (LLMs) to most natural languages, it is imperative to make these models capable of understanding and generating texts in many languages, in particular low-resource ones. While recent multilingual LLMs demonstrate remarkable performance in such capabilities, these LLMs still support a limited number of human languages due to the lack of training data for low resource languages. Moreover, these LLMs are not yet aligned with human preference for downstream tasks, which is crucial for the success of LLMs in English. In this paper, we introduce xLLaMA-100 and xBLOOM-100 (collectively xLLMs-100), which scale the multilingual capabilities of LLaMA and BLOOM to 100 languages. To do so, we construct two datasets: a multilingual instruction dataset including 100 languages, which represents the largest language coverage to date, and a cross-lingual human feedback dataset encompassing 30 languages. We perform multilingual instruction tuning on the constructed instruction data and further align the LLMs with human feedback using the DPO algorithm on our cross-lingual human feedback dataset. We evaluate the multilingual understanding and generating capabilities of xLLMs-100 on five multilingual benchmarks. Experimental results show that xLLMs-100 consistently outperforms its peers across the benchmarks by considerable margins, defining a new state-of-the-art multilingual LLM that supports 100 languages.}
}
@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}
@inproceedings{gpt3,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}
@misc{sainz2024gollieannotationguidelinesimprove,
  title         = {GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction},
  author        = {Oscar Sainz and Iker García-Ferrero and Rodrigo Agerri and Oier Lopez de Lacalle and German Rigau and Eneko Agirre},
  year          = {2024},
  eprint        = {2310.03668},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2310.03668}
}
@article{zhou2023universalner,
  title         = {UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition},
  author        = {Wenxuan Zhou and Sheng Zhang and Yu Gu and Muhao Chen and Hoifung Poon},
  year          = {2023},
  eprint        = {2308.03279},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}
@inproceedings{parekh-etal-2024-contextual,
  title     = {Contextual Label Projection for Cross-Lingual Structured Prediction},
  author    = {Parekh, Tanmay  and
               Hsu, I-Hung  and
               Huang, Kuan-Hao  and
               Chang, Kai-Wei  and
               Peng, Nanyun},
  editor    = {Duh, Kevin  and
               Gomez, Helena  and
               Bethard, Steven},
  booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  month     = jun,
  year      = {2024},
  address   = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2024.naacl-long.321},
  doi       = {10.18653/v1/2024.naacl-long.321},
  pages     = {5738--5757},
  abstract  = {Label projection, which involves obtaining translated labels and texts jointly, is essential for leveraging machine translation to facilitate cross-lingual transfer in structured prediction tasks. Prior research exploring label projection often compromise translation accuracy by favoring simplified label translation or relying solely on word-level alignments. In this paper, we introduce a novel label projection approach, CLaP, which translates text to the target language and performs *contextual translation* on the labels using the translated text as the context, ensuring better accuracy for the translated labels. We leverage instruction-tuned language models with multilingual capabilities as our contextual translator, imposing the constraint of the presence of translated labels in the translated text via instructions. We benchmark CLaP with other label projection techniques on zero-shot cross-lingual transfer across 39 languages on two representative structured prediction tasks - event argument extraction (EAE) and named entity recognition (NER), showing over 2.4 F1 improvement for EAE and 1.4 F1 improvement for NER. We further explore the applicability of CLaP on ten extremely low-resource languages to showcase its potential for cross-lingual structured prediction.}
}
@article{Le2024ConstrainedDF,
  title   = {Constrained Decoding for Cross-lingual Label Projection},
  author  = {Duong Minh Le and Yang Chen and Alan Ritter and Wei Xu},
  journal = {ArXiv},
  year    = {2024},
  volume  = {abs/2402.03131},
  url     = {https://api.semanticscholar.org/CorpusID:267412651}
}
@inproceedings{liu-etal-2021-mulda,
  title     = {{M}ul{DA}: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual {NER}},
  author    = {Liu, Linlin  and
               Ding, Bosheng  and
               Bing, Lidong  and
               Joty, Shafiq  and
               Si, Luo  and
               Miao, Chunyan},
  editor    = {Zong, Chengqing  and
               Xia, Fei  and
               Li, Wenjie  and
               Navigli, Roberto},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.453},
  doi       = {10.18653/v1/2021.acl-long.453},
  pages     = {5834--5846},
  abstract  = {Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem. This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited. The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination. With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages. These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.}
}
@misc{transfusion,
  author = {Chen, Yang and Shah, Vedaant and Ritter, Alan},
  year   = {2023},
  month  = {05},
  pages  = {},
  title  = {Better Low-Resource Entity Recognition Through Translation and Annotation Fusion},
  doi    = {10.48550/arXiv.2305.13582}
}
@inproceedings{yang-etal-2022-crop,
  title     = {{CROP}: Zero-shot Cross-lingual Named Entity Recognition with Multilingual Labeled Sequence Translation},
  author    = {Yang, Jian  and
               Huang, Shaohan  and
               Ma, Shuming  and
               Yin, Yuwei  and
               Dong, Li  and
               Zhang, Dongdong  and
               Guo, Hongcheng  and
               Li, Zhoujun  and
               Wei, Furu},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.34},
  doi       = {10.18653/v1/2022.findings-emnlp.34},
  pages     = {486--496},
  abstract  = {Named entity recognition (NER) suffers from the scarcity of annotated training data, especially for low-resource languages without labeled data. Cross-lingual NER has been proposed to alleviate this issue by transferring knowledge from high-resource languages to low-resource languages via aligned cross-lingual representations or machine translation results. However, the performance of cross-lingual NER methods is severely affected by the unsatisfactory quality of translation or label projection. To address these problems, we propose a Cross-lingual Entity Projection framework (CROP) to enable zero-shot cross-lingual NER with the help of a multilingual labeled sequence translation model. Specifically, the target sequence is first translated into the source language and then tagged by a source NER model. We further adopt a labeled sequence translation model to project the tagged sequence back to the target language and label the target raw sentence. Ultimately, the whole pipeline is integrated into an end-to-end model by the way of self-training. Experimental results on two benchmarks demonstrate that our method substantially outperforms the previous strong baseline by a large margin of +3 7 F1 scores and achieves state-of-the-art performance.}
}
@inproceedings{xue-etal-2021-mt5,
  title     = {m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer},
  author    = {Xue, Linting  and
               Constant, Noah  and
               Roberts, Adam  and
               Kale, Mihir  and
               Al-Rfou, Rami  and
               Siddhant, Aditya  and
               Barua, Aditya  and
               Raffel, Colin},
  editor    = {Toutanova, Kristina  and
               Rumshisky, Anna  and
               Zettlemoyer, Luke  and
               Hakkani-Tur, Dilek  and
               Beltagy, Iz  and
               Bethard, Steven  and
               Cotterell, Ryan  and
               Chakraborty, Tanmoy  and
               Zhou, Yichao},
  booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.naacl-main.41},
  doi       = {10.18653/v1/2021.naacl-main.41},
  pages     = {483--498},
  abstract  = {The recent {``}Text-to-Text Transfer Transformer{''} (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent {``}accidental translation{''} in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.}
}
@inproceedings{chen-etal-2023-frustratingly,
  title     = {Frustratingly Easy Label Projection for Cross-lingual Transfer},
  author    = {Chen, Yang  and
               Jiang, Chao  and
               Ritter, Alan  and
               Xu, Wei},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
  month     = jul,
  year      = {2023},
  address   = {Toronto, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.findings-acl.357},
  doi       = {10.18653/v1/2023.findings-acl.357},
  pages     = {5775--5796},
  abstract  = {Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we call EasyProject, is easily applied to many languages and works surprisingly well, outperforming the more complex word alignment-based methods. We analyze several key factors that affect the end-task performance, and show EasyProject works well because it can accurately preserve label span boundaries after translation. We will publicly release all our code and data.}
}
@inproceedings{torge-etal-2023-named,
  title     = {Named Entity Recognition for Low-Resource Languages - Profiting from Language Families},
  author    = {Torge, Sunna  and
               Politov, Andrei  and
               Lehmann, Christoph  and
               Saffar, Bochra  and
               Tao, Ziyan},
  editor    = {Piskorski, Jakub  and
               Marci{\'n}czuk, Micha{\l}  and
               Nakov, Preslav  and
               Ogrodniczuk, Maciej  and
               Pollak, Senja  and
               P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
               Rybak, Piotr  and
               Steinberger, Josef  and
               Yangarber, Roman},
  booktitle = {Proceedings of the 9th Workshop on Slavic Natural Language Processing 2023 (SlavicNLP 2023)},
  month     = may,
  year      = {2023},
  address   = {Dubrovnik, Croatia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2023.bsnlp-1.1},
  doi       = {10.18653/v1/2023.bsnlp-1.1},
  pages     = {1--10},
  abstract  = {Machine learning drives forward the development in many areas of Natural Language Processing (NLP). Until now, many NLP systems and research are focusing on high-resource languages, i.e. languages for which many data resources exist. Recently, so-called low-resource languages increasingly come into focus. In this context, multi-lingual language models, which are trained on related languages to a target low-resource language, may enable NLP tasks on this low-resource language. In this work, we investigate the use of multi-lingual models for Named Entity Recognition (NER) for low-resource languages. We consider the West Slavic language family and the low-resource languages Upper Sorbian and Kashubian. Three RoBERTa models were trained from scratch, two mono-lingual models for Czech and Polish, and one bi-lingual model for Czech and Polish. These models were evaluated on the NER downstream task for Czech, Polish, Upper Sorbian, and Kashubian, and compared to existing state-of-the-art models such as RobeCzech, HerBERT, and XLM-R. The results indicate that the mono-lingual models perform better on the language they were trained on, and both the mono-lingual and language family models outperform the large multi-lingual model in downstream tasks. Overall, the study shows that low-resource West Slavic languages can benefit from closely related languages and their models.}
}
@inproceedings{garcia-ferrero-etal-2022-model,
  title     = {Model and Data Transfer for Cross-Lingual Sequence Labelling in Zero-Resource Settings},
  author    = {Garc{\'\i}a-Ferrero, Iker  and
               Agerri, Rodrigo  and
               Rigau, German},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2022},
  month     = dec,
  year      = {2022},
  address   = {Abu Dhabi, United Arab Emirates},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.findings-emnlp.478},
  doi       = {10.18653/v1/2022.findings-emnlp.478},
  pages     = {6403--6416},
  abstract  = {Zero-resource cross-lingual transfer approaches aim to apply supervised modelsfrom a source language to unlabelled target languages. In this paper we performan in-depth study of the two main techniques employed so far for cross-lingualzero-resource sequence labelling, based either on data or model transfer.Although previous research has proposed translation and annotation projection(data-based cross-lingual transfer) as an effective technique for cross-lingualsequence labelling, in this paper we experimentally demonstrate that highcapacity multilingual language models applied in a zero-shot (model-basedcross-lingual transfer) setting consistently outperform data-basedcross-lingual transfer approaches. A detailed analysis of our results suggeststhat this might be due to important differences in language use. Morespecifically, machine translation often generates a textual signal which isdifferent to what the models are exposed to when using gold standard data,which affects both the fine-tuning and evaluation processes. Our results alsoindicate that data-based cross-lingual transfer approaches remain a competitiveoption when high-capacity multilingual language models are not available.}
}
@article{He2021DeBERTaV3ID,
  title   = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  author  = {Pengcheng He and Jianfeng Gao and Weizhu Chen},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2111.09543},
  url     = {https://api.semanticscholar.org/CorpusID:244346093}
}
@misc{huang2024leveraginglargelanguagemodels,
  title         = {Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies},
  author        = {Yining Huang and Keke Tang and Meilian Chen},
  year          = {2024},
  eprint        = {2402.09282},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2402.09282}
}
@article{He2020DeBERTaDB,
  title   = {DeBERTa: Decoding-enhanced BERT with Disentangled Attention},
  author  = {Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2006.03654},
  url     = {https://api.semanticscholar.org/CorpusID:219531210}
}
@article{lstm,
  author     = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
  title      = {Long Short-Term Memory},
  year       = {1997},
  issue_date = {November 15, 1997},
  publisher  = {MIT Press},
  address    = {Cambridge, MA, USA},
  volume     = {9},
  number     = {8},
  issn       = {0899-7667},
  url        = {https://doi.org/10.1162/neco.1997.9.8.1735},
  doi        = {10.1162/neco.1997.9.8.1735},
  abstract   = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  journal    = {Neural Comput.},
  month      = nov,
  pages      = {1735–1780},
  numpages   = {46}
}
@article{liu-etal-2020-multilingual-denoising,
  title     = {Multilingual Denoising Pre-training for Neural Machine Translation},
  author    = {Liu, Yinhan  and
               Gu, Jiatao  and
               Goyal, Naman  and
               Li, Xian  and
               Edunov, Sergey  and
               Ghazvininejad, Marjan  and
               Lewis, Mike  and
               Zettlemoyer, Luke},
  editor    = {Johnson, Mark  and
               Roark, Brian  and
               Nenkova, Ani},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {8},
  year      = {2020},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2020.tacl-1.47},
  doi       = {10.1162/tacl_a_00343},
  pages     = {726--742},
  abstract  = {This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART{---}a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1}
}
@inproceedings{dou-neubig-2021-word,
  title     = {Word Alignment by Fine-tuning Embeddings on Parallel Corpora},
  author    = {Dou, Zi-Yi  and
               Neubig, Graham},
  editor    = {Merlo, Paola  and
               Tiedemann, Jorg  and
               Tsarfaty, Reut},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  month     = apr,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.eacl-main.181},
  doi       = {10.18653/v1/2021.eacl-main.181},
  pages     = {2112--2128},
  abstract  = {Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs.}
}
@inproceedings{jalili-sabet-etal-2020-simalign,
  title     = {{S}im{A}lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings},
  author    = {Jalili Sabet, Masoud  and
               Dufter, Philipp  and
               Yvon, Fran{\c{c}}ois  and
               Sch{\"u}tze, Hinrich},
  editor    = {Cohn, Trevor  and
               He, Yulan  and
               Liu, Yang},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.findings-emnlp.147},
  doi       = {10.18653/v1/2020.findings-emnlp.147},
  pages     = {1627--1643},
  abstract  = {Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings {--} both static and contextualized {--} for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners {--} even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences.}
}




