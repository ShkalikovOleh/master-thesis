"""This module implements steps for evaluation of XLNER pipeline"""

from collections import defaultdict
import evaluate
from datasets import Dataset, DatasetDict
import numpy as np


class SeqEvalIntrinsicEvaluation:
    """Intrinsic evaluator computes SeqEval metrics for generated by XLNER
    labeling compared to ground truth labels for the real NER dataset"""

    def __init__(
        self,
        orig_column: str = "ner_tags",
        gen_column: str = "gen_ner_tags",
        gen_as_tags: bool = False,
        log_to_wandb: bool = True,
        batch_size: int = 1000,
        labels_to_ignore: list[str] = [],
    ) -> None:
        super().__init__()
        self.orig_column = orig_column
        self.gen_column = gen_column
        self.log_to_wandb = log_to_wandb
        self.labels_to_ignore = labels_to_ignore
        self.batch_size = batch_size
        self.gen_as_tags = gen_as_tags

    def __call__(self, ds: Dataset | DatasetDict) -> Dataset | DatasetDict:
        label_list, label_map = self.prepare_labels_mapping(ds)

        metric = evaluate.load("seqeval")

        def accumulate_metric(refs, preds):
            ref_labels = [[label_list[tag] for tag in gt] for gt in refs]
            if self.gen_as_tags:
                pred_labels = [[label_list[tag] for tag in pred] for pred in preds]
            else:
                pred_labels = [[label_map[label] for label in pred] for pred in preds]
            metric.add_batch(references=ref_labels, predictions=pred_labels)

        ds.map(
            accumulate_metric,
            input_columns=[self.orig_column, self.gen_column],
            batched=True,
            batch_size=self.batch_size,
        )

        results = metric.compute()
        print(results)

        if self.log_to_wandb:
            import wandb

            for gkey, gvalue in results.items():
                if isinstance(gvalue, dict):
                    for key, value in gvalue.items():
                        wandb.run.summary[f"{gkey}_{key}"] = value
                else:
                    wandb.run.summary[gkey] = gvalue

        return ds

    def prepare_labels_mapping(self, ds: Dataset | DatasetDict):
        if isinstance(ds, DatasetDict):
            split = next(iter(ds.keys()))
            label_list = ds[split].features[self.orig_column].feature.names
        else:
            label_list = ds.features[self.orig_column].feature.names

        # Make all to be ignored labels equal to O
        new_label_list = []
        label_map = defaultdict(lambda: "O")
        for label in label_list:
            if label.startswith("B-") or label.startswith("I-"):
                if label[2:] in self.labels_to_ignore:
                    new_label_list.append("O")
                else:
                    new_label_list.append(label)
                    label_map[label] = label
            else:
                new_label_list.append(label)
                label_map[label] = label

        return new_label_list, label_map


class ILPObjectiveEvaluation:

    def __init__(
        self,
        log_to_wandb: bool = True,
        percentiles: list[float] = [0.25, 0.5, 0.75, 0.95],
    ) -> None:
        super().__init__()
        self.log_to_wandb = log_to_wandb
        self.percentiles = percentiles

    def __call__(self, ds: Dataset) -> Dataset:
        total_costs = ds["total_cost"]
        rel_costs = ds["rel_cost"]

        metrics = {}
        for p in self.percentiles:
            per_rel_cost = np.percentile(rel_costs, p * 100)
            per_total_cost = np.percentile(total_costs, p * 100)
            metrics[f"rel_cost@{round(p, 2)}"] = per_rel_cost
            metrics[f"total_cost@{round(p, 2)}"] = per_total_cost

        print(metrics)

        if self.log_to_wandb:
            import wandb

            for key, value in metrics.items():
                wandb.run.summary[key] = value


class W2WAlignmentEvaluation:
    """Computes Recall, Precision, F1 and AER metrics of word-to-word alignments
    given predicted and gold alignemnts"""

    def __init__(
        self,
        gold_column: str = "gold_alignments",
        pred_column: str = "word_alignments",
        log_to_wandb: bool = True,
    ) -> None:
        super().__init__()
        self.gold_column = gold_column
        self.pred_column = pred_column
        self.log_to_wandb = log_to_wandb

    def __call__(self, ds: Dataset) -> Dataset:
        def local_metrics(refs, preds):
            S = filter(lambda gold: gold[2], refs)

            P = set((align[0], align[1]) for align in refs)
            S = set((align[0], align[1]) for align in S)
            A = set((align[0], align[1]) for align in preds)

            return {
                "p_hits": len(P & A),
                "s_hits": len(S & A),
                "s_total": len(S),
                "a_total": len(A),
            }

        ds = ds.map(
            local_metrics,
            input_columns=[self.gold_column, self.pred_column],
            batched=False,
        )

        p_hits = np.sum(ds["p_hits"])
        s_hits = np.sum(ds["s_hits"])
        s_total = np.sum(ds["s_total"])
        a_total = np.sum(ds["a_total"])

        precision = p_hits / a_total
        recall = s_hits / s_total
        f1 = (2 * precision * recall) / (precision + recall)
        aer = 1 - (s_hits + p_hits) / (a_total + s_total)

        print("Recall:", recall, "Precision:", precision, "F1:", f1, "AER:", aer)

        if self.log_to_wandb:
            import wandb

            wandb.run.summary["recall"] = recall
            wandb.run.summary["precision"] = precision
            wandb.run.summary["f1"] = f1
            wandb.run.summary["aer"] = aer

        return ds
